# Complete ML Pipeline Project

A comprehensive, production-ready machine learning pipeline that demonstrates the entire ML lifecycle from data preprocessing to deployment and monitoring.

## ğŸ¯ Project Overview

This project implements a complete ML pipeline for customer churn prediction, showcasing best practices in:
- Data preprocessing and feature engineering
- Model training and hyperparameter optimization
- Model evaluation and selection
- API development and deployment
- Monitoring and maintenance

### Key Features
- **Automated Data Pipeline**: ETL processes with validation
- **Ensemble Models**: Combining multiple algorithms
- **Real-time Predictions**: Low-latency API endpoints
- **Model Monitoring**: Performance tracking and drift detection
- **Scalable Architecture**: Containerized microservices

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Source   â”‚    â”‚  Preprocessing  â”‚    â”‚  Model Training â”‚
â”‚   (Database/    â”‚â”€â”€â”€â–¶â”‚   & Feature     â”‚â”€â”€â”€â–¶â”‚  & Optimization â”‚
â”‚    Files/API)   â”‚    â”‚   Engineering   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Storage  â”‚    â”‚  API Gateway    â”‚    â”‚  Monitoring &   â”‚
â”‚   (MLflow/S3)   â”‚â—€â”€â”€â”€â”‚   & Serving     â”‚â—€â”€â”€â”€â”‚  Alerting       â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Docker
- PostgreSQL
- Redis (optional, for caching)

### Setup

1. **Clone and navigate**:
```bash
cd 01_complete_ml_pipeline
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Set up environment**:
```bash
cp .env.example .env
# Edit .env with your database and API keys
```

4. **Initialize database**:
```bash
python scripts/setup_database.py
```

5. **Run data preprocessing**:
```bash
python scripts/preprocess_data.py
```

6. **Train models**:
```bash
python scripts/train_models.py
```

7. **Start API server**:
```bash
python src/inference/api.py
```

## ğŸ“Š Project Structure

```
01_complete_ml_pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.yaml      # Model parameters
â”‚   â”œâ”€â”€ preprocessing_config.yaml # Data processing config
â”‚   â””â”€â”€ api_config.yaml        # API configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ preprocessing.py   # Data preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py # Feature engineering
â”‚   â”‚   â”œâ”€â”€ data_loader.py     # Data loading utilities
â”‚   â”‚   â””â”€â”€ data_validation.py # Data validation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ensemble_model.py  # Ensemble model implementation
â”‚   â”‚   â”œâ”€â”€ model_registry.py  # Model management
â”‚   â”‚   â”œâ”€â”€ base_model.py      # Base model class
â”‚   â”‚   â””â”€â”€ hyperparameter_tuning.py # Hyperparameter optimization
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train.py           # Main training script
â”‚   â”‚   â”œâ”€â”€ cross_validation.py # Cross-validation
â”‚   â”‚   â””â”€â”€ model_selection.py # Model selection
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ api.py             # FastAPI application
â”‚   â”‚   â”œâ”€â”€ predict.py         # Prediction logic
â”‚   â”‚   â”œâ”€â”€ batch_processor.py # Batch processing
â”‚   â”‚   â””â”€â”€ model_loader.py    # Model loading utilities
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ monitoring.py      # Monitoring utilities
â”‚       â”œâ”€â”€ logging.py         # Logging configuration
â”‚       â”œâ”€â”€ metrics.py         # Performance metrics
â”‚       â””â”€â”€ database.py        # Database utilities
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â””â”€â”€ 04_model_evaluation.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”‚   â””â”€â”€ Dockerfile.training
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ api-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ training-job.yaml
â”‚   â”‚   â””â”€â”€ service.yaml
â”‚   â””â”€â”€ cloud/
â”‚       â”œâ”€â”€ aws/
â”‚       â”‚   â”œâ”€â”€ ecs-task-definition.json
â”‚       â”‚   â””â”€â”€ cloudformation-template.yaml
â”‚       â””â”€â”€ gcp/
â”‚           â””â”€â”€ cloud-run.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ customer_churn.csv  # Sample dataset
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ sample/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_database.py
â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”œâ”€â”€ train_models.py
â”‚   â”œâ”€â”€ evaluate_models.py
â”‚   â””â”€â”€ deploy.sh
â””â”€â”€ docs/
    â”œâ”€â”€ api_documentation.md
    â”œâ”€â”€ deployment_guide.md
    â””â”€â”€ troubleshooting.md
```

## ğŸ”§ Configuration

### Model Configuration (`config/model_config.yaml`)
```yaml
models:
  random_forest:
    n_estimators: 100
    max_depth: 10
    min_samples_split: 5

  xgboost:
    n_estimators: 200
    learning_rate: 0.1
    max_depth: 6

  logistic_regression:
    C: 1.0
    solver: 'liblinear'

ensemble:
  method: 'weighted_average'
  weights: [0.3, 0.5, 0.2]

training:
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  cross_validation_folds: 5
```

### API Configuration (`config/api_config.yaml`)
```yaml
api:
  host: '0.0.0.0'
  port: 8000
  workers: 4
  reload: true

monitoring:
  enable_metrics: true
  metrics_port: 8001

cache:
  enable_redis: false
  redis_url: 'redis://localhost:6379'
  cache_ttl: 3600
```

## ğŸ“ˆ Usage Examples

### API Endpoints

#### Health Check
```bash
curl http://localhost:8000/health
```

#### Single Prediction
```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{
       "customer_id": "12345",
       "age": 35,
       "tenure": 24,
       "monthly_charges": 75.50,
       "total_charges": 1812.00,
       "contract_type": "Month-to-month",
       "payment_method": "Electronic check"
     }'
```

#### Batch Prediction
```bash
curl -X POST "http://localhost:8000/predict/batch" \
     -H "Content-Type: application/json" \
     -d '{
       "customers": [
         {
           "customer_id": "12345",
           "age": 35,
           "tenure": 24,
           "monthly_charges": 75.50,
           "total_charges": 1812.00
         },
         {
           "customer_id": "67890",
           "age": 28,
           "tenure": 12,
           "monthly_charges": 95.00,
           "total_charges": 1140.00
         }
       ]
     }'
```

### Python SDK Usage

```python
from src.inference.predict import ChurnPredictor

# Initialize predictor
predictor = ChurnPredictor(model_path="models/ensemble_model.pkl")

# Single prediction
customer_data = {
    "age": 35,
    "tenure": 24,
    "monthly_charges": 75.50,
    "total_charges": 1812.00
}

result = predictor.predict(customer_data)
print(f"Churn probability: {result['probability']:.2f}")
print(f"Prediction: {result['prediction']}")

# Batch prediction
customers = [customer_data1, customer_data2, ...]
results = predictor.predict_batch(customers)
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_models.py
pytest tests/test_api.py

# Run with coverage
pytest --cov=src --cov-report=html
```

## ğŸš€ Deployment

### Docker Deployment

1. **Build Docker image**:
```bash
docker build -f deployment/docker/Dockerfile.api -t churn-predictor-api .
```

2. **Run with Docker Compose**:
```bash
docker-compose up -d
```

### Kubernetes Deployment

1. **Apply Kubernetes manifests**:
```bash
kubectl apply -f deployment/kubernetes/
```

2. **Check deployment status**:
```bash
kubectl get pods -l app=churn-predictor
```

### Cloud Deployment

#### AWS ECS
```bash
# Build and push to ECR
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-east-1.amazonaws.com
docker build -t churn-predictor .
docker tag churn-predictor:latest 123456789012.dkr.ecr.us-east-1.amazonaws.com/churn-predictor:latest
docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/churn-predictor:latest
```

#### Google Cloud Run
```bash
# Deploy to Cloud Run
gcloud run deploy churn-predictor \
  --image gcr.io/your-project/churn-predictor \
  --platform managed \
  --region us-central1
```

## ğŸ“Š Monitoring

### Performance Metrics
- **Prediction Latency**: < 100ms p99
- **Throughput**: 1000+ requests/second
- **Accuracy**: 85-90% depending on dataset
- **Model Drift Detection**: Automated alerts
- **Data Quality Monitoring**: Real-time validation

### Monitoring Dashboard
- **Prometheus**: Metrics collection
- **Grafana**: Visualization dashboards
- **MLflow**: Experiment tracking
- **Custom Alerts**: Email/Slack notifications

## ğŸ› ï¸ Development

### Adding New Models

1. **Create model class**:
```python
# src/models/new_model.py
from .base_model import BaseModel

class NewModel(BaseModel):
    def __init__(self, config):
        super().__init__(config)
        # Initialize your model

    def train(self, X, y):
        # Training logic
        pass

    def predict(self, X):
        # Prediction logic
        pass
```

2. **Register model**:
```python
# src/models/model_registry.py
from .new_model import NewModel

MODEL_REGISTRY = {
    'new_model': NewModel,
    # ... other models
}
```

3. **Update configuration**:
```yaml
# config/model_config.yaml
models:
  new_model:
    param1: value1
    param2: value2
```

### Adding New Features

1. **Update preprocessing**:
```python
# src/data/feature_engineering.py
def create_new_feature(df):
    # Feature engineering logic
    return df
```

2. **Update validation**:
```python
# src/data/data_validation.py
def validate_new_feature(feature_data):
    # Validation logic
    pass
```

## ğŸ“š Documentation

- [API Documentation](docs/api_documentation.md)
- [Deployment Guide](docs/deployment_guide.md)
- [Troubleshooting](docs/troubleshooting.md)
- [Model Documentation](docs/model_documentation.md)

## ğŸ”— Related Projects

- [Ensemble Methods Project](../02_ensemble_methods/)
- [Real-time Prediction System](../03_realtime_predictions/)
- [Model Monitoring Project](../04_model_monitoring/)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Update documentation
6. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Dataset: Customer Churn Prediction Dataset
- Libraries: scikit-learn, XGBoost, FastAPI, MLflow
- Inspiration: Best practices from ML engineering community

---

*Last Updated: September 2025*