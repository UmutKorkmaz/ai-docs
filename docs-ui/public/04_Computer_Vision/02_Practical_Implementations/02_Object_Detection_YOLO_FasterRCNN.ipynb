{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO and Faster R-CNN\n",
    "\n",
    "This notebook provides comprehensive implementations of object detection using YOLO (You Only Look Once) and Faster R-CNN architectures with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, ops\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    \"\"\"Dataset for object detection with COCO format annotations\"\"\"\n    "    def __init__(self, root_dir, annotation_file, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Load COCO format annotations\n",
    "        with open(annotation_file) as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        # Create image to annotations mapping\n",
    "        self.img_to_anns = defaultdict(list)\n",
    "        for ann in self.coco_data['annotations']:\n",
    "            self.img_to_anns[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Create image info mapping\n",
    "        self.img_info = {img['id']: img for img in self.coco_data['images']}\n",
    "        \n",
    "        # Category mapping\n",
    "        self.categories = {cat['id']: cat['name'] for cat in self.coco_data['categories']}\n",
    "        self.category_ids = list(self.categories.keys())\n",
    "        \n",
    "        self.img_ids = list(self.img_info.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.img_info[img_id]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "        \n",
    "        # Extract boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            # Convert COCO format [x, y, width, height] to [x1, y1, x2, y2]\n",
    "            bbox = ann['bbox']\n",
    "            x1, y1 = bbox[0], bbox[1]\n",
    "            x2, y2 = x1 + bbox[2], y1 + bbox[3]\n",
    "            \n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann.get('area', bbox[2] * bbox[3]))\n",
    "            iscrowd.append(ann.get('iscrowd', 0))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"Dataset for Pascal VOC format annotations\"\"\"\n    "    def __init__(self, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.img_dir = os.path.join(root_dir, 'JPEGImages')\n",
    "        self.ann_dir = os.path.join(root_dir, 'Annotations')\n",
    "        \n",
    "        # Get image files\n",
    "        self.img_files = [f for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        # Pascal VOC classes\n",
    "        self.classes = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "            'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "            'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
    "            'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "        ]\n",
    "        \n",
    "        self.class_to_idx = {cls: idx + 1 for idx, cls in enumerate(self.classes)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.img_files[idx]\n",
    "        img_name = img_file[:-4]  # Remove .jpg extension\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_file)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Parse XML annotation\n",
    "        ann_path = os.path.join(self.ann_dir, f'{img_name}.xml')\n",
    "        target = self.parse_voc_xml(ann_path)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def parse_voc_xml(self, xml_path):\n",
    "        \"\"\"Parse Pascal VOC XML annotation\"\"\"\n    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            if name not in self.class_to_idx:\n",
    "                continue\n",
    "            \n",
    "            bbox = obj.find('bndbox')\n",
    "            x1 = float(bbox.find('xmin').text)\n",
    "            y1 = float(bbox.find('ymin').text)\n",
    "            x2 = float(bbox.find('xmax').text)\n",
    "            y2 = float(bbox.find('ymax').text)\n",
    "            \n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(self.class_to_idx[name])\n",
    "            areas.append((x2 - x1) * (y2 - y1))\n",
    "        \n",
    "        return {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': torch.as_tensor(areas, dtype=torch.float32),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    \"\"\"Get data transforms for object detection\"\"\"\n    "    transforms = []\n    "    \n",
    "    if train:\n",
    "        # Random horizontal flip\n",
    "        transforms.append(transforms.RandomHorizontalFlip(0.5))\n",
    "        \n",
    "        # Color jitter\n",
    "        transforms.append(transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ))\n",
    "        \n",
    "        # Random rotation\n",
    "        transforms.append(transforms.RandomRotation(degrees=10))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    transforms.append(transforms.ToTensor())\n",
    "    \n",
    "    # Normalize\n",
    "    transforms.append(transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ))\n",
    "    \n",
    "    return transforms.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. YOLO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3Head(nn.Module):\n",
    "    \"\"\"YOLOv3 detection head\"\"\"\n    "    def __init__(self, in_channels, num_classes, num_anchors=3):\n",
    "        super(YOLOv3Head, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        # Detection layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        for i, in_ch in enumerate(in_channels):\n",
    "            # Series of convolutions\n",
    "            convs = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, in_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(in_ch),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Conv2d(in_ch, in_ch // 2, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(in_ch // 2),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Conv2d(in_ch // 2, num_anchors * (5 + num_classes), kernel_size=1)\n",
    "            )\n",
    "            self.conv_layers.append(convs)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        outputs = []\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            # Apply detection head\n",
    "            output = self.conv_layers[i](feature)\n",
    "            \n",
    "            # Reshape output: (batch, anchors, H, W, 5 + num_classes)\n",
    "            batch_size, _, H, W = output.shape\n",
    "            output = output.view(batch_size, self.num_anchors, 5 + self.num_classes, H, W)\n",
    "            output = output.permute(0, 1, 3, 4, 2)  # (batch, anchors, H, W, 5 + num_classes)\n",
    "            \n",
    "            outputs.append(output)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "class YOLOBackbone(nn.Module):\n",
    "    \"\"\"YOLO backbone using Darknet architecture\"\"\"\n    "    def __init__(self):\n",
    "        super(YOLOBackbone, self).__init__()\n",
    "        \n",
    "        # Darknet-53 backbone\n",
    "        self.backbone = self._create_darknet53()\n",
    "        \n",
    "    def _create_darknet53(self):\n",
    "        \"\"\"Create Darknet-53 backbone\"\"\"\n    "        layers = []\n",
    "        \n",
    "        # Initial conv\n",
    "        layers.extend([\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        ])\n",
    "        \n",
    "        # Residual blocks\n",
    "        layers.extend(self._make_residual_blocks(32, 64, 1))\n",
    "        layers.extend(self._make_residual_blocks(64, 128, 2))\n",
    "        layers.extend(self._make_residual_blocks(128, 256, 8))\n",
    "        layers.extend(self._make_residual_blocks(256, 512, 8))\n",
    "        layers.extend(self._make_residual_blocks(512, 1024, 4))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_residual_blocks(self, in_channels, out_channels, num_blocks):\n",
    "        \"\"\"Create residual blocks\"\"\"\n    "        layers = []\n",
    "        \n",
    "        # Downsample\n",
    "        layers.extend([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        ])\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_blocks):\n",
    "            layers.extend([\n",
    "                nn.Conv2d(out_channels, out_channels // 2, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels // 2),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Conv2d(out_channels // 2, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.1)\n",
    "            ])\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        \n",
    "        # Extract features at different scales\n",
    "        for name, layer in self.backbone.named_children():\n",
    "            x = layer(x)\n",
    "            \n",
    "            # Store features for different scales\n",
    "            if name in ['52', '76', '85']:  # Different scales\n",
    "                features[f'scale_{len(features)+1}'] = x\n",
    "        \n",
    "        return features\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    \"\"\"Complete YOLOv3 model\"\"\"\n    "    def __init__(self, num_classes=80, num_anchors=3):\n",
    "        super(YOLOv3, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        # Backbone\n",
    "        self.backbone = YOLOBackbone()\n",
    "        \n",
    "        # Feature pyramid network\n",
    "        self.fpn = self._create_fpn()\n",
    "        \n",
    "        # Detection heads\n",
    "        self.heads = YOLOv3Head(\n",
    "            in_channels=[256, 512, 1024],\n",
    "            num_classes=num_classes,\n",
    "            num_anchors=num_anchors\n",
    "        )\n",
    "        \n",
    "        # Anchor boxes\n",
    "        self.anchors = self._get_anchors()\n",
    "    \n",
    "    def _create_fpn(self):\n",
    "        \"\"\"Create feature pyramid network\"\"\"\n    "        fpn_layers = nn.ModuleList()\n",
    "        \n",
    "        # FPN layers for different scales\n",
    "        fpn_layers.extend([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1024, 512, kernel_size=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 256, kernel_size=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        return fpn_layers\n",
    "    \n",
    "    def _get_anchors(self):\n",
    "        \"\"\"Get anchor boxes for different scales\"\"\"\n    "        # COCO anchors (simplified)\n",
    "        anchors = [\n",
    "            [[10, 13], [16, 30], [33, 23]],    # Small objects\n",
    "            [[30, 61], [62, 45], [59, 119]],   # Medium objects\n",
    "            [[116, 90], [156, 198], [373, 326]] # Large objects\n",
    "        ]\n",
    "        return torch.tensor(anchors, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract backbone features\n",
    "        backbone_features = self.backbone(x)\n",
    "        \n",
    "        # Apply FPN\n",
    "        fpn_features = []\n",
    "        for i, (name, feature) in enumerate(backbone_features.items()):\n",
    "            if i < len(self.fpn):\n",
    "                feature = self.fpn[i](feature)\n",
    "            fpn_features.append(feature)\n",
    "        \n",
    "        # Apply detection heads\n",
    "        outputs = self.heads(fpn_features)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "def yolo_loss(predictions, targets, anchors, num_classes, device):\n",
    "    \"\"\"YOLO loss function\"\"\"\n    "    total_loss = 0\n",
    "    \n",
    "    for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
    "        # Get anchors for this scale\n",
    "        scale_anchors = anchors[i].to(device)\n",
    "        \n",
    "        # Extract predictions\n",
    "        batch_size, num_anchors, H, W, pred_dim = pred.shape\n",
    "        \n",
    "        # Reshape predictions\n",
    "        pred = pred.view(batch_size, num_anchors, H, W, 5 + num_classes)\n",
    "        \n",
    "        # Extract components\n",
    "        pred_xy = torch.sigmoid(pred[..., 0:2])  # Object center\n",
    "        pred_wh = torch.exp(pred[..., 2:4])     # Object size\n",
    "        pred_conf = torch.sigmoid(pred[..., 4])  # Objectness confidence\n",
    "        pred_cls = torch.sigmoid(pred[..., 5:])  # Class probabilities\n",
    "        \n",
    "        # Convert to absolute coordinates\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
    "        grid_x = grid_x.to(device).float()\n",
    "        grid_y = grid_y.to(device).float()\n",
    "        \n",
    "        # Calculate loss (simplified version)\n",
    "        # In practice, this would include:\n",
    "        # - Localization loss (MSE for boxes)\n",
    "        # - Confidence loss (binary cross-entropy)\n",
    "        # - Classification loss (cross-entropy)\n",
    "        \n",
    "        # Placeholder loss calculation\n",
    "        loss = torch.tensor(0.0, requires_grad=True).to(device)\n",
    "        \n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Faster R-CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    \"\"\"Region Proposal Network for Faster R-CNN\"\"\"\n    "    def __init__(self, in_channels, mid_channels=512, num_anchors=9):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        # Shared convolution\n",
    "        self.conv = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Classification layer (object vs background)\n",
    "        self.cls_logits = nn.Conv2d(mid_channels, num_anchors * 2, kernel_size=1)\n",
    "        \n",
    "        # Regression layer (box coordinates)\n",
    "        self.bbox_pred = nn.Conv2d(mid_channels, num_anchors * 4, kernel_size=1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights\"\"\"\n    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Shared convolution\n",
    "        x = F.relu(self.conv(features))\n",
    "        \n",
    "        # Classification and regression\n",
    "        cls_logits = self.cls_logits(x)\n",
    "        bbox_pred = self.bbox_pred(x)\n",
    "        \n",
    "        return cls_logits, bbox_pred\n",
    "\n",
    "class ROIAlign(nn.Module):\n",
    "    \"\"\"Region of Interest Align pooling\"\"\"\n    "    def __init__(self, output_size, spatial_scale):\n",
    "        super(ROIAlign, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "    \n",
    "    def forward(self, features, rois):\n",
    "        # Scale ROI coordinates\n",
    "        rois = rois * self.spatial_scale\n",
    "        \n",
    "        # Perform ROI align pooling\n",
    "        pooled_features = torch.ops.torchvision.roi_align(\n",
    "            features, rois, self.output_size, self.spatial_scale, 1\n",
    "        )\n",
    "        \n",
    "        return pooled_features\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    \"\"\"Faster R-CNN implementation\"\"\"\n    "    def __init__(self, backbone, num_classes, n_anchors=9):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Region Proposal Network\n",
    "        self.rpn = RegionProposalNetwork(backbone.out_channels, n_anchors=n_anchors)\n",
    "        \n",
    "        # ROI Pooling\n",
    "        self.roi_pool = ROIAlign((7, 7), spatial_scale=1/16)\n",
    "        \n",
    "        # Detection head\n",
    "        self.detector_head = nn.Sequential(\n",
    "            nn.Linear(256 * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Classification and regression heads\n",
    "        self.cls_score = nn.Linear(1024, num_classes)\n",
    "        self.bbox_pred = nn.Linear(1024, num_classes * 4)\n",
    "        \n",
    "        # Anchor generation\n",
    "        self.anchors = self._generate_anchors()\n",
    "    \n",
    "    def _generate_anchors(self):\n",
    "        \"\"\"Generate anchor boxes\"\"\"\n    "        # Simplified anchor generation\n",
    "        # In practice, this would generate anchors at multiple scales and aspect ratios\n",
    "        scales = [8, 16, 32]\n",
    "        aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        \n",
    "        anchors = []\n",
    "        for scale in scales:\n",
    "            for ratio in aspect_ratios:\n",
    "                w = scale * np.sqrt(ratio)\n",
    "                h = scale / np.sqrt(ratio)\n",
    "                anchors.append([w, h])\n",
    "        \n",
    "        return torch.tensor(anchors, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, images, targets=None):\n",
    "        # Extract features\n",
    "        features = self.backbone(images)\n",
    "        \n",
    "        # Region proposals\n",
    "        rpn_cls_logits, rpn_bbox_pred = self.rpn(features)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # Training mode\n",
    "            proposals = self._generate_proposals(rpn_cls_logits, rpn_bbox_pred)\n",
    "            \n",
    "            # Match proposals to ground truth\n",
    "            matched_gt_boxes, matched_labels = self._match_proposals_to_gt(proposals, targets)\n",
    "            \n",
    "            # ROI pooling\n",
    "            roi_features = self.roi_pool(features, proposals)\n",
    "            roi_features = roi_features.view(roi_features.size(0), -1)\n",
    "            \n",
    "            # Detection head\n",
    "            detector_features = self.detector_head(roi_features)\n",
    "            \n",
    "            cls_score = self.cls_score(detector_features)\n",
    "            bbox_pred = self.bbox_pred(detector_features)\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_dict = self._compute_losses(\n",
    "                rpn_cls_logits, rpn_bbox_pred,\n",
    "                cls_score, bbox_pred,\n",
    "                proposals, targets\n",
    "            )\n",
    "            \n",
    "            return loss_dict\n",
    "        else:\n",
    "            # Inference mode\n",
    "            proposals = self._generate_proposals(rpn_cls_logits, rpn_bbox_pred)\n",
    "            \n",
    "            # ROI pooling\n",
    "            roi_features = self.roi_pool(features, proposals)\n",
    "            roi_features = roi_features.view(roi_features.size(0), -1)\n",
    "            \n",
    "            # Detection head\n",
    "            detector_features = self.detector_head(roi_features)\n",
    "            \n",
    "            cls_score = self.cls_score(detector_features)\n",
    "            bbox_pred = self.bbox_pred(detector_features)\n",
    "            \n",
    "            # Post-processing\n",
    "            detections = self._post_process(cls_score, bbox_pred, proposals)\n",
    "            \n",
    "            return detections\n",
    "    \n",
    "    def _generate_proposals(self, rpn_cls_logits, rpn_bbox_pred):\n",
    "        \"\"\"Generate region proposals from RPN output\"\"\"\n    "        # This is a simplified version\n",
    "        # In practice, would apply NMS and other filtering\n",
    "        batch_size = rpn_cls_logits.shape[0]\n",
    "        \n",
    "        # Generate dummy proposals for now\n",
    "        num_proposals = 100\n",
    "        proposals = torch.rand(batch_size, num_proposals, 4) * 100  # Random boxes\n",
    "        \n",
    "        return proposals\n",
    "    \n",
    "    def _match_proposals_to_gt(self, proposals, targets):\n",
    "        \"\"\"Match proposals to ground truth boxes\"\"\"\n    "        # This is a simplified version\n",
    "        # In practice, would use IoU-based matching\n",
    "        batch_size = proposals.shape[0]\n",
    "        num_proposals = proposals.shape[1]\n",
    "        \n",
    "        # Dummy matching\n",
    "        matched_gt_boxes = torch.rand(batch_size, num_proposals, 4)\n",
    "        matched_labels = torch.randint(0, self.num_classes, (batch_size, num_proposals))\n",
    "        \n",
    "        return matched_gt_boxes, matched_labels\n",
    "    \n",
    "    def _compute_losses(self, rpn_cls_logits, rpn_bbox_pred, cls_score, bbox_pred, proposals, targets):\n",
    "        \"\"\"Compute losses for training\"\"\"\n    "        # This is a simplified version\n",
    "        # In practice, would include:\n",
    "        # - RPN classification loss\n",
    "        # - RPN regression loss\n",
    "        # - Detector classification loss\n",
    "        # - Detector regression loss\n",
    "        \n",
    "        # Placeholder losses\n",
    "        rpn_cls_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        rpn_bbox_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        detector_cls_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        detector_bbox_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        return {\n",
    "            'loss_rpn_cls': rpn_cls_loss,\n",
    "            'loss_rpn_bbox': rpn_bbox_loss,\n",
    "            'loss_detector_cls': detector_cls_loss,\n",
    "            'loss_detector_bbox': detector_bbox_loss,\n",
    "            'loss': rpn_cls_loss + rpn_bbox_loss + detector_cls_loss + detector_bbox_loss\n",
    "        }\n",
    "    \n",
    "    def _post_process(self, cls_score, bbox_pred, proposals):\n",
    "        \"\"\"Post-process detections\"\"\"\n    "        # Apply softmax to classification scores\n",
    "        cls_probs = F.softmax(cls_score, dim=-1)\n",
    "        \n",
    "        # This is a simplified version\n",
    "        # In practice, would apply NMS and other filtering\n",
    "        detections = {\n",
    "            'boxes': proposals,\n",
    "            'scores': cls_probs,\n",
    "            'labels': torch.argmax(cls_probs, dim=-1)\n",
    "        }\n",
    "        \n",
    "        return detections\n",
    "\n",
    "def create_faster_rcnn_model(num_classes, pretrained=True):\n",
    "    \"\"\"Create Faster R-CNN model using torchvision\"\"\"\n    "    # Load pre-trained model\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
    "    \n",
    "    # Replace the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionTrainer:\n",
    "    \"\"\"Trainer for object detection models\"\"\"\n    "    def __init__(self, model, train_loader, val_loader, device, save_dir='checkpoints'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Create save directory\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_rpn_loss': [],\n",
    "            'val_rpn_loss': [],\n",
    "            'train_detector_loss': [],\n",
    "            'val_detector_loss': [],\n",
    "            'mAP': []\n",
    "        }\n",
    "        \n",
    "        # Best model tracking\n",
    "        self.best_mAP = 0.0\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def train_epoch(self, optimizer):\n",
    "        \"\"\"Train for one epoch\"\"\"\n    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        running_rpn_loss = 0.0\n",
    "        running_detector_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(self.train_loader, desc='Training')\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "            images = [img.to(self.device) for img in images]\n",
    "            targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_dict = self.model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += losses.item()\n",
    "            running_rpn_loss += loss_dict.get('loss_rpn_cls', 0) + loss_dict.get('loss_rpn_bbox', 0)\n",
    "            running_detector_loss += loss_dict.get('loss_detector_cls', 0) + loss_dict.get('loss_detector_bbox', 0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'RPN': f'{running_rpn_loss/(batch_idx+1):.4f}',\n",
    "                'Det': f'{running_detector_loss/(batch_idx+1):.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_rpn_loss = running_rpn_loss / len(self.train_loader)\n",
    "        avg_detector_loss = running_detector_loss / len(self.train_loader)\n",
    "        \n",
    "        return avg_loss, avg_rpn_loss, avg_detector_loss\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model\"\"\"\n    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_rpn_loss = 0.0\n",
    "        running_detector_loss = 0.0\n",
    "        \n",
    "        all_detections = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(self.val_loader, desc='Validation'):\n",
    "                images = [img.to(self.device) for img in images]\n",
    "                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                # Forward pass\n",
    "                loss_dict = self.model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                running_loss += losses.item()\n",
    "                running_rpn_loss += loss_dict.get('loss_rpn_cls', 0) + loss_dict.get('loss_rpn_bbox', 0)\n",
    "                running_detector_loss += loss_dict.get('loss_detector_cls', 0) + loss_dict.get('loss_detector_bbox', 0)\n",
    "                \n",
    "                # Get predictions for mAP calculation\n",
    "                predictions = self.model(images)\n",
    "                \n",
    "                all_detections.extend(predictions)\n",
    "                all_targets.extend(targets)\n",
    "        \n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_rpn_loss = running_rpn_loss / len(self.val_loader)\n",
    "        avg_detector_loss = running_detector_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate mAP\n",
    "        mAP = self.calculate_mAP(all_detections, all_targets)\n",
    "        \n",
    "        return avg_loss, avg_rpn_loss, avg_detector_loss, mAP\n",
    "    \n",
    "    def calculate_mAP(self, detections, targets, iou_threshold=0.5):\n",
    "        \"\"\"Calculate mean Average Precision\"\"\"\n    "        # This is a simplified mAP calculation\n",
    "        # In practice, would use proper COCO evaluation\n",
    "        \n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        num_classes = 0\n",
    "        \n",
    "        for i, (det, target) in enumerate(zip(detections, targets)):\n",
    "            # Calculate IoU between detections and ground truth\n",
    "            ious = self.calculate_iou(det['boxes'], target['boxes'])\n",
    "            \n",
    "            # Simple precision/recall calculation\n",
    "            if len(det['boxes']) > 0:\n",
    "                # Find matches\n",
    "                matches = ious > iou_threshold\n",
    "                true_positives = matches.any(dim=1).sum().item()\n",
    "                false_positives = len(det['boxes']) - true_positives\n",
    "                false_negatives = len(target['boxes']) - matches.any(dim=0).sum().item()\n",
    "                \n",
    "                precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "                recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "                \n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                num_classes += 1\n",
    "        \n",
    "        avg_precision = total_precision / num_classes if num_classes > 0 else 0\n",
    "        avg_recall = total_recall / num_classes if num_classes > 0 else 0\n",
    "        \n",
    "        # Simplified mAP as average precision\n",
    "        mAP = avg_precision\n",
    "        \n",
    "        return mAP\n",
    "    \n",
    "    def calculate_iou(self, boxes1, boxes2):\n",
    "        \"\"\"Calculate IoU between two sets of boxes\"\"\"\n    "        # Calculate intersection\n",
    "        x1 = torch.max(boxes1[:, None, 0], boxes2[:, 0])\n",
    "        y1 = torch.max(boxes1[:, None, 1], boxes2[:, 1])\n",
    "        x2 = torch.min(boxes1[:, None, 2], boxes2[:, 2])\n",
    "        y2 = torch.min(boxes1[:, None, 3], boxes2[:, 3])\n",
    "        \n",
    "        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "        \n",
    "        # Calculate union\n",
    "        area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "        area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "        union = area1[:, None] + area2 - intersection\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = intersection / (union + 1e-8)\n",
    "        \n",
    "        return iou\n",
    "    \n",
    "    def train(self, num_epochs, optimizer, scheduler=None, early_stopping_patience=10):\n",
    "        \"\"\"Main training loop\"\"\"\n    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_rpn_loss, train_detector_loss = self.train_epoch(optimizer)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_rpn_loss, val_detector_loss, mAP = self.validate()\n",
    "            \n",
    "            # Update learning rate\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Record metrics\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_rpn_loss'].append(train_rpn_loss)\n",
    "            self.history['val_rpn_loss'].append(val_rpn_loss)\n",
    "            self.history['train_detector_loss'].append(train_detector_loss)\n",
    "            self.history['val_detector_loss'].append(val_detector_loss)\n",
    "            self.history['mAP'].append(mAP)\n",
    "            \n",
    "            # Print results\n",
    "            print(f'Train Loss: {train_loss:.4f} (RPN: {train_rpn_loss:.4f}, Det: {train_detector_loss:.4f})')\n",
    "            print(f'Val Loss: {val_loss:.4f} (RPN: {val_rpn_loss:.4f}, Det: {val_detector_loss:.4f})')\n",
    "            print(f'mAP: {mAP:.4f}')\n",
    "            print('-' * 50)\n",
    "            \n",
    "            # Save best model\n",
    "            if mAP > self.best_mAP:\n",
    "                self.best_mAP = mAP\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                self.save_checkpoint(epoch, optimizer, scheduler, is_best=True)\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f'Loaded best model with mAP: {self.best_mAP:.4f}')\n",
    "    \n",
    "    def save_checkpoint(self, epoch, optimizer, scheduler, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "            'best_mAP': self.best_mAP,\n",
    "            'history': self.history\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            filename = os.path.join(self.save_dir, 'best_model.pth')\n",
    "        else:\n",
    "            filename = os.path.join(self.save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        \n",
    "        torch.save(checkpoint, filename)\n",
    "        print(f'Saved checkpoint: {filename}')\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        if 'history' in checkpoint:\n",
    "            self.history = checkpoint['history']\n",
    "        \n",
    "        if 'best_mAP' in checkpoint:\n",
    "            self.best_mAP = checkpoint['best_mAP']\n",
    "            \n",
    "        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]}')\n",
    "        print(f'Best mAP: {self.best_mAP:.4f}')\n",
    "        \n",
    "        return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(images, detections, targets=None, class_names=None, score_threshold=0.5):\n",
    "    \"\"\"Visualize object detection results\"\"\"\n    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (image, detection) in enumerate(zip(images[:4], detections[:4])):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Convert tensor to numpy\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).cpu().numpy()\n",
    "            # Denormalize\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            image = std * image + mean\n",
    "            image = np.clip(image, 0, 1)\n",
    "        \n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Draw ground truth boxes if available\n",
    "        if targets is not None and i < len(targets):\n",
    "            gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "            gt_labels = targets[i]['labels'].cpu().numpy()\n",
    "            \n",
    "            for box, label in zip(gt_boxes, gt_labels):\n",
    "                x1, y1, x2, y2 = box\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                       linewidth=2, edgecolor='green', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                if class_names:\n",
    "                    ax.text(x1, y1-5, class_names[label], color='green', fontsize=8)\n",
    "        \n",
    "        # Draw predicted boxes\n",
    "        pred_boxes = detection['boxes'].cpu().numpy()\n",
    "        pred_scores = detection['scores'].cpu().numpy()\n",
    "        pred_labels = detection['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "            if score > score_threshold:\n",
    "                x1, y1, x2, y2 = box\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                if class_names:\n",
    "                    ax.text(x1, y1-5, f'{class_names[label]} {score:.2f}',\n",
    "                           color='red', fontsize=8)\n",
    "        \n",
    "        ax.set_title(f'Image {i+1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plots\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # RPN loss plots\n",
    "    axes[0, 1].plot(history['train_rpn_loss'], label='Train RPN Loss')\n",
    "    axes[0, 1].plot(history['val_rpn_loss'], label='Val RPN Loss')\n",
    "    axes[0, 1].set_title('RPN Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Detector loss plots\n",
    "    axes[1, 0].plot(history['train_detector_loss'], label='Train Detector Loss')\n",
    "    axes[1, 0].plot(history['val_detector_loss'], label='Val Detector Loss')\n",
    "    axes[1, 0].set_title('Detector Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # mAP plot\n",
    "    axes[1, 1].plot(history['mAP'], label='mAP')\n",
    "    axes[1, 1].set_title('Mean Average Precision')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('mAP')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader, device, iou_thresholds=[0.5, 0.75]):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n    "    model.eval()\n",
    "    \n",
    "    all_detections = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_loader, desc='Evaluating'):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            predictions = model(images)\n",
    "            \n",
    "            all_detections.extend(predictions)\n",
    "            all_targets.extend(targets)\n",
    "    \n",
    "    # Calculate metrics for different IoU thresholds\n",
    "    metrics = {}\n",
    "    for iou_threshold in iou_thresholds:\n",
    "        mAP = calculate_mAP(all_detections, all_targets, iou_threshold)\n",
    "        metrics[f'mAP@{int(iou_threshold*100)}'] = mAP\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def non_maximum_suppression(boxes, scores, threshold=0.5):\n",
    "    \"\"\"Apply Non-Maximum Suppression\"\"\"\n    "    # Convert to numpy if needed\n",
    "    if isinstance(boxes, torch.Tensor):\n",
    "        boxes = boxes.cpu().numpy()\n",
    "    if isinstance(scores, torch.Tensor):\n",
    "        scores = scores.cpu().numpy()\n    "    \n    "    # Sort by score\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    \n",
    "    keep = []\n",
    "    while len(indices) > 0:\n",
    "        current = indices[0]\n",
    "        keep.append(current)\n",
    "        \n",
    "        if len(indices) == 1:\n",
    "            break\n",
    "        \n",
    "        # Calculate IoU with remaining boxes\n",
    "        current_box = boxes[current]\n",
    "        remaining_boxes = boxes[indices[1:]]\n",
    "        \n",
    "        ious = calculate_iou_single(current_box, remaining_boxes)\n",
    "        \n",
    "        # Keep boxes with IoU below threshold\n",
    "        mask = ious < threshold\n",
    "        indices = indices[1:][mask]\n",
    "    \n",
    "    return keep\n",
    "\n",
    "def calculate_iou_single(box1, boxes2):\n",
    "    \"\"\"Calculate IoU between one box and multiple boxes\"\"\"\n    "    x1 = np.maximum(box1[0], boxes2[:, 0])\n",
    "    y1 = np.maximum(box1[1], boxes2[:, 1])\n",
    "    x2 = np.minimum(box1[2], boxes2[:, 2])\n",
    "    y2 = np.minimum(box1[3], boxes2[:, 3])\n",
    "    \n",
    "    intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / (union + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_example():\n",
    "    \"\"\"Main training example for object detection\"\"\"\n    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        'data_dir': 'path/to/your/dataset',  # Replace with your data path\n",
    "        'model_type': 'faster_rcnn',  # 'faster_rcnn' or 'yolo'\n",
    "        'num_classes': 20,  # Pascal VOC has 20 classes\n",
    "        'batch_size': 2,  # Small batch size due to memory constraints\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-4,\n",
    "        'save_dir': 'checkpoints/object_detection'\n",
    "    }\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = PascalVOCDataset(\n",
    "        os.path.join(config['data_dir'], 'train'),\n",
    "        transforms=get_transforms(train=True)\n",
    "    )\n",
    "    \n",
    "    val_dataset = PascalVOCDataset(\n",
    "        os.path.join(config['data_dir'], 'val'),\n",
    "        transforms=get_transforms(train=False)\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(train_dataset)} training, {len(val_dataset)} validation samples\")\n",
    "    print(f\"Classes: {train_dataset.classes}\")\n",
    "    \n",
    "    # Create model\n",
    "    if config['model_type'] == 'faster_rcnn':\n",
    "        model = create_faster_rcnn_model(config['num_classes'], pretrained=True)\n",
    "    else:\n",
    "        model = YOLOv3(num_classes=config['num_classes'])\n",
    "    \n",
    "    print(f\"\\nModel: {config['model_type']}\")\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ObjectDetectionTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        save_dir=config['save_dir']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train(\n",
    "        num_epochs=config['num_epochs'],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(trainer.history)\n",
    "    \n",
    "    # Visualize some results\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images, targets = next(iter(val_loader))\n",
    "        images = [img.to(device) for img in images]\n",
    "        predictions = model(images)\n",
    "        \n",
    "        visualize_detections(\n",
    "            images[:4],\n",
    "            predictions[:4],\n",
    "            targets[:4],\n",
    "            class_names=train_dataset.classes\n",
    "        )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Run the example (uncomment to execute)\n",
    "# trainer = main_training_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector:\n",
    "    \"\"\"Object detector for inference\"\"\"\n    "    def __init__(self, model_path, class_names, device='cpu', score_threshold=0.5, nms_threshold=0.5):\n",
    "        self.device = torch.device(device)\n",
    "        self.class_names = class_names\n",
    "        self.score_threshold = score_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        \n",
    "        # Load model\n",
    "        self.model = self.load_model(model_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Setup preprocessing\n",
    "        self.transform = get_transforms(train=False)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Load trained model\"\"\"\n    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # Recreate model architecture\n",
    "        num_classes = len(self.class_names) + 1  # +1 for background\n",
    "        model = create_faster_rcnn_model(num_classes, pretrained=False)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def detect(self, image_path):\n",
    "        \"\"\"Detect objects in a single image\"\"\"\n    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Detect objects\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(input_tensor)[0]\n",
    "        \n",
    "        # Filter by score\n",
    "        mask = prediction['scores'] > self.score_threshold\n",
    "        boxes = prediction['boxes'][mask].cpu().numpy()\n",
    "        scores = prediction['scores'][mask].cpu().numpy()\n",
    "        labels = prediction['labels'][mask].cpu().numpy()\n",
    "        \n",
    "        # Apply NMS\n",
    "        keep = non_maximum_suppression(boxes, scores, self.nms_threshold)\n",
    "        \n",
    "        # Format results\n",
    "        detections = []\n",
    "        for idx in keep:\n",
    "            detections.append({\n",
    "                'bbox': boxes[idx].tolist(),\n",
    "                'score': scores[idx],\n",
    "                'class': self.class_names[labels[idx] - 1]  # -1 to account for background\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'detections': detections,\n",
    "            'image_size': image.size\n",
    "        }\n",
    "    \n",
    "    def detect_batch(self, image_paths):\n",
    "        \"\"\"Detect objects in batch of images\"\"\"\n    "        results = []\n",
    "        \n",
    "        for image_path in image_paths:\n",
    "            result = self.detect(image_path)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_detection(self, image_path, save_path=None):\n",
    "        \"\"\"Visualize detection results\"\"\"\n    "        result = self.detect(image_path)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for detection in result['detections']:\n",
    "            x1, y1, x2, y2 = detection['bbox']\n",
    "            score = detection['score']\n",
    "            class_name = detection['class']\n",
    "            \n",
    "            # Draw rectangle\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                   linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            ax.text(x1, y1-5, f'{class_name} {score:.2f}',\n",
    "                   color='red', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.set_title(f'Detected {len(result[\"detections\"])} objects')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return result\n",
    "\n",
    "def benchmark_detector(detector, test_images, batch_size=1):\n",
    "    \"\"\"Benchmark detector performance\"\"\"\n    "    import time\n",
    "    \n",
    "    total_time = 0\n",
    "    total_detections = 0\n",
    "    \n",
    "    print(f\"Benchmarking detector on {len(test_images)} images...\")\n",
    "    \n",
    "    for i in range(0, len(test_images), batch_size):\n",
    "        batch_images = test_images[i:i+batch_size]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = detector.detect_batch(batch_images)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        total_time += (end_time - start_time)\n",
    "        total_detections += sum(len(result['detections']) for result in results)\n",
    "    \n",
    "    avg_inference_time = total_time / len(test_images)\n",
    "    fps = len(test_images) / total_time\n",
    "    avg_detections_per_image = total_detections / len(test_images)\n",
    "    \n",
    "    print(f\"\\nBenchmark Results:\")\n",
    "    print(f\"Average inference time: {avg_inference_time*1000:.2f} ms per image\")\n",
    "    print(f\"Frames per second: {fps:.2f}\")\n",
    "    print(f\"Average detections per image: {avg_detections_per_image:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'avg_inference_time_ms': avg_inference_time * 1000,\n",
    "        'fps': fps,\n",
    "        'avg_detections_per_image': avg_detections_per_image\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Techniques Covered:\n",
    "1. **Dataset Preparation**: COCO and Pascal VOC format datasets\n",
    "2. **Model Architectures**: YOLOv3 and Faster R-CNN implementations\n",
    "3. **Training Pipeline**: Complete training with region proposals and detection heads\n",
    "4. **Evaluation**: mAP calculation and comprehensive metrics\n",
    "5. **Visualization**: Detection results and training history\n",
    "6. **Inference**: Deployment-ready object detector\n",
    "7. **Optimization**: Non-maximum suppression and benchmarking\n",
    "\n",
    "### Performance Considerations:\n",
    "- **YOLO**: Faster inference, real-time capable, slightly lower accuracy\n",
    "- **Faster R-CNN**: Higher accuracy, slower inference, two-stage approach\n",
    "- **Memory Usage**: Object detection is memory intensive, use small batch sizes\n",
    "- **Data Augmentation**: Crucial for good generalization\n",
    "- **Anchor Boxes**: Proper anchor configuration affects performance significantly\n",
    "\n",
    "### Best Practices:\n",
    "- Use pre-trained backbones when possible\n",
    "- Implement proper data augmentation\n",
    "- Monitor training losses separately (RPN vs detector)\n",
    "- Use early stopping based on validation mAP\n",
    "- Apply NMS for cleaner results\n",
    "- Benchmark inference speed for deployment\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different backbone architectures\n",
    "- Implement more advanced augmentation techniques\n",
    "- Try ensemble methods for better accuracy\n",
    "- Add model quantization for faster inference\n",
    "- Deploy to edge devices or cloud services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}