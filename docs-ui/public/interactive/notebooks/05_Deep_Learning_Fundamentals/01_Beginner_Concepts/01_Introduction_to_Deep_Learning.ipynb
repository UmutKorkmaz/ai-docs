{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "\n",
    "**Interactive Notebook** - Section 5: Deep Learning Fundamentals\n",
    "\n",
    "Welcome to the fascinating world of Deep Learning! This notebook will guide you through the fundamental concepts of neural networks and deep learning with hands-on exercises and interactive visualizations.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand the basic concepts of neural networks\n",
    "- ‚úÖ Learn about perceptrons, activation functions, and backpropagation\n",
    "- ‚úÖ Build your first neural networks using TensorFlow and PyTorch\n",
    "- ‚úÖ Implement deep learning models for real-world problems\n",
    "- ‚úÖ Understand training techniques and optimization\n",
    "- ‚úÖ Evaluate and tune deep learning models\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Completion of \"Introduction to Machine Learning\" notebook\n",
    "- Understanding of basic ML concepts (supervised learning, model evaluation)\n",
    "- Basic calculus and linear algebra knowledge\n",
    "- Python programming skills\n",
    "\n",
    "**Estimated Time**: 3-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Installation\n",
    "\n",
    "Let's set up our environment with the necessary deep learning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow torch torchvision matplotlib seaborn numpy pandas scikit-learn ipywidgets\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import deep learning frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Deep learning environment setup complete!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† What is Deep Learning?\n",
    "\n",
    "Deep Learning is a subset of machine learning based on artificial neural networks. These networks are inspired by the human brain and can learn complex patterns from large amounts of data.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Neurons**: Basic computational units that receive inputs, apply weights, and produce outputs\n",
    "2. **Layers**: Collections of neurons (input, hidden, output layers)\n",
    "3. **Activation Functions**: Non-linear functions that enable neural networks to learn complex patterns\n",
    "4. **Backpropagation**: Algorithm for training neural networks by updating weights\n",
    "5. **Gradient Descent**: Optimization algorithm to minimize the loss function\n",
    "\n",
    "### Types of Neural Networks:\n",
    "- **Feedforward Neural Networks (FNN)**: Basic neural network architecture\n",
    "- **Convolutional Neural Networks (CNN)**: Specialized for image processing\n",
    "- **Recurrent Neural Networks (RNN)**: Specialized for sequential data\n",
    "- **Transformers**: State-of-the-art architecture for NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neural network architectures\n",
    "def visualize_neural_networks():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Neural Network Architectures', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Single Perceptron\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter([0], [0], s=200, c='lightblue', marker='o', label='Input')\n",
    "    ax.scatter([1], [0], s=200, c='lightgreen', marker='o', label='Neuron')\n",
    "    ax.scatter([2], [0], s=200, c='lightcoral', marker='o', label='Output')\n",
    "    ax.arrow(0.15, 0, 0.7, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "    ax.arrow(1.15, 0, 0.7, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "    ax.text(0, -0.3, 'x‚ÇÅ', ha='center', fontsize=12)\n",
    "    ax.text(1, -0.3, 'w‚ÇÅ', ha='center', fontsize=12)\n",
    "    ax.text(2, -0.3, 'y', ha='center', fontsize=12)\n",
    "    ax.set_xlim(-0.5, 2.5)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.set_title('Single Perceptron')\n",
    "    ax.legend()\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Multi-layer Perceptron\n",
    "    ax = axes[0, 1]\n",
    "    layers = [3, 4, 2]  # Input, hidden, output\n",
    "    layer_positions = [0, 1, 2]\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "    for i, (n_neurons, x_pos, color) in enumerate(zip(layers, layer_positions, colors)):\n",
    "        y_positions = np.linspace(-1, 1, n_neurons)\n",
    "        for y_pos in y_positions:\n",
    "            ax.scatter(x_pos, y_pos, s=200, c=color, marker='o')\n",
    "\n",
    "        # Draw connections\n",
    "        if i < len(layers) - 1:\n",
    "            next_y_positions = np.linspace(-1, 1, layers[i + 1])\n",
    "            for y1 in y_positions:\n",
    "                for y2 in next_y_positions:\n",
    "                    ax.arrow(x_pos + 0.1, y1, 0.8, y2 - y1, \n",
    "                            head_width=0.02, head_length=0.05, \n",
    "                            fc='gray', ec='gray', alpha=0.3)\n",
    "\n",
    "    ax.set_xlim(-0.5, 2.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_title('Multi-layer Perceptron')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # CNN Architecture\n",
    "    ax = axes[1, 0]\n",
    "    # Input image\n",
    "    ax.add_patch(plt.Rectangle((0, 0), 0.5, 1, fill=True, color='lightblue', alpha=0.7))\n",
    "    ax.text(0.25, 1.1, 'Input\\n32x32', ha='center', fontsize=10)\n",
    "    # Convolutional layers\n",
    "    ax.add_patch(plt.Rectangle((0.8, 0), 0.4, 1, fill=True, color='lightgreen', alpha=0.7))\n",
    "    ax.text(1.0, 1.1, 'Conv\\n28x28', ha='center', fontsize=10)\n",
    "    ax.add_patch(plt.Rectangle((1.4, 0), 0.4, 1, fill=True, color='lightgreen', alpha=0.7))\n",
    "    ax.text(1.6, 1.1, 'Conv\\n24x24', ha='center', fontsize=10)\n",
    "    # Pooling\n",
    "    ax.add_patch(plt.Rectangle((2.0, 0), 0.4, 1, fill=True, color='yellow', alpha=0.7))\n",
    "    ax.text(2.2, 1.1, 'Pool\\n12x12', ha='center', fontsize=10)\n",
    "    # Fully connected\n",
    "    ax.add_patch(plt.Rectangle((2.6, 0.2), 0.4, 0.6, fill=True, color='lightcoral', alpha=0.7))\n",
    "    ax.text(2.8, 1.1, 'FC\\n128', ha='center', fontsize=10)\n",
    "    # Output\n",
    "    ax.add_patch(plt.Rectangle((3.2, 0.3), 0.4, 0.4, fill=True, color='orange', alpha=0.7))\n",
    "    ax.text(3.4, 1.1, 'Out\\n10', ha='center', fontsize=10)\n",
    "\n",
    "    ax.set_xlim(-0.2, 3.8)\n",
    "    ax.set_ylim(-0.2, 1.3)\n",
    "    ax.set_title('CNN Architecture')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # RNN Architecture\n",
    "    ax = axes[1, 1]\n",
    "    # Input sequence\n",
    "    for i in range(4):\n",
    "        ax.scatter(0, i*0.3, s=100, c='lightblue', marker='s')\n",
    "        ax.text(-0.1, i*0.3, f'x{i+1}', ha='right', va='center')\n",
    "    # RNN cells\n",
    "    for i in range(4):\n",
    "        circle = plt.Circle((0.5, i*0.3), 0.08, fill=True, color='lightgreen', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(0.5, i*0.3, 'RNN', ha='center', va='center', fontsize=8)\n",
    "        # Connections\n",
    "        if i > 0:\n",
    "            ax.arrow(0.5, (i-1)*0.3, 0, 0.25, head_width=0.02, head_length=0.02, fc='red', ec='red')\n",
    "        ax.arrow(0.15, i*0.3, 0.25, 0, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "        ax.arrow(0.58, i*0.3, 0.25, 0, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "    # Output\n",
    "    for i in range(4):\n",
    "        ax.scatter(1, i*0.3, s=100, c='lightcoral', marker='s')\n",
    "        ax.text(1.1, i*0.3, f'y{i+1}', ha='left', va='center')\n",
    "\n",
    "    ax.set_xlim(-0.3, 1.3)\n",
    "    ax.set_ylim(-0.2, 1.2)\n",
    "    ax.set_title('RNN Architecture')\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_neural_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns. Let's explore the most common activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore activation functions\n",
    "def explore_activation_functions():\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "    # Define activation functions\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def leaky_relu(x, alpha=0.01):\n",
    "        return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "    def elu(x, alpha=1.0):\n",
    "        return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Activation Functions', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Sigmoid\n",
    "    ax = axes[0, 0]\n",
    "    y = sigmoid(x)\n",
    "    ax.plot(x, y, 'b-', linewidth=2, label='sigmoid(x)')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Sigmoid')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('sigmoid(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    # Tanh\n",
    "    ax = axes[0, 1]\n",
    "    y = tanh(x)\n",
    "    ax.plot(x, y, 'g-', linewidth=2, label='tanh(x)')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Hyperbolic Tangent (tanh)')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('tanh(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    # ReLU\n",
    "    ax = axes[0, 2]\n",
    "    y = relu(x)\n",
    "    ax.plot(x, y, 'r-', linewidth=2, label='ReLU(x)')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Rectified Linear Unit (ReLU)')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('ReLU(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    # Leaky ReLU\n",
    "    ax = axes[1, 0]\n",
    "    y = leaky_relu(x)\n",
    "    ax.plot(x, y, 'm-', linewidth=2, label='Leaky ReLU(x)')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Leaky ReLU')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Leaky ReLU(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    # ELU\n",
    "    ax = axes[1, 1]\n",
    "    y = elu(x)\n",
    "    ax.plot(x, y, 'c-', linewidth=2, label='ELU(x)')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Exponential Linear Unit (ELU)')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('ELU(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    # Derivatives\n",
    "    ax = axes[1, 2]\n",
    "    dy_sigmoid = sigmoid(x) * (1 - sigmoid(x))\n",
    "    dy_tanh = 1 - tanh(x)**2\n",
    "    dy_relu = np.where(x > 0, 1, 0)\n",
    "\n",
    "    ax.plot(x, dy_sigmoid, 'b--', alpha=0.7, label=\"sigmoid'\")\n",
    "    ax.plot(x, dy_tanh, 'g--', alpha=0.7, label=\"tanh'\")\n",
    "    ax.plot(x, dy_relu, 'r--', alpha=0.7, label=\"ReLU'\")\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Derivatives')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel(\"f'(x)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print properties\n",
    "    print(\"üìä Activation Function Properties:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Function':<15} {'Range':<10} {'Vanishing':<10} {'Computing':<12}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Sigmoid':<15} {'(0,1)':<10} {'Yes':<10} {'Expensive':<12}\")\n",
    "    print(f\"{'Tanh':<15} {'(-1,1)':<10} {'Yes':<10} {'Expensive':<12}\")\n",
    "    print(f\"{'ReLU':<15} {'[0,‚àû)':<10} {'No':<10} {'Cheap':<12}\")\n",
    "    print(f\"{'Leaky ReLU':<15} {'(-‚àû,‚àû)':<10} {'No':<10} {'Cheap':<12}\")\n",
    "    print(f\"{'ELU':<15} {'(-Œ±,‚àû)':<10} {'No':<10} {'Expensive':<12}\")\n",
    "\n",
    "explore_activation_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Building Your First Neural Network with TensorFlow\n",
    "\n",
    "Let's build our first neural network using TensorFlow/Keras to solve a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "def generate_complex_dataset():\n",
    "    # Generate different types of datasets\n",
    "    X_moons, y_moons = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "    X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=42)\n",
    "    X_class, y_class = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                                          n_informative=2, random_state=42, n_clusters_per_class=1)\n",
    "\n",
    "    # Visualize datasets\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('Synthetic Datasets for Neural Network Training', fontsize=16, fontweight='bold')\n",
    "\n",
    "    datasets = [(X_moons, y_moons, 'Moons'), (X_circles, y_circles, 'Circles'), (X_class, y_class, 'Linear')]\n",
    "\n",
    "    for i, (X, y, title) in enumerate(datasets):\n",
    "        ax = axes[i]\n",
    "        scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return X_moons, y_moons  # We'll use the moons dataset for our first model\n",
    "\n",
    "X, y = generate_complex_dataset()\n",
    "\n",
    "# Split and scale the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Dataset prepared:\")\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TensorFlow/Keras neural network\n",
    "def build_tensorflow_model(input_shape, hidden_layers=[64, 32], activation='relu'):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation=activation))\n",
    "        model.add(layers.Dropout(0.2))  # Regularization\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = build_tensorflow_model(input_shape=(2,), hidden_layers=[64, 32, 16])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "print(\"üèóÔ∏è Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nüöÄ Training the model...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                          np.arange(y_min, y_max, 0.1))\n",
    "    \n",
    "    # Make predictions\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.8, edgecolors='black')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.colorbar(label='Class Probability')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X_test_scaled, y_test, \"Neural Network Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Building the Same Model with PyTorch\n",
    "\n",
    "Now let's implement the same neural network using PyTorch to understand the differences between the frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Neural Network Implementation\n",
    "class PyTorchNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[64, 32, 16]):\n",
    "        super(PyTorchNN, self).__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "pytorch_model = PyTorchNN(input_size=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"üî• PyTorch Model Architecture:\")\n",
    "print(pytorch_model)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "print(\"\\nüöÄ Training PyTorch model...\")\n",
    "for epoch in range(epochs):\n",
    "    pytorch_model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = pytorch_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PyTorch model\n",
    "pytorch_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = pytorch_model(X_test_tensor)\n",
    "    test_predictions = (test_outputs > 0.5).float()\n",
    "    test_accuracy = (test_predictions == y_test_tensor).float().mean()\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "\n",
    "print(f\"\\nüìä PyTorch Test Results:\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.set_title('PyTorch Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accuracies, label='Training Accuracy')\n",
    "ax2.set_title('PyTorch Training Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare TensorFlow vs PyTorch results\n",
    "print(\"\\nüèÜ Framework Comparison:\")\n",
    "print(f\"TensorFlow Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"PyTorch Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"\\nüìù Key Differences:\")\n",
    "print(\"‚Ä¢ TensorFlow: Higher-level API, easier for beginners\")\n",
    "print(\"‚Ä¢ PyTorch: More flexible, preferred for research\")\n",
    "print(\"‚Ä¢ TensorFlow: Better production deployment\")\n",
    "print(\"‚Ä¢ PyTorch: More Pythonic, dynamic computation graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Interactive Exercise: Hyperparameter Tuning\n",
    "\n",
    "Let's experiment with different hyperparameters and see how they affect model performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive hyperparameter tuning\n",
    "def interactive_hyperparameter_tuning():\n",
    "    # Create widgets\n",
    "    learning_rate_slider = widgets.FloatLogSlider(\n",
    "        value=0.001, base=10, min=-4, max=-1, step=0.1,\n",
    "        description='Learning Rate:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    hidden_units_slider = widgets.IntSlider(\n",
    "        value=32, min=8, max=128, step=8,\n",
    "        description='Hidden Units:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    layers_slider = widgets.IntSlider(\n",
    "        value=2, min=1, max=4, step=1,\n",
    "        description='Hidden Layers:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    dropout_slider = widgets.FloatSlider(\n",
    "        value=0.2, min=0.0, max=0.5, step=0.05,\n",
    "        description='Dropout Rate:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    epochs_slider = widgets.IntSlider(\n",
    "        value=50, min=10, max=200, step=10,\n",
    "        description='Epochs:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    def train_model_with_params(learning_rate, hidden_units, layers, dropout, epochs):\n",
    "        # Build model with specified parameters\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.InputLayer(input_shape=(2,)))\n",
    "        \n",
    "        for i in range(layers):\n",
    "            model.add(layers.Dense(hidden_units, activation='relu'))\n",
    "            if dropout > 0:\n",
    "                model.add(layers.Dropout(dropout))\n",
    "        \n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # Compile with custom learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüéØ Results with LR={learning_rate:.4f}, Hidden={hidden_units}, Layers={layers}, Dropout={dropout:.2f}, Epochs={epochs}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "        \n",
    "        # Plot training history\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.plot(history.history['accuracy'], label='Training')\n",
    "        ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.plot(history.history['loss'], label='Training')\n",
    "        ax2.plot(history.history['val_loss'], label='Validation')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Check for overfitting\n",
    "        train_acc = history.history['accuracy'][-1]\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        overfitting_gap = train_acc - val_acc\n",
    "        \n",
    "        if overfitting_gap > 0.1:\n",
    "            print(f\"‚ö†Ô∏è  Potential overfitting detected! Gap: {overfitting_gap:.4f}\")\n",
    "        elif overfitting_gap < 0:\n",
    "            print(f\"‚úÖ Model is well-fit. Gap: {overfitting_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"üìä Model performance is good. Gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "    # Create interactive widget\n",
    "    interactive_plot = widgets.interactive(\n",
    "        train_model_with_params,\n",
    "        learning_rate=learning_rate_slider,\n",
    "        hidden_units=hidden_units_slider,\n",
    "        layers=layers_slider,\n",
    "        dropout=dropout_slider,\n",
    "        epochs=epochs_slider\n",
    "    )\n",
    "    \n",
    "    display(interactive_plot)\n",
    "\n",
    "interactive_hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Hands-on Challenge\n",
    "\n",
    "Now it's your turn to apply what you've learned! Complete the following challenges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Multi-class classification\n",
    "def challenge_1():\n",
    "    print(\"üéØ Challenge 1: Multi-class Classification\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Generate multi-class dataset\n",
    "    X_multi, y_multi = make_classification(\n",
    "        n_samples=2000, n_features=4, n_classes=4, n_informative=4,\n",
    "        n_redundant=0, n_clusters_per_class=1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # TODO: Complete the following tasks\n",
    "    \n",
    "    # Task 1: Build a multi-class neural network\n",
    "    model_multi = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(4,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(4, activation='softmax')  # 4 classes with softmax\n",
    "    ])\n",
    "    \n",
    "    model_multi.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',  # For integer labels\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Task 1: Multi-class model built successfully!\")\n",
    "    print(f\"Model architecture: {model_multi.summary()}\")\n",
    "    \n",
    "    # Task 2: Train the model\n",
    "    history = model_multi.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Task 2: Model training complete!\")\n",
    "    \n",
    "    # Task 3: Evaluate the model\n",
    "    test_loss, test_accuracy = model_multi.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Task 4: Make predictions and analyze results\n",
    "    y_pred = model_multi.predict(X_test_scaled)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Multi-class Classification')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "challenge_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Regression with Neural Networks\n",
    "def challenge_2():\n",
    "    print(\"üìà Challenge 2: Neural Network Regression\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Generate regression dataset\n",
    "    from sklearn.datasets import make_regression\n",
    "    X_reg, y_reg = make_regression(\n",
    "        n_samples=2000, n_features=6, n_targets=1, noise=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reg, y_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Scale target variable\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # TODO: Complete the following tasks\n",
    "    \n",
    "    # Task 1: Build regression neural network\n",
    "    model_reg = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(6,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')  # Linear activation for regression\n",
    "    ])\n",
    "    \n",
    "    model_reg.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',  # Mean squared error\n",
    "        metrics=['mae']  # Mean absolute error\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Task 1: Regression model built successfully!\")\n",
    "    \n",
    "    # Task 2: Train the model\n",
    "    history = model_reg.fit(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Task 2: Model training complete!\")\n",
    "    \n",
    "    # Task 3: Evaluate the model\n",
    "    test_loss, test_mae = model_reg.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "    print(f\"\\nüìä Test Results (scaled):\")\n",
    "    print(f\"Test MSE: {test_loss:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}\")\n",
    "    \n",
    "    # Make predictions and inverse transform\n",
    "    y_pred_scaled = model_reg.predict(X_test_scaled)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    # Calculate metrics on original scale\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä Test Results (original scale):\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Predictions vs Actual Values')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(history.history['loss'], label='Training')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation')\n",
    "    ax1.set_title('Model Loss (MSE)')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(history.history['mae'], label='Training')\n",
    "    ax2.plot(history.history['val_mae'], label='Validation')\n",
    "    ax2.set_title('Model MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return r2\n",
    "\n",
    "challenge_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Model Optimization Experiment\n",
    "def challenge_3():\n",
    "    print(\"üîß Challenge 3: Model Optimization Experiment\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Use the circles dataset from earlier\n",
    "    X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_circles, y_circles, test_size=0.2, random_state=42, stratify=y_circles)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Different architectures to test\n",
    "    architectures = [\n",
    "        {'name': 'Shallow', 'layers': [32, 16], 'dropout': 0.1},\n",
    "        {'name': 'Medium', 'layers': [64, 32, 16], 'dropout': 0.2},\n",
    "        {'name': 'Deep', 'layers': [128, 64, 32, 16], 'dropout': 0.3},\n",
    "        {'name': 'Wide', 'layers': [256, 128], 'dropout': 0.2},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for arch in architectures:\n",
    "        print(f\"\\nüèóÔ∏è Training {arch['name']} architecture...\")\n",
    "        \n",
    "        # Build model\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.InputLayer(input_shape=(2,)))\n",
    "        \n",
    "        for units in arch['layers']:\n",
    "            model.add(layers.Dense(units, activation='relu'))\n",
    "            model.add(layers.Dropout(arch['dropout']))\n",
    "        \n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'architecture': arch['name'],\n",
    "            'layers': arch['layers'],\n",
    "            'dropout': arch['dropout'],\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'final_train_accuracy': history.history['accuracy'][-1],\n",
    "            'final_val_accuracy': history.history['val_accuracy'][-1],\n",
    "            'overfitting_gap': history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\nüìä Architecture Comparison Results:\")\n",
    "    display(df_results[['architecture', 'test_accuracy', 'test_loss', 'overfitting_gap']])\n",
    "    \n",
    "    # Find best architecture\n",
    "    best_idx = df_results['test_accuracy'].idxmax()\n",
    "    best_arch = df_results.loc[best_idx]\n",
    "    print(f\"\\nüèÜ Best Architecture: {best_arch['architecture']}\")\n",
    "    print(f\"Test Accuracy: {best_arch['test_accuracy']:.4f}\")\n",
    "    print(f\"Overfitting Gap: {best_arch['overfitting_gap']:.4f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Test accuracy comparison\n",
    "    bars1 = ax1.bar(df_results['architecture'], df_results['test_accuracy'], color='lightblue')\n",
    "    ax1.set_title('Test Accuracy by Architecture')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    for bar, acc in zip(bars1, df_results['test_accuracy']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Overfitting gap comparison\n",
    "    bars2 = ax2.bar(df_results['architecture'], df_results['overfitting_gap'], color='lightcoral')\n",
    "    ax2.set_title('Overfitting Gap by Architecture')\n",
    "    ax2.set_ylabel('Train - Val Accuracy Gap')\n",
    "    ax2.axhline(y=0.1, color='red', linestyle='--', label='Overfitting Threshold')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Test loss comparison\n",
    "    bars3 = ax3.bar(df_results['architecture'], df_results['test_loss'], color='lightgreen')\n",
    "    ax3.set_title('Test Loss by Architecture')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    \n",
    "    # Number of parameters estimation\n",
    "    param_counts = []\n",
    "    for arch in architectures:\n",
    "        # Estimate parameters: input * units + units + units * next_units + ...\n",
    "        total_params = 2 * arch['layers'][0] + arch['layers'][0]  # Input to first hidden\n",
    "        for i in range(len(arch['layers']) - 1):\n",
    "            total_params += arch['layers'][i] * arch['layers'][i + 1] + arch['layers'][i + 1]\n",
    "        total_params += arch['layers'][-1] + 1  # Last hidden to output\n",
    "        param_counts.append(total_params)\n",
    "    \n",
    "    ax4.scatter(param_counts, df_results['test_accuracy'], s=100, alpha=0.7)\n",
    "    for i, arch in enumerate(architectures):\n",
    "        ax4.annotate(arch['name'], (param_counts[i], df_results['test_accuracy'][i]),\n",
    "                     xytext=(5, 5), textcoords='offset points')\n",
    "    ax4.set_xlabel('Estimated Parameters')\n",
    "    ax4.set_ylabel('Test Accuracy')\n",
    "    ax4.set_title('Accuracy vs Model Complexity')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "challenge_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Deep Learning Fundamentals**:\n",
    "   - Neural network architectures (perceptrons, MLPs, CNNs, RNNs)\n",
    "   - Activation functions and their properties\n",
    "   - Forward propagation and backpropagation\n",
    "\n",
    "2. **Framework Proficiency**:\n",
    "   - TensorFlow/Keras implementation\n",
    "   - PyTorch implementation\n",
    "   - Understanding the differences and use cases\n",
    "\n",
    "3. **Model Building**:\n",
    "   - Binary classification with neural networks\n",
    "   - Multi-class classification\n",
    "   - Regression with neural networks\n",
    "   - Proper model architecture design\n",
    "\n",
    "4. **Training and Optimization**:\n",
    "   - Loss functions and optimizers\n",
    "   - Hyperparameter tuning\n",
    "   - Overfitting prevention (dropout, regularization)\n",
    "   - Model evaluation and validation\n",
    "\n",
    "5. **Practical Skills**:\n",
    "   - Data preprocessing for neural networks\n",
    "   - Training history visualization\n",
    "   - Decision boundary visualization\n",
    "   - Model comparison and selection\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- **Continue to next notebook**: \"Advanced Deep Learning Techniques\" for CNNs, RNNs, and transformers\n",
    "- **Practice with real datasets**: MNIST, CIFAR-10, ImageNet\n",
    "- **Learn about transfer learning**: Using pre-trained models\n",
    "- **Explore advanced topics**: Attention mechanisms, generative models\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- [Deep Learning by Ian Goodfellow](https://www.deeplearningbook.org/)\n",
    "- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "**Congratulations on completing your first Deep Learning notebook! üéâ**\n",
    "You've taken a significant step into the exciting world of neural networks and deep learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}