{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamentals of prompt engineering\n",
    "- Learn effective prompt design principles\n",
    "- Master different prompting techniques\n",
    "- Apply prompt engineering to various tasks\n",
    "- Evaluate and optimize prompt performance\n",
    "\n",
    "**Expected Duration:** 60-90 minutes\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic understanding of Large Language Models\n",
    "- Familiarity with Python programming\n",
    "- Access to LLM API (OpenAI, Anthropic, or local models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Prompt Engineering?\n",
    "\n",
    "Prompt engineering is the art and science of designing effective input prompts to guide Large Language Models (LLMs) to produce desired outputs. It's a critical skill for working with AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up LLM Access\n",
    "\n",
    "### 2.1 API Configuration\n",
    "Let's set up access to different LLM providers. You can use any of the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Interface Class\n",
    "class LLMInterface:\n",
    "    \"\"\"Unified interface for different LLM providers\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"openai\", model: str = None, api_key: str = None):\n",
    "        self.provider = provider\n",
    "        self.model = model or self._get_default_model(provider)\n",
    "        self.api_key = api_key or os.getenv(f\"{provider.upper()}_API_KEY\")\n",
    "        self.base_url = self._get_base_url(provider)\n",
    "        \n",
    "    def _get_default_model(self, provider: str) -> str:\n",
    "        \"\"\"Get default model for provider\"\"\"\n",
    "        defaults = {\n",
    "            \"openai\": \"gpt-3.5-turbo\",\n",
    "            \"anthropic\": \"claude-3-sonnet-20240229\",\n",
    "            \"local\": \"llama2\"\n",
    "        }\n",
    "        return defaults.get(provider, \"gpt-3.5-turbo\")\n",
    "    \n",
    "    def _get_base_url(self, provider: str) -> str:\n",
    "        \"\"\"Get base URL for provider\"\"\"\n",
    "        urls = {\n",
    "            \"openai\": \"https://api.openai.com/v1\",\n",
    "            \"anthropic\": \"https://api.anthropic.com\",\n",
    "            \"local\": \"http://localhost:8000\"\n",
    "        }\n",
    "        return urls.get(provider, \"https://api.openai.com/v1\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> str:\n",
    "        \"\"\"Generate response from LLM\"\"\"\n",
    "        if self.provider == \"openai\":\n",
    "            return self._generate_openai(prompt, temperature, max_tokens)\n",
    "        elif self.provider == \"anthropic\":\n",
    "            return self._generate_anthropic(prompt, temperature, max_tokens)\n",
    "        else:\n",
    "            return self._generate_local(prompt, temperature, max_tokens)\n",
    "    \n",
    "    def _generate_openai(self, prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using OpenAI API\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def _generate_anthropic(self, prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Anthropic API\"\"\"\n",
    "        headers = {\n",
    "            \"x-api-key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"anthropic-version\": \"2023-06-01\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/messages\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()['content'][0]['text']\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def _generate_local(self, prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using local model (placeholder)\"\"\"\n",
    "        # This would integrate with local LLM setup\n",
    "        return \"Local model response would appear here. Set up your local model integration.\"\n",
    "\n",
    "# Mock LLM for demonstration (if no API key available)\n",
    "class MockLLM:\n",
    "    \"\"\"Mock LLM for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.responses = {\n",
    "            \"basic\": \"This is a mock response for demonstration purposes.\",\n",
    "            \"analysis\": \"Based on the analysis, here are the key findings...\",\n",
    "            \"creative\": \"Here's a creative response to your prompt...\",\n",
    "            \"code\": \"```python\\nprint('Hello, World!')\\n```\",\n",
    "            \"summary\": \"This is a summary of the key points discussed...\"\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> str:\n",
    "        \"\"\"Generate mock response based on prompt content\"\"\"\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        if \"analyze\" in prompt_lower or \"analysis\" in prompt_lower:\n",
    "            return self.responses[\"analysis\"]\n",
    "        elif \"creative\" in prompt_lower or \"story\" in prompt_lower:\n",
    "            return self.responses[\"creative\"]\n",
    "        elif \"code\" in prompt_lower or \"python\" in prompt_lower:\n",
    "            return self.responses[\"code\"]\n",
    "        elif \"summary\" in prompt_lower or \"summarize\" in prompt_lower:\n",
    "            return self.responses[\"summary\"]\n",
    "        else:\n",
    "            return self.responses[\"basic\"]\n",
    "\n",
    "# Initialize LLM (use mock if no API key)\n",
    "try:\n",
    "    llm = LLMInterface(\"openai\")\n",
    "    # Test if API key is available\n",
    "    if not llm.api_key:\n",
    "        raise ValueError(\"No API key found\")\n",
    "except:\n",
    "    print(\"No API key found. Using mock LLM for demonstration.\")\n",
    "    print(\"To use real LLMs, set your API key as an environment variable.\")\n",
    "    llm = MockLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Prompting Principles\n",
    "\n",
    "### 3.1 Clear and Specific Instructions\n",
    "Let's explore how specificity affects LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vague vs specific prompts\n",
    "prompt_examples = {\n",
    "    \"vague\": \"Tell me about machine learning\",\n",
    "    \"specific\": \"Explain supervised learning in machine learning, including its definition, key algorithms, and real-world applications\"\n",
    "    \"structured\": \"Please explain supervised learning in machine learning. Your response should include:\n",
    "1. Definition and core concept\n",
    "2. Key algorithms with examples\n",
    "3. Real-world applications in different industries\n",
    "4. Advantages and limitations\n",
    "5. Current trends and future directions\"\n",
    "}\n",
    "\n",
    "# Test different prompts\n",
    "results = {}\n",
    "for name, prompt in prompt_examples.items():\n",
    "    response = llm.generate_response(prompt)\n",
    "    results[name] = {\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'word_count': len(response.split())\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Prompt Type: {name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nResponse ({results[name]['word_count']} words):\")\n",
    "    print(response)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "\n",
    "# Visualize prompt effectiveness\n",
    "plt.figure(figsize=(12, 6))\n",
    "effectiveness_data = {\n",
    "    'Prompt Type': list(results.keys()),\n",
    "    'Response Length': [results[k]['word_count'] for k in results.keys()]\n",
    "}\n",
    "\n",
    "df_effectiveness = pd.DataFrame(effectiveness_data)\n",
    "sns.barplot(data=df_effectiveness, x='Prompt Type', y='Response Length')\n",
    "plt.title('Prompt Specificity vs Response Length')\n",
    "plt.ylabel('Word Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Role Setting\n",
    "Assigning roles to the LLM can significantly improve response quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role-based prompting examples\n",
    "role_prompts = {\n",
    "    \"no_role\": \"Explain quantum computing\",\n",
    "    \"expert\": \"You are a quantum computing expert with 20 years of experience. Explain quantum computing to someone with a technical background but no quantum physics knowledge.\",\n",
    "    \"teacher\": \"You are a high school physics teacher. Explain quantum computing to your students in simple terms they can understand.\",\n",
    "    \"journalist\": \"You are a technology journalist writing for a mainstream audience. Explain quantum computing and its potential impact on society.\"\n",
    "}\n",
    "\n",
    "role_results = {}\n",
    "for role, prompt in role_prompts.items():\n",
    "    response = llm.generate_response(prompt)\n",
    "    role_results[role] = {\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'sentences': len(response.split('.'))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ROLE: {role.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nResponse:\")\n",
    "    print(response)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Analyze role effectiveness\n",
    "role_analysis = pd.DataFrame({\n",
    "    'Role': list(role_results.keys()),\n",
    "    'Complexity': [len(r['response'].split()) for r in role_results.values()],\n",
    "    'Structure': [r['sentences'] for r in role_results.values()]\n",
    "})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.barplot(data=role_analysis, x='Role', y='Complexity', ax=ax1)\n",
    "ax1.set_title('Response Complexity by Role')\n",
    "ax1.set_ylabel('Word Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.barplot(data=role_analysis, x='Role', y='Structure', ax=ax2)\n",
    "ax2.set_title('Response Structure by Role')\n",
    "ax2.set_ylabel('Sentence Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Prompting Techniques\n",
    "\n",
    "### 4.1 Chain-of-Thought (CoT) Prompting\n",
    "Guiding the model to think step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought prompting examples\n",
    "cot_examples = {\n",
    "    \"direct\": \"What is 25 * 15 + 37?\",\n",
    "    "cot\": \"Solve this step by step: What is 25 * 15 + 37? Show your work and explain each step.\",\n",
    "    "cot_few_shot\": \"Example: Solve step by step: 12 * 8 + 5\\n\" +\n",
    "                   \"Step 1: 12 * 8 = 96\\n\" +\n",
    "                   \"Step 2: 96 + 5 = 101\\n\" +\n",
    "                   \"Final answer: 101\\n\\n\" +\n",
    "                   \"Now solve: 25 * 15 + 7. Show your work step by step.\"\n",
    "}\n",
    "\n",
    "cot_results = {}\n",
    "for technique, prompt in cot_examples.items():\n",
    "    response = llm.generate_response(prompt)\n",
    "    cot_results[technique] = {\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'has_steps': 'step' in response.lower() or 'step' in response.lower()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TECHNIQUE: {technique.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nResponse:\")\n",
    "    print(response)\n",
    "    print(f\"\\nContains step-by-step reasoning: {cot_results[technique]['has_steps']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Visualize CoT effectiveness\n",
    "cot_analysis = pd.DataFrame({\n",
    "    'Technique': list(cot_results.keys()),\n",
    "    'Has Steps': [int(r['has_steps']) for r in cot_results.values()],\n",
    "    'Response Length': [len(r['response'].split()) for r in cot_results.values()]\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "cot_analysis.set_index('Technique')[['Has Steps', 'Response Length']].plot(kind='bar', ax=ax)\n",
    "ax.set_title('Chain-of-Thought Prompting Effectiveness')\n",
    "ax.set_ylabel('Score / Length')\n",
    "ax.legend(['Has Step-by-Step Reasoning', 'Response Length'])\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Few-Shot Learning\n",
    "Providing examples to guide the model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot learning examples\n",
    "few_shot_examples = {\n",
    "    \"zero_shot\": \"Classify the sentiment of this review: 'The movie was absolutely fantastic!'\",\n",
    "    \"few_shot\": \"Classify the sentiment of these reviews:\n\n\" +\n",
    "               \"Review: 'This product is terrible, I want my money back!'\\n\" +\n",
    "               \"Sentiment: Negative\\n\\n\" +\n",
    "               \"Review: 'The service was okay, nothing special.'\\n\" +\n",
    "               \"Sentiment: Neutral\\n\\n\" +\n",
    "               \"Review: 'I love this new restaurant, the food is amazing!'\\n\" +\n",
    "               \"Sentiment: Positive\\n\\n\" +\n",
    "               \"Now classify: 'The movie was absolutely fantastic!'\"\n",
    "    \"structured_few_shot\": \"Task: Classify sentiment as Positive, Negative, or Neutral\\n\" +\n",
    "                         \"Format: Review -> Sentiment\\n\\n\" +\n",
    "                         \"Examples:\\n\" +\n",
    "                         \"'This product broke after one day' -> Negative\\n\" +\n",
    "                         \"'It was an average experience' -> Neutral\\n\" +\n",
    "                         \"'Best purchase I've made this year!' -> Positive\\n\\n\" +\n",
    "                         \"Task: 'The movie was absolutely fantastic!' -> \"\n",
    "}\n",
    "\n",
    "few_shot_results = {}\n",
    "for technique, prompt in few_shot_examples.items():\n",
    "    response = llm.generate_response(prompt)\n",
    "    few_shot_results[technique] = {\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'has_sentiment': any(word in response.lower() for word in ['positive', 'negative', 'neutral']),\n",
    "        'has_format': '->' in response or ':' in response\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TECHNIQUE: {technique.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nResponse:\")\n",
    "    print(response)\n",
    "    print(f\"\\nContains sentiment classification: {few_shot_results[technique]['has_sentiment']}\")\n",
    "    print(f\"Follows format: {few_shot_results[technique]['has_format']}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Templates and Patterns\n",
    "\n",
    "### 5.1 Common Prompt Patterns\n",
    "Let's explore reusable prompt patterns for different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template library\n",
    "class PromptTemplateLibrary:\n",
    "    \"\"\"Collection of reusable prompt templates\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def summarization_template(text: str, length: str = \"medium\") -> str:\n",
    "        \"\"\"Template for text summarization\"\"\"\n",
    "        length_guidelines = {\n",
    "            \"short\": \"1-2 sentences capturing the main point\",\n",
    "            \"medium\": \"3-5 sentences covering key points\",\n",
    "            \"detailed\": \"comprehensive summary preserving important details\"\n",
    "        }\n",
    "        \n",
    "        return f\"\"\"Please summarize the following text in {length} format: {length_guidelines[length]}\n\nText to summarize:\n{text}\n\nSummary:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def classification_template(text: str, categories: List[str]) -> str:\n",
    "        \"\"\"Template for text classification\"\"\"\n",
    "        categories_str = \"\\n\".join([f\"- {cat}\" for cat in categories])\n",
    "        return f\"\"\"Classify the following text into one of these categories:\n{categories_str}\n\nText: {text}\n\nCategory:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def code_generation_template(task: str, language: str, requirements: List[str]) -> str:\n",
    "        \"\"\"Template for code generation\"\"\"\n",
    "        req_str = \"\\n\".join([f\"- {req}\" for req in requirements])\n",
    "        return f\"\"\"Generate {language} code for the following task:\n\nTask: {task}\n\nRequirements:\n{req_str}\n\nCode:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def creative_writing_template(prompt: str, style: str, length: str) -> str:\n",
    "        \"\"\"Template for creative writing\"\"\"\n",
    "        return f\"\"\"Write a {style} piece of approximately {length} words based on this prompt:\n\nPrompt: {prompt}\n\n{style} writing:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analysis_template(data: str, analysis_type: str) -> str:\n",
    "        \"\"\"Template for data analysis\"\"\"\n",
    "        return f\"\"\"Analyze the following data for {analysis_type}:\n\nData: {data}\n\nAnalysis:\n1. Key observations:\n2. Patterns identified:\n3. Insights and recommendations:\"\"\"\n",
    "\n",
    "# Test prompt templates\n",
    "template_examples = [\n",
    "    {\n",
    "        \"template\": PromptTemplateLibrary.summarization_template,\n",
    "        \"args\": {\n",
    "            \"text\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on those patterns.\",\n",
    "            \"length\": \"medium\"\n",
    "        },\n",
    "        \"name\": \"Summarization\"\n",
    "    },\n",
    "    {\n",
    "        \"template\": PromptTemplateLibrary.classification_template,\n",
    "        \"args\": {\n",
    "            \"text\": \"I absolutely love this new phone! The camera quality is amazing and the battery lasts all day.\",\n",
    "            \"categories\": [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "        },\n",
    "        \"name\": \"Classification\"\n",
    "    },\n",
    "    {\n",
    "        \"template\": PromptTemplateLibrary.code_generation_template,\n",
    "        \"args\": {\n",
    "            \"task\": \"Calculate factorial of a number\",\n",
    "            \"language\": \"Python\",\n",
    "            \"requirements\": [\"Handle edge cases\", \"Include docstring\", \"Add error handling\"]\n",
    "        },\n",
    "        \"name\": \"Code Generation\"\n",
    "    }\n",
    "]\n",
    "\n",
    "template_results = {}\n",
    "for example in template_examples:\n",
    "    prompt = example[\"template\"](**example[\"args\"])\n",
    "    response = llm.generate_response(prompt)\n",
    "    template_results[example[\"name\"]] = {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"template_type\": example[\"name\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEMPLATE: {example['name'].upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Generated Prompt:\")\n",
    "    print(prompt)\n",
    "    print(f\"\\nResponse:\")\n",
    "    print(response)\n",
    "    print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Optimization\n",
    "\n",
    "### 6.1 A/B Testing Prompts\n",
    "Comparing different prompt versions to find the most effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt A/B testing framework\n",
    "class PromptABTester:\n",
    "    \"\"\"Framework for testing and comparing prompt versions\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_interface):\n",
    "        self.llm = llm_interface\n",
    "        self.results = {}\n",
    "    \n",
    "    def test_prompts(self, prompts: Dict[str, str], test_cases: List[Dict], \n",
    "                    eval_criteria: List[str]) -> Dict[str, Dict]:\n",
    "        \"\"\"Test multiple prompts against test cases\"\"\"\n",
    "        \n",
    "        for prompt_name, prompt_template in prompts.items():\n",
    "            self.results[prompt_name] = {\n",
    "                'responses': [],\n",
    '                'scores': defaultdict(list),\n',
    "                'avg_scores': {}\n",
    "            }\n",
    "            \n",
    "            for case in test_cases:\n",
    "                # Format prompt with test case\n",
    "                formatted_prompt = prompt_template.format(**case)\n",
    "                \n",
    "                # Generate response\n",
    "                response = self.llm.generate_response(formatted_prompt)\n",
    "                \n",
    "                # Evaluate response\n",
    "                scores = self._evaluate_response(response, case, eval_criteria)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[prompt_name]['responses'].append({\n",
    "                    'case': case,\n",
    "                    'prompt': formatted_prompt,\n",
    "                    'response': response\n",
    "                })\n",
    "                \n",
    "                for criterion, score in scores.items():\n",
    "                    self.results[prompt_name]['scores'][criterion].append(score)\n",
    "            \n",
    "            # Calculate average scores\n",
    "            for criterion in eval_criteria:\n",
    "                scores_list = self.results[prompt_name]['scores'][criterion]\n",
    "                self.results[prompt_name]['avg_scores'][criterion] = np.mean(scores_list)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _evaluate_response(self, response: str, test_case: Dict, \n",
    "                           criteria: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate response based on criteria\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for criterion in criteria:\n",
    "            if criterion == 'length':\n",
    "                # Score based on appropriate length (target 50-200 words)\n",
    "                word_count = len(response.split())\n",
    "                if 50 <= word_count <= 200:\n",
    "                    scores[criterion] = 1.0\n",
    "                elif 30 <= word_count <= 300:\n",
    "                    scores[criterion] = 0.8\n",
    "                else:\n",
    "                    scores[criterion] = 0.5\n",
    "            \n",
    "            elif criterion == 'structure':\n",
    "                # Score based on structure (has paragraphs, lists, etc.)\n",
    "                has_paragraphs = '\\n\\n' in response\n",
    "                has_lists = any(marker in response for marker in ['-', 'â€¢', '1.', '2.'])\n",
    "                scores[criterion] = 0.5 + (0.3 if has_paragraphs else 0) + (0.2 if has_lists else 0)\n",
    "            \n",
    "            elif criterion == 'completeness':\n",
    "                # Score based on addressing all aspects of the task\n",
    "                task_keywords = test_case.get('keywords', [])\n",
    "                response_lower = response.lower()\n",
    "                keyword_matches = sum(1 for keyword in task_keywords if keyword.lower() in response_lower)\n",
    "                scores[criterion] = keyword_matches / max(len(task_keywords), 1)\n",
    "            \n",
    "            elif criterion == 'clarity':\n",
    "                # Score based on readability (simple proxy)\n",
    "                avg_sentence_length = len(response.split()) / max(response.count('.'), 1)\n",
    "                if avg_sentence_length <= 20:\n",
    "                    scores[criterion] = 1.0\n",
    "                elif avg_sentence_length <= 30:\n",
    "                    scores[criterion] = 0.8\n",
    "                else:\n",
    "                    scores[criterion] = 0.6\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize A/B test results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize. Run test_prompts first.\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data for visualization\n",
    "        comparison_data = []\n",
    "        for prompt_name, data in self.results.items():\n",
    "            for criterion, avg_score in data['avg_scores'].items():\n",
    "                comparison_data.append({\n",
    "                    'Prompt': prompt_name,\n",
    "                    'Criterion': criterion,\n",
    "                    'Score': avg_score\n",
    "                })\n",
    "        \n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.barplot(data=df_comparison, x='Criterion', y='Score', hue='Prompt', ax=ax)\n",
    "        ax.set_title('Prompt A/B Test Results')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title='Prompt Version')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display summary table\n",
    "        print(\"\\nPrompt Performance Summary:\")\n",
    "        summary_df = pd.DataFrame({\n",
    "            name: data['avg_scores'] for name, data in self.results.items()\n",
    "        }).T\n",
    "        display(summary_df)\n",
    "\n",
    "# Example A/B test\n",
    "prompts_to_test = {\n",
    "    \"basic\": \"Explain {topic} in simple terms.\",\n",
    "    \"structured\": \"Please explain {topic}. Your response should include:\\n1. Definition\\n2. Key concepts\\n3. Examples\\n4. Applications\",\n",
    "    \"role_based\": \"You are an expert educator. Explain {topic} to a beginner in a way that's engaging and easy to understand.\"\n",
    "}\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"topic\": \"blockchain technology\",\n",
    "        \"keywords\": [\"blockchain\", \"distributed\", \"ledger\", \"cryptocurrency\", \"decentralized\"]\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"machine learning\",\n",
    "        \"keywords\": [\"algorithm\", \"data\", \"training\", \"prediction\", \"pattern\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluation_criteria = [\"length\", \"structure\", \"completeness\", \"clarity\"]\n",
    "\n",
    "# Run A/B test\n",
    "ab_tester = PromptABTester(llm)\n",
    "ab_test_results = ab_tester.test_prompts(prompts_to_test, test_cases, evaluation_criteria)\n",
    "ab_tester.visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Applications\n",
    "\n",
    "### 7.1 Content Generation\n",
    "Using prompt engineering for content creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content generation examples\n",
    "content_generation_prompts = {\n",
    "    \"blog_post\": PromptTemplateLibrary.creative_writing_template(\n",
    "        \"The future of remote work and its impact on company culture\",\n",
    "        \"professional blog post\",\n",
    "        \"500\"\n",
    "    ),\n",
    "    \"product_description\": f\"\"\"You are a marketing copywriter. Write a compelling product description for a new smart home device.

Product details:
- Device: Smart Home Hub Pro
- Features: Voice control, app integration, energy monitoring
- Target audience: Tech-savvy homeowners
- Tone: Modern, professional, approachable

Product Description:\"\"\",
    "    \"social_media\": f\"\"\"You are a social media manager for a tech startup. Create 3 engaging tweets about AI in healthcare.

Requirements:
- Each tweet must be under 280 characters
- Include relevant hashtags
- Focus on benefits and innovations
- Vary the angles (patient care, diagnosis, research)

Tweets:\"\"\"\n",
    "}\n",
    "\n",
    "content_results = {}\n",
    "for content_type, prompt in content_generation_prompts.items():\n",
    "    response = llm.generate_response(prompt)\n",
    "    content_results[content_type] = {\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'char_count': len(response),\n",
    "        'word_count': len(response.split())\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONTENT TYPE: {content_type.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Generated content ({content_results[content_type]['word_count']} words):\")\n",
    "    print(response)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Analyze content generation results\n",
    "content_analysis = pd.DataFrame({\n",
    "    'Content Type': list(content_results.keys()),\n",
    "    'Word Count': [r['word_count'] for r in content_results.values()],\n",
    "    'Character Count': [r['char_count'] for r in content_results.values()]\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "content_analysis.set_index('Content Type').plot(kind='bar', ax=ax)\n",
    "ax.set_title('Content Generation Analysis')\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Code Generation and Debugging\n",
    "Using prompt engineering for programming tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation and debugging examples\n",
    "code_prompts = {\n",
    "    \"algorithm_implementation\": PromptTemplateLibrary.code_generation_template(\n",
    "        \"Implement Quick Sort algorithm\",\n",
    "        \"Python\",\n",
    "        [\"Include comments\", \"Handle edge cases\", \"Show example usage\"]\n",
    "    ),\n",
    "    \"code_debugging\": f\"\"\"You are an expert Python developer. Debug and fix the following code:

```python
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    average = total / len(numbers)
    return average

# Test case
result = calculate_average([10, 20, 30, 40, 50])
print(f\"Average: {{result}}\")
```

Issues to fix:
1. Handle empty list case
2. Add error handling for non-numeric input
3. Optimize performance if needed

Fixed code:\"\"\",\n",
    "    \"code_explanation\": f\"\"\"You are a programming instructor. Explain this Python code snippet to a beginner:

```python
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
```

Your explanation should cover:
1. What the function does
2. How recursion works here
3. Time complexity
4. Example usage
5. Potential improvements

Explanation:\"\"\"\n",
    "}\n",
    "\n",
    "code_results = {}\n",
    "for task_type, prompt in code_prompts.items():\n",
    "    response = llm.generate_response(prompt)\n",
    "    code_results[task_type] = {\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'has_code': '```' in response or 'def ' in response,\n",
    "        'lines': len(response.split('\\n'))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CODE TASK: {task_type.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Prompt:\")\n",
    "    print(prompt)\n",
    "    print(f\"\\nGenerated Response:\")\n",
    "    print(response)\n",
    "    print(f\"\\nContains code: {code_results[task_type]['has_code']}\")\n",
    "    print(f\"Response length: {code_results[task_type]['lines']} lines\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Prompt Engineering\n",
    "\n",
    "### 8.1 Interactive Prompt Builder\n",
    "Build and test prompts in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt builder\n",
    "def build_prompt(task_type, topic, complexity, length, include_examples):\n",
    "    \"\"\"Build a prompt based on user preferences\"\"\"\n",
    "    \n",
    "    base_prompt = f\"\"\"You are an expert {task_type}. \n",
    "    Explain {topic} at a {complexity} level. \n",
    "    Your response should be approximately {length} words.\n",
    "    \"\"\"\n",
    "    \n",
    "    if include_examples:\n",
    "        base_prompt += \"Include relevant examples to illustrate key concepts.\"\n",
    "    \n",
    "    base_prompt += \"\\n\\nYour response:\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Create interactive widgets\n",
    "task_widget = widgets.Dropdown(\n",
    "    options=['educator', 'scientist', 'journalist', 'consultant'],\n",
    "    value='educator',\n",
    "    description='Role:'\n",
    ")\n",
    "\n",
    "topic_widget = widgets.Text(\n",
    "    value='artificial intelligence',\n",
    "    description='Topic:'\n",
    ")\n",
    "\n",
    "complexity_widget = widgets.Dropdown(\n",
    "    options=['beginner', 'intermediate', 'advanced'],\n",
    "    value='intermediate',\n",
    "    description='Complexity:'\n",
    ")\n",
    "\n",
    "length_widget = widgets.IntSlider(\n",
    "    value=200, min=50, max=500, step=50,\n",
    "    description='Length (words):'\n",
    ")\n",
    "\n",
    "examples_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Include Examples'\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_generate_click(b):\n",
    "    with output_widget:\n",
    "        output_widget.clear_output()\n",
    "        \n",
    "        prompt = build_prompt(\n",
    "            task_widget.value,\n",
    "            topic_widget.value,\n",
    "            complexity_widget.value,\n",
    "            length_widget.value,\n",
    "            examples_widget.value\n",
    "        )\n",
    "        \n",
    "        print(\"Generated Prompt:\")\n",
    "        print(\"=\"*50)\n",
    "        print(prompt)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Generate response\n",
    "        response = llm.generate_response(prompt)\n",
    "        \n",
    "        print(\"\\nGenerated Response:\")\n",
    "        print(\"=\"*50)\n",
    "        print(response)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "generate_button = widgets.Button(description=\"Generate Prompt & Response\")\n",
    "generate_button.on_click(on_generate_click)\n",
    "\n",
    "# Display interactive interface\n",
    "display(widgets.VBox([\n",
    "    task_widget, topic_widget, complexity_widget,\n",
    "    length_widget, examples_widget, generate_button, output_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Common Pitfalls\n",
    "\n",
    "### 9.1 Prompt Engineering Best Practices\n",
    "- **Be Specific**: Clear, detailed instructions produce better results\n",
    "- **Use Examples**: Few-shot learning guides the model effectively\n",
    "- **Set Context**: Provide background information and constraints\n",
    "- **Iterate**: Test and refine prompts based on results\n",
    "- **Consider Temperature**: Adjust for creativity vs. consistency\n",
    "- **Handle Edge Cases**: Plan for unexpected inputs\n",
    "\n",
    "### 9.2 Common Pitfalls to Avoid\n",
    "- **Vague Instructions**: Leads to unfocused responses\n",
    "- **Contradictory Requirements**: Confuses the model\n",
    "- **Overly Complex Prompts**: Can be difficult to parse\n",
    "- **Ignoring Model Limitations**: Each model has different capabilities\n",
    "- **Not Testing Thoroughly**: Prompts may work differently on various inputs\n",
    "- **Forgetting Safety**: Always consider ethical implications\n",
    "\n",
    "### 9.3 Advanced Techniques\n",
    "- **Prompt Chaining**: Use outputs as inputs for subsequent prompts\n",
    "- **Self-Consistency**: Generate multiple responses and select the best\n",
    "- **Prompt Decomposition**: Break complex tasks into smaller steps\n",
    "- **Retrieval-Augmented Generation**: Combine with external knowledge bases\n",
    "- **Prompt Tuning**: Optimize prompts for specific domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises and Challenges\n",
    "\n",
    "### Exercise 1: Basic Prompt Engineering\n",
    "Create prompts for 5 different tasks (summarization, classification, generation, analysis, translation). Test each prompt and evaluate the results.\n",
    "\n",
    "### Exercise 2: Role-Based Prompting\n",
    "Take a complex topic and create prompts for 3 different roles (expert, beginner, journalist). Compare how the responses differ in tone, complexity, and structure.\n",
    "\n",
    "### Exercise 3: Chain-of-Thought Implementation\n",
    "Create a Chain-of-Thought prompt for a mathematical or logical reasoning problem. Compare it with a direct prompt and analyze the difference in reasoning quality.\n",
    "\n",
    "### Exercise 4: Prompt Template Library\n",
    "Build a collection of 10 reusable prompt templates for common tasks in your field of interest.\n",
    "\n",
    "### Exercise 5: A/B Testing Challenge\n",
    "Design and run an A/B test comparing 3 different prompt approaches for a specific task. Use multiple evaluation criteria and determine the best approach.\n",
    "\n",
    "**Challenge**: Create a complete prompt engineering workflow that includes prompt design, testing, evaluation, and optimization for a real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Further Learning Resources\n",
    "\n",
    "### Documentation and Guides:\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [Google Prompt Engineering Guide](https://developers.google.com/machine-learning/resources/prompt-eng)\n",
    "\n",
    "### Research Papers:\n",
    "- \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n",
    "- \"Large Language Models are Zero-Shot Reasoners\"\n",
    "- \"Refined Web Supervision for Open-Domain Question Answering\"\n",
    "\n",
    "### Courses and Tutorials:\n",
    "- DeepLearning.AI: ChatGPT Prompt Engineering for Developers\n",
    "- Coursera: Prompt Engineering for ChatGPT Specialization\n",
    "- Prompt Engineering Guide (online book)\n",
    "\n",
    "### Tools and Frameworks:\n",
    "- LangChain (prompt templates and chains)\n",
    "- Promptfoo (prompt testing and evaluation)\n",
    "- Guardrails AI (output validation)\n",
    "\n",
    "### Community Resources:\n",
    "- r/PromptEngineering on Reddit\n",
    "- Prompt Engineering Discord communities\n",
    "- GitHub repositories with prompt collections\n",
    "- Twitter/X accounts of AI researchers and practitioners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}