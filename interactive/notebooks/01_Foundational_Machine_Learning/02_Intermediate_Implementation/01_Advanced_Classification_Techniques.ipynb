{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Classification Techniques\n",
    "\n",
    "**Interactive Notebook** - Section 1: Foundational Machine Learning\n",
    "\n",
    "Dive deep into sophisticated classification methods and techniques used in real-world applications. This notebook covers advanced algorithms, ensemble methods, and practical implementation strategies.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Master Support Vector Machines (SVM) with different kernels\n",
    "- ‚úÖ Implement Random Forest and Gradient Boosting classifiers\n",
    "- ‚úÖ Learn ensemble methods and voting classifiers\n",
    "- ‚úÖ Handle imbalanced datasets effectively\n",
    "- ‚úÖ Perform advanced feature selection and engineering\n",
    "- ‚úÖ Apply classification to complex real-world scenarios\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Completion of beginner classification notebooks\n",
    "- Strong understanding of basic classification metrics\n",
    "- Experience with scikit-learn and pandas\n",
    "- Basic knowledge of optimization concepts\n",
    "\n",
    "**Estimated Time**: 3-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Installation\n",
    "\n",
    "Let's set up our environment with advanced libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn xgboost lightgbm catboost imbalanced-learn ipywidgets\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, load_digits, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, RFE, RFECV, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report, \n",
    "    ConfusionMatrixDisplay, RocCurveDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance, partial_dependence\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "print(\"‚úÖ All advanced libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Dataset: Breast Cancer Classification\n",
    "\n",
    "Let's work with the Breast Cancer dataset to predict tumor malignancy using advanced classification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "target_names = cancer.target_names\n",
    "\n",
    "# Create DataFrame\n",
    "df_cancer = pd.DataFrame(X, columns=feature_names)\n",
    "df_cancer['target'] = y\n",
    "df_cancer['target_name'] = [target_names[i] for i in y]\n",
    "\n",
    "print(\"üè• Breast Cancer Dataset Overview\")\n",
    "print(f\"Number of samples: {len(df_cancer)}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Classes: {target_names.tolist()}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "for i, name in enumerate(target_names):\n",
    "    count = np.sum(y == i)\n",
    "    percentage = count / len(y) * 100\n",
    "    print(f\"  {name}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Display feature categories\n",
    "print(\"\\nüìä Feature Categories:\")\n",
    "mean_features = [f for f in feature_names if 'mean' in f]\n",
    "error_features = [f for f in feature_names if 'error' in f]\n",
    "worst_features = [f for f in feature_names if 'worst' in f]\n",
    "\n",
    "print(f\"Mean features ({len(mean_features)}): {', '.join(mean_features[:3])}...\")\n",
    "print(f\"Error features ({len(error_features)}): {', '.join(error_features[:3])}...\")\n",
    "print(f\"Worst features ({len(worst_features)}): {', '.join(worst_features[:3])}...\")\n",
    "\n",
    "# Display dataset statistics\n",
    "display(HTML(\"<h4>Dataset Statistics:</h4>\"))\n",
    "display(df_cancer.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced data exploration\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Advanced Data Exploration: Breast Cancer Dataset', fontsize=16)\n",
    "\n",
    "# 1. Feature correlation heatmap\n",
    "corr_matrix = df_cancer[feature_names].corr()\n",
    "sns.heatmap(corr_matrix.iloc[:10, :10], annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Feature Correlations (First 10 Features)')\n",
    "\n",
    "# 2. Feature distributions by class\n",
    "for i, feature in enumerate(['mean radius', 'mean texture', 'mean perimeter']):\n",
    "    for target_val in [0, 1]:\n",
    "        subset = df_cancer[df_cancer['target'] == target_val]\n",
    "        axes[0, 1].hist(subset[feature], alpha=0.6, label=target_names[target_val], bins=20)\n",
    "axes[0, 1].set_title('Feature Distributions by Class')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel('Feature Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Box plots for key features\n",
    "key_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']\n",
    "df_melted = df_cancer[key_features + ['target']].melt(id_vars=['target'], var_name='feature', value_name='value')\n",
    "df_melted['target'] = df_melted['target'].map({0: 'malignant', 1: 'benign'})\n",
    "sns.boxplot(data=df_melted, x='feature', y='value', hue='target', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Feature Distributions by Class')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Principal Component Analysis\n",
    "scaler_pca = StandardScaler()\n",
    "X_scaled = scaler_pca.fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "scatter = axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, alpha=0.7, cmap='viridis')\n",
    "axes[1, 0].set_title(f'PCA (Explained Variance: {sum(pca.explained_variance_ratio_):.2%})')\n",
    "axes[1, 0].set_xlabel('PC1')\n",
    "axes[1, 0].set_ylabel('PC2')\n",
    "\n",
    "# 5. Feature importance (using correlation with target)\n",
    "target_correlations = df_cancer[feature_names].corrwith(df_cancer['target']).abs().sort_values(ascending=False)\n",
    "top_10_features = target_correlations.head(10)\n",
    "top_10_features.plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Top 10 Features by Target Correlation')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Missing values check\n",
    "missing_values = df_cancer.isnull().sum().sum()\n",
    "axes[1, 2].text(0.5, 0.5, f'Missing Values:\\n{missing_values}', \n",
    "                ha='center', va='center', transform=axes[1, 2].transAxes, fontsize=16)\n",
    "axes[1, 2].set_title('Data Quality Check')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üîç Key Insights:\")\n",
    "print(f\"- Most correlated feature: {target_correlations.index[0]} ({target_correlations.iloc[0]:.3f})\")\n",
    "print(f\"- PCA explained variance ratio: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "print(f\"- No missing values in the dataset\")\n",
    "print(f\"- Features show good separation between classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Advanced Data Preparation and Feature Engineering\n",
    "\n",
    "Let's apply sophisticated data preparation techniques including feature scaling, selection, and engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced data preparation\n",
    "class AdvancedDataPreprocessor:\n",
    "    \"\"\"Advanced data preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {\n",
    "            'standard': StandardScaler(),\n",
    "            'robust': RobustScaler(),\n",
    "            'minmax': MinMaxScaler()\n",
    "        }\n",
    "        self.feature_selectors = {}\n",
    "        self.pca = None\n",
    "        self.selected_features = None\n",
    "    \n",
    "    def engineer_features(self, X, feature_names):\n",
    "        \"\"\"Create new features from existing ones\"\"\"\n",
    "        X_new = X.copy()\n",
    "        new_feature_names = list(feature_names)\n",
    "        \n",
    "        # Statistical features\n",
    "        mean_features = [f for f in feature_names if 'mean' in f]\n",
    "        if mean_features:\n",
    "            X_new['mean_of_means'] = X_new[mean_features].mean(axis=1)\n",
    "            X_new['std_of_means'] = X_new[mean_features].std(axis=1)\n",
    "            new_feature_names.extend(['mean_of_means', 'std_of_means'])\n",
    "        \n",
    "        # Ratio features\n",
    "        if 'mean radius' in feature_names and 'mean texture' in feature_names:\n",
    "            X_new['radius_texture_ratio'] = X_new['mean radius'] / X_new['mean texture']\n",
    "            new_feature_names.append('radius_texture_ratio')\n",
    "        \n",
    "        if 'mean perimeter' in feature_names and 'mean radius' in feature_names:\n",
    "            X_new['perimeter_radius_ratio'] = X_new['mean perimeter'] / X_new['mean radius']\n",
    "            new_feature_names.append('perimeter_radius_ratio')\n",
    "        \n",
    "        # Interaction features\n",
    "        if 'mean area' in feature_names and 'mean smoothness' in feature_names:\n",
    "            X_new['area_smoothness_interaction'] = X_new['mean area'] * X_new['mean smoothness']\n",
    "            new_feature_names.append('area_smoothness_interaction')\n",
    "        \n",
    "        return X_new.values, new_feature_names\n",
    "    \n",
    "    def select_features(self, X, y, method='rfe', n_features=15):\n",
    "        \"\"\"Feature selection using various methods\"\"\"\n",
    "        if method == 'rfe':\n",
    "            estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
    "        elif method == 'rfecv':\n",
    "            estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            selector = RFECV(estimator, step=1, cv=5, scoring='accuracy')\n",
    "        elif method == 'kbest':\n",
    "            selector = SelectKBest(k=n_features)\n",
    "        elif method == 'model_based':\n",
    "            estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            selector = SelectFromModel(estimator, max_features=n_features)\n",
    "        \n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        self.feature_selectors[method] = selector\n",
    "        \n",
    "        return X_selected, selector\n",
    "    \n",
    "    def apply_pca(self, X, n_components=None, variance_threshold=0.95):\n",
    "        \"\"\"Apply PCA for dimensionality reduction\"\"\"\n",
    "        if n_components is None:\n",
    "            # Find components that explain variance_threshold of variance\n",
    "            pca_temp = PCA()\n",
    "            pca_temp.fit(X)\n",
    "            cumulative_variance = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "            n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n",
    "        \n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        X_pca = self.pca.fit_transform(X)\n",
    "        \n",
    "        return X_pca, n_components\n",
    "    \n",
    "    def get_feature_importance(self, method='rfe'):\n",
    "        \"\"\"Get feature importance scores\"\"\"\n",
    "        if method in self.feature_selectors:\n",
    "            selector = self.feature_selectors[method]\n",
    "            if hasattr(selector, 'ranking_'):\n",
    "                return selector.ranking_\n",
    "            elif hasattr(selector, 'scores_'):\n",
    "                return selector.scores_\n",
    "            elif hasattr(selector, 'get_support'):\n",
    "                return selector.get_support()\n",
    "        return None\n",
    "\n",
    "# Apply advanced preprocessing\n",
    "preprocessor = AdvancedDataPreprocessor()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature engineering\n",
    "X_train_eng, feature_names_eng = preprocessor.engineer_features(\n",
    "    pd.DataFrame(X_train, columns=feature_names), feature_names\n",
    ")\n",
    "X_test_eng, _ = preprocessor.engineer_features(\n",
    "    pd.DataFrame(X_test, columns=feature_names), feature_names\n",
    ")\n",
    "\n",
    "print(\"üîß Advanced Data Preprocessing\")\n",
    "print(f\"Original features: {len(feature_names)}\")\n",
    "print(f\"After engineering: {len(feature_names_eng)} features\")\n",
    "print(f\"New features: {set(feature_names_eng) - set(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different feature selection methods\n",
    "feature_methods = ['rfe', 'kbest', 'model_based']\n",
    "selection_results = {}\n",
    "\n",
    "print(\"üîç Feature Selection Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method in feature_methods:\n",
    "    # Scale data first\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_eng)\n",
    "    \n",
    "    # Apply feature selection\n",
    "    X_selected, selector = preprocessor.select_features(\n",
    "        X_train_scaled, y_train, method=method, n_features=15\n",
    "    )\n",
    "    \n",
    "    # Get selected feature indices\n",
    "    if hasattr(selector, 'get_support'):\n",
    "        selected_indices = np.where(selector.get_support())[0]\n",
    "        selected_features = [feature_names_eng[i] for i in selected_indices]\n",
    "    else:\n",
    "        selected_features = feature_names_eng[:15]  # Fallback\n",
    "    \n",
    "    selection_results[method] = {\n",
    "        'X_selected': X_selected,\n",
    "        'selected_features': selected_features,\n",
    "        'selector': selector\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Method:\")\n",
    "    print(f\"Selected features: {len(selected_features)}\")\n",
    "    print(f\"Top features: {', '.join(selected_features[:5])}...\")\n",
    "\n",
    "# Show overlap between methods\n",
    "print(\"\\nüîó Feature Selection Overlap:\")\n",
    "rfe_features = set(selection_results['rfe']['selected_features'])\n",
    "kbest_features = set(selection_results['kbest']['selected_features'])\n",
    "model_features = set(selection_results['model_based']['selected_features'])\n",
    "\n",
    "common_features = rfe_features.intersection(kbest_features).intersection(model_features)\n",
    "print(f\"Features selected by all methods: {len(common_features)}\")\n",
    "print(f\"Common features: {', '.join(list(common_features)[:5])}{'...' if len(common_features) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Support Vector Machines: Advanced Implementation\n",
    "\n",
    "Support Vector Machines are powerful classifiers that work well in high-dimensional spaces. Let's explore different kernels and optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SVM implementation with different kernels\n",
    "class AdvancedSVMClassifier:\n",
    "    \"\"\"Advanced SVM with kernel optimization and cross-validation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.param_grids = {\n",
    "            'linear': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'kernel': ['linear']\n",
    "            },\n",
    "            'rbf': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "                'kernel': ['rbf']\n",
    "            },\n",
    "            'poly': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'degree': [2, 3, 4],\n",
    "                'gamma': ['scale', 'auto'],\n",
    "                'kernel': ['poly']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_with_grid_search(self, X_train, y_train, kernel='rbf', cv=5):\n",
    "        \"\"\"Train SVM with grid search hyperparameter optimization\"\"\"\n",
    "        # Scale data\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Grid search\n",
    "        svm = SVC(probability=True, random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            svm, self.param_grids[kernel], \n",
    "            cv=cv, scoring='accuracy', \n",
    "            n_jobs=-1, verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        self.models[kernel] = {\n",
    "            'model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'cv_results': grid_search.cv_results_\n",
    "            'scaler': self.scaler\n",
    "        }\n",
    "        \n",
    "        return self.models[kernel]\n",
    "    \n",
    "    def predict(self, X_test, kernel='rbf'):\n",
    "        \"\"\"Make predictions with trained model\"\"\"\n",
    "        if kernel not in self.models:\n",
    "            raise ValueError(f\"Model with kernel '{kernel}' not trained\")\n",
    "        \n",
    "        model_info = self.models[kernel]\n",
    "        X_test_scaled = model_info['scaler'].transform(X_test)\n",
    "        \n",
    "        predictions = model_info['model'].predict(X_test_scaled)\n",
    "        probabilities = model_info['model'].predict_proba(X_test_scaled)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def get_support_vectors(self, kernel='rbf'):\n",
    "        \"\"\"Get support vectors for analysis\"\"\"\n",
    "        if kernel not in self.models:\n",
    "            return None\n",
    "        \n",
    "        model = self.models[kernel]['model']\n",
    "        if hasattr(model, 'support_vectors_'):\n",
    "            return {\n",
    "                'support_vectors': model.support_vectors_,\n",
    "                'support_indices': model.support_,\n",
    "                'n_support': model.n_support_\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def compare_kernels(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Compare different SVM kernels\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for kernel in ['linear', 'rbf', 'poly']:\n",
    "            print(f\"\\nüîÑ Training {kernel.upper()} SVM...\")\n",
    "            model_info = self.train_with_grid_search(X_train, y_train, kernel)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred, y_proba = self.predict(X_test, kernel)\n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            try:\n",
    "                auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "            except:\n",
    "                auc = None\n",
    "            \n",
    "            results[kernel] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'best_params': model_info['best_params'],\n",
    "                'cv_score': model_info['best_score']\n",
    "            }\n",
    "            \n",
    "            print(f\"  Best parameters: {model_info['best_params']}\")\n",
    "            print(f\"  CV Score: {model_info['best_score']:.4f}\")\n",
    "            print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Test F1-Score: {f1:.4f}\")\n",
    "            if auc:\n",
    "                print(f\"  Test AUC: {auc:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Train and compare SVM models\n",
    "svm_classifier = AdvancedSVMClassifier()\n",
    "\n",
    "# Use RFE-selected features for SVM\n",
    "X_train_svm = selection_results['rfe']['X_selected']\n",
    "X_test_svm = selection_results['rfe']['selector'].transform(\n",
    "    StandardScaler().fit_transform(X_test_eng)\n",
    ")\n",
    "\n",
    "print(\"üßÆ Advanced SVM Implementation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "svm_results = svm_classifier.compare_kernels(X_train_svm, y_train, X_test_svm, y_test)\n",
    "\n",
    "# Find best kernel\n",
    "best_kernel = max(svm_results.keys(), key=lambda k: svm_results[k]['accuracy'])\n",
    "print(f\"\\nüèÜ Best SVM Kernel: {best_kernel.upper()}\")\n",
    "print(f\"Best Accuracy: {svm_results[best_kernel]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVM results and decision boundaries\n",
    "def visualize_svm_results(svm_results, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Visualize SVM performance and decision boundaries\"\"\"\n    \n    "    # Performance comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Performance metrics comparison\n",
    "    kernels = list(svm_results.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    x = np.arange(len(kernels))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [svm_results[kernel][metric] for kernel in kernels]\n",
    "        axes[0, 0].bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('SVM Kernel')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_title('Performance Metrics by Kernel')\n",
    "    axes[0, 0].set_xticks(x + width*1.5)\n",
    "    axes[0, 0].set_xticklabels(kernels)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ROC curves (2D visualization using PCA)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_2d = pca.fit_transform(X_train)\n",
    "    X_test_2d = pca.transform(X_test)\n",
    "    \n",
    "    for kernel in kernels:\n",
    "        svm_classifier.train_with_grid_search(X_train_2d, y_train, kernel, cv=3)\n",
    "        y_pred, y_proba = svm_classifier.predict(X_test_2d, kernel)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n",
    "        auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "        axes[0, 1].plot(fpr, tpr, label=f'{kernel} (AUC = {auc:.3f})')\n",
    "    \n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 1].set_xlabel('False Positive Rate')\n",
    "    axes[0, 1].set_ylabel('True Positive Rate')\n",
    "    axes[0, 1].set_title('ROC Curves (2D PCA Space)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Support vectors visualization\n",
    "    best_kernel = max(svm_results.keys(), key=lambda k: svm_results[k]['accuracy'])\n",
    "    support_info = svm_classifier.get_support_vectors(best_kernel)\n",
    "    \n",
    "    if support_info:\n",
    "        axes[1, 0].scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train, alpha=0.6, cmap='viridis')\n",
    "        if hasattr(svm_classifier.models[best_kernel]['model'], 'support_vectors_'):\n",
    "            sv = svm_classifier.models[best_kernel]['model'].support_vectors_\n",
    "            axes[1, 0].scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none', edgecolors='red', linewidth=2)\n",
    "        axes[1, 0].set_title(f'Support Vectors ({best_kernel} kernel)')\n",
    "        axes[1, 0].set_xlabel('PC1')\n",
    "        axes[1, 0].set_ylabel('PC2')\n",
    "    \n",
    "    # 4. Hyperparameter impact\n",
    "    if 'rbf' in svm_results:\n",
    "        cv_results = svm_classifier.models['rbf']['cv_results']\n",
    "        C_values = [param['C'] for param in cv_results['params'] if param['kernel'] == 'rbf']\n",
    "        test_scores = [score for score, param in zip(cv_results['mean_test_score'], cv_results['params']) if param['kernel'] == 'rbf']\n",
    "        \n",
    "        axes[1, 1].semilogx(C_values, test_scores, 'o-', linewidth=2, markersize=8)\n",
    "        axes[1, 1].set_xlabel('C (log scale)')\n",
    "        axes[1, 1].set_ylabel('CV Accuracy')\n",
    "        axes[1, 1].set_title('Hyperparameter Impact (RBF Kernel)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "visualize_svm_results(svm_results, X_train_svm, y_train, X_test_svm, y_test)\n",
    "\n",
    "print(\"üìä SVM Analysis Complete!\")\n",
    "print(f\"Key insights:\")\n",
    "print(f\"- RBF kernel typically performs best for non-linear data\")\n",
    "print(f\"- Support vectors define the decision boundary\")\n",
    "print(f\"- Proper C parameter selection is crucial for performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≥ Ensemble Methods: Advanced Tree-Based Models\n",
    "\n",
    "Ensemble methods combine multiple models to achieve better performance. Let's explore Random Forest, Gradient Boosting, and advanced tree-based techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced ensemble methods implementation\n",
    "class AdvancedEnsembleClassifier:\n",
    "    \"\"\"Advanced ensemble methods with optimization and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def train_random_forest(self, X_train, y_train, param_grid=None):\n",
    "        \"\"\"Train Random Forest with hyperparameter optimization\"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None]\n",
    "            }\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['random_forest'] = {\n",
    "            'model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'feature_importance': grid_search.best_estimator_.feature_importances_\n",
    "        }\n",
    "        \n",
    "        return self.models['random_forest']\n",
    "    \n",
    "    def train_gradient_boosting(self, X_train, y_train, param_grid=None):\n",
    "        \"\"\"Train Gradient Boosting with optimization\"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'subsample': [0.8, 1.0]\n",
    "            }\n",
    "        \n",
    "        gb = GradientBoostingClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['gradient_boosting'] = {\n",
    "            'model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'feature_importance': grid_search.best_estimator_.feature_importances_\n",
    "        }\n",
    "        \n",
    "        return self.models['gradient_boosting']\n",
    "    \n",
    "    def train_xgboost(self, X_train, y_train, param_grid=None):\n",
    "        \"\"\"Train XGBoost with optimization\"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'colsample_bytree': [0.8, 1.0]\n",
    "            }\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "        grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['xgboost'] = {\n",
    "            'model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'feature_importance': grid_search.best_estimator_.feature_importances_\n",
    "        }\n",
    "        \n",
    "        return self.models['xgboost']\n",
    "    \n",
    "    def train_lightgbm(self, X_train, y_train, param_grid=None):\n",
    "        \"\"\"Train LightGBM with optimization\"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'num_leaves': [31, 50, 100],\n",
    "                'feature_fraction': [0.8, 1.0]\n",
    "            }\n",
    "        \n",
    "        lgb_model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "        grid_search = GridSearchCV(lgb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['lightgbm'] = {\n",
    "            'model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'feature_importance': grid_search.best_estimator_.feature_importances_\n",
    "        }\n",
    "        \n",
    "        return self.models['lightgbm']\n",
    "    \n",
    "    def train_voting_classifier(self, X_train, y_train):\n",
    "        \"\"\"Train Voting Classifier with best models\"\"\"\n",
    "        estimators = []\n",
    "        \n",
    "        if 'random_forest' in self.models:\n",
    "            estimators.append(('rf', self.models['random_forest']['model']))\n",
    "        if 'gradient_boosting' in self.models:\n",
    "            estimators.append(('gb', self.models['gradient_boosting']['model']))\n",
    "        if 'xgboost' in self.models:\n",
    "            estimators.append(('xgb', self.models['xgboost']['model']))\n",
    "        \n",
    "        if len(estimators) < 2:\n",
    "            return None\n",
    "        \n",
    "        voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['voting'] = {\n",
    "            'model': voting_clf,\n",
    "            'estimators': estimators\n",
    "        }\n",
    "        \n",
    "        return self.models['voting']\n",
    "    \n",
    "    def evaluate_all(self, X_test, y_test):\n",
    "        \"\"\"Evaluate all trained models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model_info in self.models.items():\n",
    "            model = model_info['model']\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            try:\n",
    "                y_proba = model.predict_proba(X_test)\n",
    "                auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "            except:\n",
    "                auc = None\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Train ensemble models\n",
    "ensemble_classifier = AdvancedEnsembleClassifier()\n",
    "\n",
    "print(\"üå≥ Advanced Ensemble Methods\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scale data for ensemble methods\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train_eng)\n",
    "X_test_scaled = StandardScaler().transform(X_test_eng)\n",
    "\n",
    "# Train all ensemble models\n",
    "print(\"üîÑ Training Random Forest...\")\n",
    "ensemble_classifier.train_random_forest(X_train_scaled, y_train)\n",
    "\n",
    "print(\"üîÑ Training Gradient Boosting...\")\n",
    "ensemble_classifier.train_gradient_boosting(X_train_scaled, y_train)\n",
    "\n",
    "print(\"üîÑ Training XGBoost...\")\n",
    "ensemble_classifier.train_xgboost(X_train_scaled, y_train)\n",
    "\n",
    "print(\"üîÑ Training LightGBM...\")\n",
    "ensemble_classifier.train_lightgbm(X_train_scaled, y_train)\n",
    "\n",
    "print(\"üîÑ Training Voting Classifier...\")\n",
    "ensemble_classifier.train_voting_classifier(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate all models\n",
    "ensemble_results = ensemble_classifier.evaluate_all(X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\nüìä Ensemble Model Performance:\")\n",
    "for model_name, metrics in ensemble_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if value is not None:\n",
    "            print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ensemble results\n",
    "def visualize_ensemble_results(ensemble_results, ensemble_classifier, feature_names):\n",
    "    \"\"\"Visualize ensemble model performance and feature importance\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Performance comparison\n",
    "    models = list(ensemble_results.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [ensemble_results[model][metric] for model in models]\n",
    "        axes[0, 0].bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Models')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_title('Performance Comparison')\n",
    "    axes[0, 0].set_xticks(x + width*1.5)\n",
    "    axes[0, 0].set_xticklabels([m.replace('_', ' ').title() for m in models], rotation=45)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Feature importance comparison\n",
    "    model_importances = {}\n",
    "    for model_name in ['random_forest', 'gradient_boosting', 'xgboost']:\n",
    "        if model_name in ensemble_classifier.models:\n",
    "            importance = ensemble_classifier.models[model_name]['feature_importance']\n",
    "            model_importances[model_name] = importance\n",
    "    \n",
    "    if model_importances:\n",
    "        importance_df = pd.DataFrame(model_importances, index=feature_names[:15])\n",
    "        importance_df.mean(axis=1).sort_values(ascending=True).plot(kind='barh', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Average Feature Importance')\n",
    "        axes[0, 1].set_xlabel('Importance')\n",
    "    \n",
    "    # 3. ROC curves\n",
    "    for model_name in models:\n",
    "        if model_name in ensemble_classifier.models:\n",
    "            model = ensemble_classifier.models[model_name]['model']\n",
    "            try:\n",
    "                y_proba = model.predict_proba(X_test_scaled)\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n",
    "                auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "                axes[0, 2].plot(fpr, tpr, label=f'{model_name.replace(\"_\", \" \").title()} (AUC = {auc:.3f})')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 2].set_xlabel('False Positive Rate')\n",
    "    axes[0, 2].set_ylabel('True Positive Rate')\n",
    "    axes[0, 2].set_title('ROC Curves')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Learning curves for best model\n",
    "    best_model_name = max(ensemble_results.keys(), key=lambda k: ensemble_results[k]['accuracy'])\n",
    "    if best_model_name in ensemble_classifier.models:\n",
    "        from sklearn.model_selection import learning_curve\n",
    "        model = ensemble_classifier.models[best_model_name]['model']\n",
    "        \n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, X_train_scaled, y_train, cv=5, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        axes[1, 0].plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training')\n",
    "        axes[1, 0].plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Validation')\n",
    "        axes[1, 0].set_xlabel('Training Size')\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].set_title(f'Learning Curves ({best_model_name.replace(\"_\", \" \").title()})')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Confusion matrix for best model\n",
    "    if best_model_name in ensemble_classifier.models:\n",
    "        model = ensemble_classifier.models[best_model_name]['model']\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title(f'Confusion Matrix ({best_model_name.replace(\"_\", \" \").title()})')\n",
    "        axes[1, 1].set_xlabel('Predicted')\n",
    "        axes[1, 1].set_ylabel('Actual')\n",
    "    \n",
    "    # 6. Hyperparameter importance\n",
    "    axes[1, 2].text(0.5, 0.5, 'Best Model Analysis\\n\\n' + \n",
    "                   f'Best Model: {best_model_name.replace(\"_\", \" \").title()}\\n' +\n",
    "                   f'Accuracy: {ensemble_results[best_model_name][\"accuracy\"]:.4f}\\n' +\n",
    "                   f'F1-Score: {ensemble_results[best_model_name][\"f1\"]:.4f}',\n",
    "                   ha='center', va='center', transform=axes[1, 2].transAxes,\n",
    "                   fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    axes[1, 2].set_title('Best Model Summary')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "visualize_ensemble_results(ensemble_results, ensemble_classifier, feature_names_eng)\n",
    "\n",
    "# Find best ensemble model\n",
    "best_ensemble = max(ensemble_results.keys(), key=lambda k: ensemble_results[k]['accuracy'])\n",
    "print(f\"\\nüèÜ Best Ensemble Model: {best_ensemble.replace('_', ' ').title()}\")\n",
    "print(f\"Accuracy: {ensemble_results[best_ensemble]['accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {ensemble_results[best_ensemble]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Interactive Model Explorer\n",
    "\n",
    "Let's create an interactive tool to explore different models and their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive model explorer\n",
    "model_selector = widgets.SelectMultiple(\n",
    "    options=['Logistic Regression', 'SVM (RBF)', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM'],\n",
    "    value=['Random Forest', 'XGBoost'],\n",
    "    description='Models:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "feature_method_dropdown = widgets.Dropdown(\n",
    "    options=['all_features', 'rfe_selected', 'pca_transformed'],\n",
    "    value='rfe_selected',\n",
    "    description='Feature Method:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "scaling_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Apply Feature Scaling',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "cv_slider = widgets.IntSlider(\n",
    "    value=5, min=3, max=10, step=1,\n",
    "    description='CV Folds:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "explore_button = widgets.Button(description='Explore Models', button_style='primary')\n",
    "explore_output = widgets.Output()\n",
    "\n",
    "def explore_models_interactive(b):\n",
    "    with explore_output:\n",
    "        explore_output.clear_output()\n",
    "        \n",
    "        # Prepare data based on selection\n",
    "        if feature_method_dropdown.value == 'all_features':\n",
    "            X_train_exp = X_train_eng\n",
    "            X_test_exp = X_test_eng\n",
    "            feature_names_exp = feature_names_eng\n",
    "        elif feature_method_dropdown.value == 'rfe_selected':\n",
    "            X_train_exp = selection_results['rfe']['X_selected']\n",
    "            X_test_exp = selection_results['rfe']['selector'].transform(\n",
    "                StandardScaler().fit_transform(X_test_eng) if scaling_checkbox.value else X_test_eng\n",
    "            )\n",
    "            selected_indices = np.where(selection_results['rfe']['selector'].get_support())[0]\n",
    "            feature_names_exp = [feature_names_eng[i] for i in selected_indices]\n",
    "        else:  # pca_transformed\n",
    "            pca = PCA(n_components=0.95)\n",
    "            X_train_exp = pca.fit_transform(\n",
    "                StandardScaler().fit_transform(X_train_eng) if scaling_checkbox.value else X_train_eng\n",
    "            )\n",
    "            X_test_exp = pca.transform(\n",
    "                StandardScaler().transform(X_test_eng) if scaling_checkbox.value else X_test_eng\n",
    "            )\n",
    "            feature_names_exp = [f'PC{i+1}' for i in range(X_train_exp.shape[1])]\n",
    "        \n",
    "        # Apply scaling if requested\n",
    "        if scaling_checkbox.value:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_exp = scaler.fit_transform(X_train_exp)\n",
    "            X_test_exp = scaler.transform(X_test_exp)\n",
    "        \n",
    "        # Initialize models\n",
    "        models = {}\n",
    "        \n",
    "        if 'Logistic Regression' in model_selector.value:\n",
    "            models['Logistic Regression'] = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        if 'SVM (RBF)' in model_selector.value:\n",
    "            models['SVM (RBF)'] = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "        if 'Random Forest' in model_selector.value:\n",
    "            models['Random Forest'] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        if 'Gradient Boosting' in model_selector.value:\n",
    "            models['Gradient Boosting'] = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        if 'XGBoost' in model_selector.value:\n",
    "            models['XGBoost'] = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "        if 'LightGBM' in model_selector.value:\n",
    "            models['LightGBM'] = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        results = {}\n",
    "        \n",
    "        print(f\"üîç Model Exploration Results\")\n",
    "        print(f\"Feature Method: {feature_method_dropdown.value}\")\n",
    "        print(f\"Scaling: {scaling_checkbox.value}\")\n",
    "        print(f\"CV Folds: {cv_slider.value}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train_exp, y_train, cv=cv_slider.value, scoring='accuracy')\n",
    "            \n",
    "            # Train on full training set\n",
    "            model.fit(X_train_exp, y_train)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = model.predict(X_test_exp)\n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'test_f1': test_f1,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  CV Accuracy: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "            print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"  Test F1: {test_f1:.4f}\")\n",
    "        \n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Performance comparison\n",
    "        model_names = list(results.keys())\n",
    "        cv_means = [results[name]['cv_mean'] for name in model_names]\n",
    "        cv_stds = [results[name]['cv_std'] for name in model_names]\n",
    "        test_accs = [results[name]['test_accuracy'] for name in model_names]\n",
    "        \n",
    "        x_pos = np.arange(len(model_names))\n",
    "        \n",
    "        plt.bar(x_pos - 0.2, cv_means, 0.4, yerr=cv_stds, label='CV Accuracy', alpha=0.7, capsize=5)\n",
    "        plt.bar(x_pos + 0.2, test_accs, 0.4, label='Test Accuracy', alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xticks(x_pos, model_names, rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance for tree-based models\n",
    "        tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']\n",
    "        for model_name in tree_models:\n",
    "            if model_name in results and hasattr(results[model_name]['model'], 'feature_importances_'):\n",
    "                if feature_method_dropdown.value != 'pca_transformed':\n",
    "                    importances = results[model_name]['model'].feature_importances_\n",
    "                    importance_df = pd.DataFrame({\n",
    "                        'feature': feature_names_exp,\n",
    "                        'importance': importances\n",
    "                    }).sort_values('importance', ascending=False).head(10)\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "                    plt.title(f'Top 10 Feature Importances - {model_name}')\n",
    "                    plt.xlabel('Importance')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "explore_button.on_click(explore_models_interactive)\n",
    "\n",
    "print(\"üéõÔ∏è Interactive Model Explorer\")\n",
    "print(\"Select models and configuration to explore their performance!\")\n",
    "display(widgets.VBox([\n",
    "    model_selector, feature_method_dropdown, scaling_checkbox, \n",
    "    cv_slider, explore_button, explore_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Challenge Exercises\n",
    "\n",
    "Test your advanced classification skills with these challenging exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Advanced Feature Engineering\n",
    "print(\"üèÜ Challenge 1: Advanced Feature Engineering\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Create sophisticated features and evaluate their impact\")\n",
    "\n",
    "# Create advanced features\n",
    "def create_advanced_features(X_df):\n",
    "    \"\"\"Create advanced feature engineering pipeline\"\"\"\n",
    "    X_advanced = X_df.copy()\n",
    "    \n",
    "    # Polynomial features for top correlated features\n",
    "    top_features = ['mean radius', 'mean perimeter', 'mean area']\n",
    "    for feat in top_features:\n",
    "        if feat in X_advanced.columns:\n",
    "            X_advanced[f'{feat}_squared'] = X_advanced[feat] ** 2\n",
    "            X_advanced[f'{feat}_cubed'] = X_advanced[feat] ** 3\n",
    "            X_advanced[f'{feat}_sqrt'] = np.sqrt(X_advanced[feat])\n",
    "            X_advanced[f'{feat}_log'] = np.log1p(X_advanced[feat])\n",
    "    \n",
    "    # Interaction features\n",
    "    if 'mean radius' in X_advanced.columns and 'mean texture' in X_advanced.columns:\n",
    "        X_advanced['radius_texture_product'] = X_advanced['mean radius'] * X_advanced['mean texture']\n",
    "        X_advanced['radius_texture_ratio'] = X_advanced['mean radius'] / (X_advanced['mean texture'] + 1e-6)\n",
    "    \n",
    "    # Statistical aggregation\n",
    "    mean_cols = [col for col in X_advanced.columns if 'mean' in col and col not in [f for f in X_advanced.columns if '_' in f]]\n",
    "    if mean_cols:\n",
    "        X_advanced['mean_of_means'] = X_advanced[mean_cols].mean(axis=1)\n",
    "        X_advanced['std_of_means'] = X_advanced[mean_cols].std(axis=1)\n",
    "        X_advanced['max_of_means'] = X_advanced[mean_cols].max(axis=1)\n",
    "        X_advanced['min_of_means'] = X_advanced[mean_cols].min(axis=1)\n",
    "    \n",
    "    # Domain-specific features\n",
    "    if 'mean radius' in X_advanced.columns and 'mean perimeter' in X_advanced.columns:\n",
    "        # Circularity approximation: perimeter / (2 * œÄ * radius)\n",
    "        X_advanced['circularity'] = X_advanced['mean perimeter'] / (2 * np.pi * X_advanced['mean radius'])\n",
    "        # Area approximation: œÄ * radius¬≤\n",
    "        X_advanced['area_approx'] = np.pi * X_advanced['mean radius'] ** 2\n",
    "        # Compactness: perimeter¬≤ / area\n",
    "        X_advanced['compactness'] = X_advanced['mean perimeter'] ** 2 / (4 * np.pi * X_advanced['mean area'])\n",
    "    \n",
    "    return X_advanced\n",
    "\n",
    "# Apply advanced feature engineering\n",
    "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "X_train_advanced = create_advanced_features(X_train_df)\n",
    "X_test_advanced = create_advanced_features(X_test_df)\n",
    "\n",
    "print(f\"‚úÖ Advanced features created\")\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Advanced features: {X_train_advanced.shape[1]}\")\n",
    "print(f\"New features: {X_train_advanced.shape[1] - X_train.shape[1]}\")\n",
    "\n",
    "# Train models with advanced features\n",
    "X_train_adv_scaled = StandardScaler().fit_transform(X_train_advanced)\n",
    "X_test_adv_scaled = StandardScaler().transform(X_test_advanced)\n",
    "\n",
    "# Compare performance\n",
    "models_compare = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results_comparison = {}\n",
    "\n",
    "for model_name, model in models_compare.items():\n",
    "    # Original features\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_orig = model.predict(X_test_scaled)\n",
    "    accuracy_orig = accuracy_score(y_test, y_pred_orig)\n",
    "    \n",
    "    # Advanced features\n",
    "    model.fit(X_train_adv_scaled, y_train)\n",
    "    y_pred_adv = model.predict(X_test_adv_scaled)\n",
    "    accuracy_adv = accuracy_score(y_test, y_pred_adv)\n",
    "    \n",
    "    results_comparison[model_name] = {\n",
    "        'original': accuracy_orig,\n",
    "        'advanced': accuracy_adv,\n",
    "        'improvement': accuracy_adv - accuracy_orig\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Original: {accuracy_orig:.4f}\")\n",
    "    print(f\"  Advanced: {accuracy_adv:.4f}\")\n",
    "    print(f\"  Improvement: {accuracy_adv - accuracy_orig:.4f}\")\n",
    "\n",
    "# Visualize improvement\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = list(results_comparison.keys())\n",
    "original_scores = [results_comparison[m]['original'] for m in models]\n",
    "advanced_scores = [results_comparison[m]['advanced'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, original_scores, width, label='Original Features', alpha=0.7)\n",
    "plt.bar(x + width/2, advanced_scores, width, label='Advanced Features', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Feature Engineering Impact')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Feature Engineering Analysis:\")\n",
    "improvement_avg = np.mean([results_comparison[m]['improvement'] for m in models])\n",
    "print(f\"Average improvement: {improvement_avg:.4f}\")\n",
    "if improvement_avg > 0:\n",
    "    print(\"‚úÖ Advanced features improved performance!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Advanced features didn't improve performance. Feature engineering requires careful validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Handling Imbalanced Data\n",
    "print(\"üèÜ Challenge 2: Handling Imbalanced Data\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Apply techniques to handle class imbalance\")\n",
    "\n",
    "# Create imbalanced dataset for demonstration\n",
    "X_imbalanced = X\n",
    "y_imbalanced = y.copy()\n",
    "\n",
    "# Make it imbalanced (keep only 20% of minority class)\n",
    "minority_indices = np.where(y_imbalanced == 0)[0]\n",
    "np.random.seed(42)\n",
    "keep_indices = np.random.choice(minority_indices, size=int(0.2 * len(minority_indices)), replace=False)\n",
    "remove_indices = np.setdiff1d(minority_indices, keep_indices)\n",
    "\n",
    "X_imbalanced = np.delete(X_imbalanced, remove_indices, axis=0)\n",
    "y_imbalanced = np.delete(y_imbalanced, remove_indices)\n",
    "\n",
    "print(f\"üìä Imbalanced Dataset Created:\")\n",
    "print(f\"Total samples: {len(y_imbalanced)}\")\n",
    "print(f\"Class 0 (malignant): {np.sum(y_imbalanced == 0)} ({np.sum(y_imbalanced == 0)/len(y_imbalanced)*100:.1f}%)\")\n",
    "print(f\"Class 1 (benign): {np.sum(y_imbalanced == 1)} ({np.sum(y_imbalanced == 1)/len(y_imbalanced)*100:.1f}%)\")\n",
    "\n",
    "# Split imbalanced data\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imbalanced, y_imbalanced, test_size=0.3, random_state=42, stratify=y_imbalanced\n",
    ")\n",
    "\n",
    "# Apply different sampling techniques\n",
    "print(\"\\nüîÑ Applying Sampling Techniques...\")\n",
    "\n",
    "# 1. No sampling (baseline)\n",
    "scaler_imb = StandardScaler()\n",
    "X_train_imb_scaled = scaler_imb.fit_transform(X_train_imb)\n",
    "X_test_imb_scaled = scaler_imb.transform(X_test_imb)\n",
    "\n",
    "# 2. SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_imb_scaled, y_train_imb)\n",
    "\n",
    "# 3. Random Under-sampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train_imb_scaled, y_train_imb)\n",
    "\n",
    "# 4. Combined SMOTE and ENN\n",
    "smoteenn = SMOTEENN(random_state=42)\n",
    "X_train_smoteenn, y_train_smoteenn = smoteenn.fit_resample(X_train_imb_scaled, y_train_imb)\n",
    "\n",
    "# Train and evaluate models\n",
    "model_imb = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "sampling_methods = {\n",
    "    'No Sampling': (X_train_imb_scaled, y_train_imb),\n",
    "    'SMOTE': (X_train_smote, y_train_smote),\n",
    "    'Undersampling': (X_train_under, y_train_under),\n",
    "    'SMOTEENN': (X_train_smoteenn, y_train_smoteenn)\n",
    "}\n",
    "\n",
    "results_imb = {}\n",
    "\n",
    "for method_name, (X_train_method, y_train_method) in sampling_methods.items():\n",
    "    # Train model\n",
    "    model_imb.fit(X_train_method, y_train_method)\n",
    "    y_pred_imb = model_imb.predict(X_test_imb_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_imb, y_pred_imb)\n",
    "    precision = precision_score(y_test_imb, y_pred_imb, average='weighted')\n",
    "    recall = recall_score(y_test_imb, y_pred_imb, average='weighted')\n",
    "    f1 = f1_score(y_test_imb, y_pred_imb, average='weighted')\n",
    "    \n",
    "    # Calculate class-specific metrics\n",
    "    report = classification_report(y_test_imb, y_pred_imb, output_dict=True)\n",
    "    \n",
    "    results_imb[method_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'class_0_recall': report['0']['recall'],\n",
    "        'class_1_recall': report['1']['recall'],\n",
    "        'class_0_f1': report['0']['f1-score'],\n",
    "        'class_1_f1': report['1']['f1-score']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Class 0 Recall: {results_imb[method_name]['class_0_recall']:.4f}\")\n",
    "    print(f\"  Class 1 Recall: {results_imb[method_name]['class_1_recall']:.4f}\")\n",
    "    print(f\"  Class 0 F1: {results_imb[method_name]['class_0_f1']:.4f}\")\n",
    "    print(f\"  Class 1 F1: {results_imb[method_name]['class_1_f1']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Overall performance\n",
    "methods = list(results_imb.keys())\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    values = [results_imb[method][metric] for method in methods]\n",
    "    plt.bar(methods, values, alpha=0.7)\n",
    "    plt.title(metric.title())\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Class-specific performance\n",
    "plt.subplot(2, 3, 5)\n",
    "class_0_recall = [results_imb[method]['class_0_recall'] for method in methods]\n",
    "class_1_recall = [results_imb[method]['class_1_recall'] for method in methods]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, class_0_recall, width, label='Class 0 Recall', alpha=0.7)\n",
    "plt.bar(x + width/2, class_1_recall, width, label='Class 1 Recall', alpha=0.7)\n",
    "plt.xlabel('Sampling Method')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Class-Specific Recall')\n",
    "plt.xticks(x, methods, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Class-specific F1\n",
    "plt.subplot(2, 3, 6)\n",
    "class_0_f1 = [results_imb[method]['class_0_f1'] for method in methods]\n",
    "class_1_f1 = [results_imb[method]['class_1_f1'] for method in methods]\n",
    "\n",
    "plt.bar(x - width/2, class_0_f1, width, label='Class 0 F1', alpha=0.7)\n",
    "plt.bar(x + width/2, class_1_f1, width, label='Class 1 F1', alpha=0.7)\n",
    "plt.xlabel('Sampling Method')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Class-Specific F1')\n",
    "plt.xticks(x, methods, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Imbalanced Data Analysis:\")\n",
    "best_method = max(results_imb.keys(), key=lambda k: results_imb[k]['f1'])\n",
    "print(f\"Best method: {best_method}\")\n",
    "print(f\"Key insight: Sampling techniques can significantly improve minority class performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Real-world Application - Customer Churn Prediction\n",
    "print(\"üèÜ Challenge 3: Customer Churn Prediction\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Apply advanced classification to a business problem\")\n",
    "\n",
    "# Create synthetic customer churn dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "# Generate features\n",
    "tenure = np.random.exponential(24, n_customers)  # Months as customer\n",
    "monthly_charges = np.random.normal(70, 20, n_customers)  # Monthly bill\n",
    "total_charges = tenure * monthly_charges + np.random.normal(0, 100, n_customers)\n",
    "contract_type = np.random.choice([0, 1, 2], n_customers, p=[0.5, 0.3, 0.2])  # Month-to-month, 1-year, 2-year\n",
    "internet_service = np.random.choice([0, 1, 2], n_customers, p=[0.2, 0.4, 0.4])  # No, DSL, Fiber\n",
    "tech_support = np.random.choice([0, 1], n_customers, p=[0.7, 0.3])\n",
    "payment_method = np.random.choice([0, 1, 2, 3], n_customers, p=[0.3, 0.2, 0.2, 0.3])\n",
    "senior_citizen = np.random.choice([0, 1], n_customers, p=[0.8, 0.2])\n",
    "partner = np.random.choice([0, 1], n_customers, p=[0.5, 0.5])\n",
    "dependents = np.random.choice([0, 1], n_customers, p=[0.7, 0.3])\n",
    "phone_service = np.random.choice([0, 1], n_customers, p=[0.1, 0.9])\n",
    "\n",
    "# Create churn probability based on features\n",
    "churn_prob = (\n",
    "    0.1 +  # Base rate\n",
    "    0.3 * (tenure < 12) +  # New customers more likely to churn\n",
    "    0.2 * (monthly_charges > 100) +  # High monthly charges increase churn\n",
    "    0.15 * (contract_type == 0) +  # Month-to-month contracts more likely to churn\n",
    "    0.1 * (internet_service == 2) +  # Fiber optic customers more likely to churn\n",
    "    0.15 * (tech_support == 0) +  # No tech support increases churn\n",
    "    0.1 * (payment_method == 0) +  # Electronic check increases churn\n",
    "    0.05 * senior_citizen +  # Senior citizens slightly more likely to churn\n",
    "    0.1 * (partner == 0) * (dependents == 0)  # Single customers without dependents more likely to churn\n",
    ")\n",
    "\n",
    "# Cap probability and generate churn labels\n",
    "churn_prob = np.clip(churn_prob, 0, 0.8)  # Cap at 80%\n",
    "churn = (np.random.random(n_customers) < churn_prob).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "churn_data = pd.DataFrame({\n",
    "    'tenure': tenure,\n",
    "    'monthly_charges': monthly_charges,\n",
    "    'total_charges': total_charges,\n",
    "    'contract_type': contract_type,\n",
    "    'internet_service': internet_service,\n",
    "    'tech_support': tech_support,\n",
    "    'payment_method': payment_method,\n",
    "    'senior_citizen': senior_citizen,\n",
    "    'partner': partner,\n",
    "    'dependents': dependents,\n",
    "    'phone_service': phone_service,\n",
    "    'churn': churn\n",
    "})\n",
    "\n",
    "# Create additional features\n",
    "churn_data['avg_monthly_charges'] = churn_data['total_charges'] / (churn_data['tenure'] + 1)\n",
    "churn_data['tenure_to_age_ratio'] = churn_data['tenure'] / 120  # Assuming 10 years max\n",
    "churn_data['high_value_customer'] = (churn_data['monthly_charges'] > churn_data['monthly_charges'].quantile(0.8)).astype(int)\n",
    "churn_data['loyal_customer'] = (churn_data['tenure'] > churn_data['tenure'].quantile(0.8)).astype(int)\n",
    "\n",
    "print(\"üìà Customer Churn Dataset Created\")\n",
    "print(f\"Total customers: {len(churn_data)}\")\n",
    "print(f\"Churn rate: {churn_data['churn'].mean():.2%}\")\n",
    "print(f\"Non-churn: {np.sum(churn_data['churn'] == 0)} customers\")\n",
    "print(f\"Churn: {np.sum(churn_data['churn'] == 1)} customers\")\n",
    "\n",
    "# Prepare data\n",
    "X_churn = churn_data.drop('churn', axis=1).values\n",
    "y_churn = churn_data['churn'].values\n",
    "churn_feature_names = churn_data.drop('churn', axis=1).columns.tolist()\n",
    "\n",
    "# Split data\n",
    "X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.3, random_state=42, stratify=y_churn\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_churn = StandardScaler()\n",
    "X_train_churn_scaled = scaler_churn.fit_transform(X_train_churn)\n",
    "X_test_churn_scaled = scaler_churn.transform(X_test_churn)\n",
    "\n",
    "# Train advanced models\n",
    "churn_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced'),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=200, random_state=42, eval_metric='logloss', scale_pos_weight=3),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=200, random_state=42, class_weight='balanced', verbose=-1)\n",
    "}\n",
    "\n",
    "churn_results = {}\n",
    "\n",
    "print(\"\\nüîÑ Training Churn Prediction Models...\")\n",
    "\n",
    "for model_name, model in churn_models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_churn_scaled, y_train_churn)\n",
    "    y_pred_churn = model.predict(X_test_churn_scaled)\n",
    "    y_proba_churn = model.predict_proba(X_test_churn_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_churn, y_pred_churn)\n",
    "    precision = precision_score(y_test_churn, y_pred_churn)\n",
    "    recall = precision_score(y_test_churn, y_pred_churn)  # Note: Using precision for both due to binary nature\n",
    "    recall = recall_score(y_test_churn, y_pred_churn)\n",
    "    f1 = f1_score(y_test_churn, y_pred_churn)\n",
    "    auc = roc_auc_score(y_test_churn, y_proba_churn)\n",
    "    \n",
    "    churn_results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'model': model,\n",
    "        'predictions': y_pred_churn,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "\n",
    "# Visualize churn results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Model comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "model_names = list(churn_results.keys())\n",
    "accuracies = [churn_results[m]['accuracy'] for m in model_names]\n",
    "aucs = [churn_results[m]['auc'] for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.7)\n",
    "plt.bar(x + width/2, aucs, width, label='AUC', alpha=0.7)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Churn Model Performance')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature importance\n",
    "plt.subplot(2, 3, 2)\n",
    "best_churn_model = max(churn_results.keys(), key=lambda k: churn_results[k]['auc'])\n",
    "if hasattr(churn_results[best_churn_model]['model'], 'feature_importances_'):\n",
    "    importances = churn_results[best_churn_model]['model'].feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': churn_feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "    plt.title(f'Top 10 Features - {best_churn_model}')\n",
    "    plt.xlabel('Importance')\n",
    "\n",
    "# 3. ROC curves\n",
    "plt.subplot(2, 3, 3)\n",
    "for model_name in model_names:\n",
    "    y_proba = churn_results[model_name]['model'].predict_proba(X_test_churn_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test_churn, y_proba)\n",
    "    auc = churn_results[model_name]['auc']\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confusion matrix for best model\n",
    "plt.subplot(2, 3, 4)\n",
    "cm = confusion_matrix(y_test_churn, churn_results[best_churn_model]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix - {best_churn_model}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# 5. Business impact analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "# Calculate business metrics\n",
    "total_customers = len(y_test_churn)\n",
    "actual_churn = np.sum(y_test_churn)\n",
    "predicted_churn = np.sum(churn_results[best_churn_model]['predictions'])\n",
    "correct_churn = np.sum((y_test_churn == 1) & (churn_results[best_churn_model]['predictions'] == 1))\n",
    "business_metrics = {\n",
    "    'Total Customers': total_customers,\n",
    "    'Actual Churn': actual_churn,\n",
    "    'Predicted Churn': predicted_churn,\n",
    "    'Correctly Identified': correct_churn,\n",
    "    'Churn Detection Rate': correct_churn / actual_churn if actual_churn > 0 else 0,\n",
    "    'False Positive Rate': (predicted_churn - correct_churn) / (total_customers - actual_churn) if (total_customers - actual_churn) > 0 else 0\n",
    "}\n",
    "\n",
    "metrics_names = list(business_metrics.keys())[3:]  # Show only calculated metrics\n",
    "metrics_values = list(business_metrics.values())[3:]\n",
    "plt.bar(metrics_names, metrics_values, alpha=0.7)\n",
    "plt.title('Business Impact')\n",
    "plt.ylabel('Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Model summary\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.text(0.5, 0.5, f'Churn Prediction Summary\\n\\n' +\n",
    "                f'Best Model: {best_churn_model}\\n' +\n",
    "                f'AUC: {churn_results[best_churn_model][\"auc\"]:.3f}\\n' +\n",
    "                f'F1-Score: {churn_results[best_churn_model][\"f1\"]:.3f}\\n' +\n",
    "                f'Churn Detection: {business_metrics[\"Churn Detection Rate\"]:.1%}\\n' +\n",
    "                f'False Positive: {business_metrics[\"False Positive Rate\"]:.1%}',\n",
    "                ha='center', va='center', transform=plt.gca().transAxes,\n",
    "                fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "plt.title('Summary')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Churn Prediction Complete!\")\n",
    "print(f\"Business insights:\")\n",
    "print(f\"- {best_churn_model} provides the best balance of metrics\")\n",
    "print(f\"- Model can identify {business_metrics['Churn Detection Rate']:.1%} of at-risk customers\")\n",
    "print(f\"- False positive rate: {business_metrics['False Positive Rate']:.1%}\")\n",
    "print(f\"- This enables targeted retention strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Key Takeaways\n",
    "\n",
    "### What You've Mastered:\n",
    "\n",
    "‚úÖ **Support Vector Machines**: Advanced kernels, optimization, and support vector analysis\n",
    "\n",
    "‚úÖ **Ensemble Methods**: Random Forest, Gradient Boosting, XGBoost, and LightGBM\n",
    "\n",
    "‚úÖ **Advanced Feature Engineering**: Creating sophisticated features from domain knowledge\n",
    "\n",
    "‚úÖ **Feature Selection**: RFE, RFECV, model-based selection techniques\n",
    "\n",
    "‚úÖ **Imbalanced Data Handling**: SMOTE, undersampling, and combined approaches\n",
    "\n",
    "‚úÖ **Model Optimization**: Grid search, cross-validation, and hyperparameter tuning\n",
    "\n",
    "‚úÖ **Business Applications**: Customer churn prediction and practical implementation\n",
    "\n",
    "‚úÖ **Advanced Evaluation**: ROC curves, precision-recall analysis, and business metrics\n",
    "\n",
    "### Key Advanced Concepts:\n",
    "\n",
    "- **Kernel Trick**: SVMs can handle non-linear decision boundaries\n",
    "- **Ensemble Learning**: Combining multiple models for better performance\n",
    "- **Feature Importance**: Understanding which features drive predictions\n",
    "- **Class Imbalance**: Handling unequal class distributions effectively\n",
    "- **Hyperparameter Optimization**: Systematic parameter search for best performance\n",
    "- **Model Interpretability**: Making complex models explainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "### Continue Your Advanced Learning Journey:\n",
    "\n",
    "1. **üìö Next Notebook**: \"02_Neural_Networks_Introduction.ipynb\" - Dive into deep learning\n",
    "\n",
    "2. **üéØ Advanced Challenges**: Try these techniques on larger datasets like ImageNet or Kaggle competitions\n",
    "\n",
    "3. **üåê Real Applications**: Apply these methods to business problems like fraud detection, medical diagnosis, or recommendation systems\n",
    "\n",
    "4. **üìñ Recommended Reading:**\n",
    "   - \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "   - \"The Elements of Statistical Learning\" by Hastie, Tibshirani, Friedman\n",
    "   - \"Ensemble Methods: Foundations and Algorithms\" by Seni and Elder\n",
    "\n",
    "5. **üõ†Ô∏è Advanced Topics to Explore:**\n",
    "   - Stacking and blending ensemble methods\n",
    "   - Bayesian optimization for hyperparameter tuning\n",
    "   - Automated Machine Learning (AutoML)\n",
    "   - Model explainability and SHAP values\n",
    "   - Multi-class and multi-label classification\n",
    "\n",
    "### Industry Applications:\n",
    "\n",
    "- **Healthcare**: Disease diagnosis, patient risk stratification\n",
    "- **Finance**: Credit scoring, fraud detection, algorithmic trading\n",
    "- **E-commerce**: Customer segmentation, recommendation systems\n",
    "- **Marketing**: Lead scoring, campaign optimization\n",
    "- **Manufacturing**: Quality control, predictive maintenance\n",
    "\n",
    "### Quick Self-Assessment:\n",
    "\n",
    "Can you explain:\n",
    "- How SVM kernels work to handle non-linear data?\n",
    "- Why ensemble methods often outperform individual models?\n",
    "- How to handle class imbalance in real-world datasets?\n",
    "- The trade-offs between different feature selection methods?\n",
    "- How to translate classification metrics into business value?\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've completed the Advanced Classification Techniques notebook! You now have sophisticated skills that are highly valued in industry and research.\n",
    "\n",
    "**Remember**: Advanced classification is a powerful tool - use it responsibly and always consider the ethical implications of your models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}