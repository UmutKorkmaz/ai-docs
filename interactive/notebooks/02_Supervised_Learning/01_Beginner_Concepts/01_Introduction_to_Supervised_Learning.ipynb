{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Supervised Learning\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamental concepts of supervised learning\n",
    "- Learn about different types of supervised learning problems\n",
    "- Implement basic classification and regression algorithms\n",
    "- Evaluate model performance using appropriate metrics\n",
    "- Apply supervised learning to real-world datasets\n",
    "\n",
    "**Expected Duration:** 60-90 minutes\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python programming\n",
    "- Understanding of basic statistics\n",
    "- Familiarity with NumPy and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Supervised Learning?\n",
    "\n",
    "Supervised learning is a type of machine learning where algorithms learn from labeled training data. The goal is to learn a mapping function that can predict the output for new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.datasets import load_iris, load_boston, make_classification, make_regression\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Supervised Learning\n",
    "\n",
    "### 2.1 Classification\n",
    "Predicting discrete categories or classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic classification dataset\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "    n_classes=3, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Classification dataset shape: {X_class.shape}\")\n",
    "print(f\"Classes: {np.unique(y_class)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_class)}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class, cmap='viridis', alpha=0.6)\n",
    "plt.title('Feature Space Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_class)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Regression\n",
    "Predicting continuous numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000, n_features=20, n_informative=15, noise=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Regression dataset shape: {X_reg.shape}\")\n",
    "print(f\"Target range: [{y_reg.min():.2f}, {y_reg.max():.2f}]\")\n",
    "print(f\"Target statistics: Mean={y_reg.mean():.2f}, Std={y_reg.std():.2f}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_reg[:, 0], y_reg, alpha=0.6)\n",
    "plt.title('Feature vs Target Relationship')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_reg, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Target Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Supervised Learning Workflow\n",
    "\n",
    "### 3.1 Data Preparation\n",
    "Loading and preprocessing data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real-world dataset - Iris classification\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"Shape: {X_iris.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Target classes: {target_names}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Training\n",
    "Training multiple algorithms and comparing their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'f1_score': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results[name]['precision']:.4f}\")\n",
    "    print(f\"Recall: {results[name]['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {results[name]['f1_score']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "results_df = pd.DataFrame(results).T\n",
    "plt.figure(figsize=(12, 6))\n",
    "results_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Evaluation\n",
    "Comprehensive evaluation of the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_model_name = results_df['accuracy'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Make predictions with best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled) if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Classification Report\n",
    "plt.subplot(1, 2, 2)\n",
    "report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).iloc[:-1, :].T\n",
    "sns.heatmap(report_df.iloc[:, :-1], annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "plt.title('Classification Report')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Model Exploration\n",
    "\n",
    "### 4.1 Parameter Tuning Widget\n",
    "Explore how different parameters affect model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widgets for parameter exploration\n",
    "def train_and_evaluate_model(model_type, test_size, max_depth=None, n_estimators=100):\n",
    "    \"\"\"Train and evaluate model with given parameters\"\"\"\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_iris, y_iris, test_size=test_size, random_state=42, stratify=y_iris\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create and train model\n",
    "    if model_type == 'Logistic Regression':\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    elif model_type == 'Decision Tree':\n",
    "        model = DecisionTreeClassifier(random_state=42, max_depth=max_depth)\n",
    "    elif model_type == 'Random Forest':\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=n_estimators, max_depth=max_depth)\n",
    "    elif model_type == 'SVM':\n",
    "        model = SVC(random_state=42, probability=True)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Create widgets\n",
    "model_widget = widgets.Dropdown(\n",
    "    options=['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM'],\n",
    "    value='Random Forest',\n",
    "    description='Model:'\n",
    ")\n",
    "\n",
    "test_size_widget = widgets.FloatSlider(\n",
    "    value=0.2, min=0.1, max=0.5, step=0.05,\n",
    "    description='Test Size:'\n",
    ")\n",
    "\n",
    "max_depth_widget = widgets.IntSlider(\n",
    "    value=5, min=1, max=20, step=1,\n",
    "    description='Max Depth:'\n",
    ")\n",
    "\n",
    "n_estimators_widget = widgets.IntSlider(\n",
    "    value=100, min=10, max=200, step=10,\n",
    "    description='N Estimators:'\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget:\n",
    "        output_widget.clear_output()\n",
    "        \n",
    "        accuracy, precision, recall, f1 = train_and_evaluate_model(\n",
    "            model_widget.value, test_size_widget.value, \n",
    "            max_depth_widget.value, n_estimators_widget.value\n",
    "        )\n",
    "        \n",
    "        print(f\"Model: {model_widget.value}\")\n",
    "        print(f\"Test Size: {test_size_widget.value}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "button = widgets.Button(description=\"Train Model\")\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    model_widget, test_size_widget, max_depth_widget, \n",
    "    n_estimators_widget, button, output_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Importance Analysis\n",
    "Understand which features contribute most to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest for feature importance\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance_df, x='importance', y='feature')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "for idx, row in feature_importance_df.iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World Application: Customer Churn Prediction\n",
    "\n",
    "Let's apply supervised learning to a practical business problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic customer churn dataset\n",
    "def create_customer_churn_data(n_samples=1000):\n",
    "    \"\"\"Create synthetic customer churn dataset\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Customer features\n",
    "    age = np.random.normal(40, 15, n_samples)\n",
    "    tenure = np.random.exponential(24, n_samples)\n",
    "    monthly_charges = np.random.normal(70, 25, n_samples)\n",
    "    total_charges = tenure * monthly_charges + np.random.normal(0, 100, n_samples)\n",
    "    contract_type = np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.3, 0.2])  # Month-to-month, 1-year, 2-year\n",
    "    \n",
    "    # Create churn based on features\n",
    "    churn_prob = 1 / (1 + np.exp(-(\n",
    "        -3 + 0.02 * age - 0.05 * tenure + 0.01 * monthly_charges - 0.8 * contract_type\n",
    "    )))\n",
    "    churn = np.random.binomial(1, churn_prob)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'tenure': tenure,\n",
    "        'monthly_charges': monthly_charges,\n",
    "        'total_charges': total_charges,\n",
    "        'contract_type': contract_type,\n",
    "        'churn': churn\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create and explore the dataset\n",
    "churn_df = create_customer_churn_data(1000)\n",
    "print(\"Customer Churn Dataset:\")\n",
    "print(churn_df.head())\n",
    "print(f\"\\nDataset shape: {churn_df.shape}\")\n",
    "print(f\"Churn rate: {churn_df['churn'].mean():.2%}\")\n",
    "\n",
    "# Visualize relationships\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.boxplot(data=churn_df, x='churn', y='age')\n",
    "plt.title('Age vs Churn')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.boxplot(data=churn_df, x='churn', y='tenure')\n",
    "plt.title('Tenure vs Churn')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.boxplot(data=churn_df, x='churn', y='monthly_charges')\n",
    "plt.title('Monthly Charges vs Churn')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.countplot(data=churn_df, x='contract_type', hue='churn')\n",
    "plt.title('Contract Type vs Churn')\n",
    "plt.xticks([0, 1, 2], ['Month-to-month', '1-year', '2-year'])\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.scatterplot(data=churn_df, x='tenure', y='monthly_charges', hue='churn', alpha=0.6)\n",
    "plt.title('Tenure vs Monthly Charges')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "churn_df['churn'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Churn Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X_churn = churn_df.drop('churn', axis=1)\n",
    "y_churn = churn_df['churn']\n",
    "\n",
    "# Split data\n",
    "X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_churn = StandardScaler()\n",
    "X_train_churn_scaled = scaler_churn.fit_transform(X_train_churn)\n",
    "X_test_churn_scaled = scaler_churn.transform(X_test_churn)\n",
    "\n",
    "# Train models\n",
    "churn_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "churn_results = {}\n",
    "\n",
    "for name, model in churn_models.items():\n",
    "    model.fit(X_train_churn_scaled, y_train_churn)\n",
    "    y_pred = model.predict(X_test_churn_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_churn_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    churn_results[name] = {\n",
    "        'accuracy': accuracy_score(y_test_churn, y_pred),\n",
    "        'precision': precision_score(y_test_churn, y_pred),\n",
    "        'recall': recall_score(y_test_churn, y_pred),\n",
    "        'f1_score': f1_score(y_test_churn, y_pred),\n",
    "        'auc_roc': roc_auc_score(y_test_churn, y_pred_proba) if y_pred_proba is not None else None\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "churn_results_df = pd.DataFrame(churn_results).T\n",
    "print(\"Churn Prediction Model Performance:\")\n",
    "display(churn_results_df)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "churn_results_df.drop('auc_roc', axis=1).plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Churn Prediction Model Performance')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Concepts Summary\n",
    "\n",
    "### What We Learned:\n",
    "1. **Types of Supervised Learning**: Classification (discrete outputs) vs Regression (continuous outputs)\n",
    "2. **Model Training**: How algorithms learn from labeled data\n",
    "3. **Evaluation Metrics**: Accuracy, precision, recall, F1-score for classification\n",
    "4. **Cross-Validation**: Ensuring models generalize well to unseen data\n",
    "5. **Feature Importance**: Understanding which features drive predictions\n",
    "6. **Real-World Applications**: Applying supervised learning to business problems\n",
    "\n",
    "### Best Practices:\n",
    "- Always split data into training and test sets\n",
    "- Use appropriate evaluation metrics for your problem\n",
    "- Scale features when using distance-based algorithms\n",
    "- Perform hyperparameter tuning for optimal performance\n",
    "- Consider class imbalance in classification problems\n",
    "- Validate models on multiple metrics, not just accuracy\n",
    "\n",
    "### Next Steps:\n",
    "- Explore more advanced algorithms (XGBoost, LightGBM)\n",
    "- Learn about ensemble methods and stacking\n",
    "- Study feature engineering techniques\n",
    "- Dive into hyperparameter optimization\n",
    "- Learn about model deployment and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises and Challenges\n",
    "\n",
    "### Exercise 1: Model Comparison\n",
    "Train at least 3 different classification models on the Iris dataset and compare their performance using multiple metrics.\n",
    "\n",
    "### Exercise 2: Hyperparameter Tuning\n",
    "Use GridSearchCV or RandomizedSearchCV to find the best hyperparameters for a Random Forest classifier.\n",
    "\n",
    "### Exercise 3: Feature Engineering\n",
    "Create new features from the customer churn dataset and see if they improve model performance.\n",
    "\n",
    "### Exercise 4: Imbalanced Data\n",
    "Create an imbalanced dataset and apply techniques like SMOTE or class weights to handle the imbalance.\n",
    "\n",
    "### Exercise 5: Cross-Validation\n",
    "Implement k-fold cross-validation and compare the results with a simple train-test split.\n",
    "\n",
    "**Challenge**: Build a complete machine learning pipeline that includes data preprocessing, model training, evaluation, and deployment considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Further Learning Resources\n",
    "\n",
    "### Books:\n",
    "- \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\n",
    "\n",
    "### Online Courses:\n",
    "- Andrew Ng's Machine Learning Course (Coursera)\n",
    "- Fast.ai Practical Deep Learning course\n",
    "- Google's Machine Learning Crash Course\n",
    "\n",
    "### Documentation:\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [NumPy Documentation](https://numpy.org/doc/)\n",
    "\n",
    "### Practice Platforms:\n",
    "- Kaggle (kaggle.com)\n",
    "- DataCamp (datacamp.com)\n",
    "- LeetCode (leetcode.com)\n",
    "\n",
    "### Community:\n",
    "- Stack Overflow\n",
    "- Reddit r/MachineLearning\n",
    "- Towards Data Science (Medium)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}