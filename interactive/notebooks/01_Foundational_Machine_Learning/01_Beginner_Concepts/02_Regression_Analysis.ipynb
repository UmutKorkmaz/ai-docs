{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis: Predicting Continuous Values\n",
    "\n",
    "**Interactive Notebook** - Section 1: Foundational Machine Learning\n",
    "\n",
    "Welcome to your deep dive into regression analysis! In this notebook, you'll learn how to build models that predict continuous values like house prices, temperature, or sales figures.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand the difference between classification and regression\n",
    "- ‚úÖ Master linear regression and its mathematical foundations\n",
    "- ‚úÖ Learn about polynomial regression and overfitting\n",
    "- ‚úÖ Implement regularization techniques (Lasso, Ridge)\n",
    "- ‚úÖ Evaluate regression models with appropriate metrics\n",
    "- ‚úÖ Apply regression to real-world datasets\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Completion of \"01_Introduction_to_Machine_Learning.ipynb\"\n",
    "- Basic understanding of algebra and statistics\n",
    "- Familiarity with Python and scikit-learn\n",
    "\n",
    "**Estimated Time**: 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Installation\n",
    "\n",
    "Let's set up our environment with the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn ipywidgets\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing, make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Understanding Regression vs Classification\n",
    "\n",
    "Let's start by understanding the key differences between regression and classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "ml_comparison = {\n",
    "    'Aspect': [\n",
    "        'Output Type',\n",
    "        'Problem Type',\n",
    "        'Examples',\n",
    "        'Algorithms',\n",
    "        'Evaluation Metrics',\n",
    "        'Visualization'\n",
    "    ],\n",
    "    'Classification': [\n",
    "        'Discrete categories/classes',\n",
    "        'Predict which category',\n",
    "        'Spam detection, Image recognition, Medical diagnosis',\n",
    "        'Logistic Regression, Decision Trees, Random Forest, SVM',\n",
    "        'Accuracy, Precision, Recall, F1-Score, AUC',\n",
    "        'Confusion matrix, ROC curves'\n",
    "    ],\n",
    "    'Regression': [\n",
    "        'Continuous numerical values',\n",
    "        'Predict numerical value',\n",
    "        'House prices, Temperature, Sales forecasting, Stock prices',\n",
    "        'Linear Regression, Polynomial Regression, Gradient Boosting',\n",
    "        'MSE, RMSE, MAE, R¬≤',\n",
    "        'Scatter plots, Residual plots'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(ml_comparison)\n",
    "display(HTML(\"<h3>Classification vs Regression</h3>\"))\n",
    "display(df_comparison.style.background_gradient(cmap='RdYlGn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè† Dataset: California Housing Prices\n",
    "\n",
    "Let's work with the California Housing dataset to predict house prices based on various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "feature_names = housing.feature_names\n",
    "target_name = housing.target_names[0]\n",
    "\n",
    "# Create DataFrame\n",
    "df_housing = pd.DataFrame(X, columns=feature_names)\n",
    "df_housing[target_name] = y\n",
    "\n",
    "print(\"üè† California Housing Dataset Overview\")\n",
    "print(f\"Number of samples: {len(df_housing)}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Target: {target_name}\")\n",
    "print(f\"\\nFeatures: {feature_names}\")\n",
    "\n",
    "# Display dataset statistics\n",
    "display(HTML(\"<h4>Dataset Statistics:</h4>\"))\n",
    "display(df_housing.describe().round(2))\n",
    "\n",
    "# Display first few samples\n",
    "display(HTML(\"<h4>Sample Data:</h4>\"))\n",
    "display(df_housing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the target variable distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y, kde=True, bins=30)\n",
    "plt.title('Distribution of House Prices')\n",
    "plt.xlabel('Median House Value ($100,000s)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=y)\n",
    "plt.title('Box Plot of House Prices')\n",
    "plt.xlabel('Median House Value ($100,000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Target Variable Statistics:\")\n",
    "print(f\"Mean: ${np.mean(y)*100000:,.0f}\")\n",
    "print(f\"Median: ${np.median(y)*100000:,.0f}\")\n",
    "print(f\"Standard Deviation: ${np.std(y)*100000:,.0f}\")\n",
    "print(f\"Min: ${np.min(y)*100000:,.0f}\")\n",
    "print(f\"Max: ${np.max(y)*100000:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df_housing.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find features most correlated with target\n",
    "target_correlations = correlation_matrix[target_name].drop(target_name).sort_values(ascending=False)\n",
    "print(\"üîó Feature Correlations with House Prices:\")\n",
    "for feature, correlation in target_correlations.items():\n",
    "    print(f\"{feature}: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Linear Regression: The Foundation\n",
    "\n",
    "Linear regression is the simplest regression algorithm. It assumes a linear relationship between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"üìä Data Preparation Complete\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_scaled)\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_regression(y_true, y_pred, dataset_name=\"Dataset\"):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"üìä {dataset_name} Performance:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    \n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "print(\"üéØ Linear Regression Results\")\n",
    "print(\"=\"*50)\n",
    "train_mse, train_rmse, train_mae, train_r2 = evaluate_regression(y_train, y_train_pred, \"Training\")\n",
    "print()\n",
    "test_mse, test_rmse, test_mae, test_r2 = evaluate_regression(y_test, y_test_pred, \"Testing\")\n",
    "\n",
    "# Display coefficients\n",
    "print(\"\\nüìà Model Coefficients:\")\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "for idx, row in coefficients.iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Coefficient']:.4f}\")\n",
    "\n",
    "print(f\"\\nIntercept: {lr_model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_train_pred, alpha=0.6, s=20)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Training Set (R¬≤ = {train_r2:.3f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6, s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Testing Set (R¬≤ = {test_r2:.3f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Residual Analysis:\")\n",
    "print(f\"Mean of residuals: {np.mean(residuals):.4f}\")\n",
    "print(f\"Standard deviation of residuals: {np.std(residuals):.4f}\")\n",
    "print(f\"Skewness: {stats.skew(residuals):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(residuals):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Interactive Linear Regression Explorer\n",
    "\n",
    "Let's create an interactive tool to explore how linear regression works with different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive linear regression explorer\n",
    "feature1_dropdown = widgets.Dropdown(\n",
    "    options=feature_names,\n",
    "    value='MedInc',  # Most correlated feature\n",
    "    description='Feature 1:'\n",
    ")\n",
    "\n",
    "feature2_dropdown = widgets.Dropdown(\n",
    "    options=['None'] + list(feature_names),\n",
    "    value='AveRooms',\n",
    "    description='Feature 2:'\n",
    ")\n",
    "\n",
    "show_plane_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show Regression Plane (3D)'\n",
    ")\n",
    "\n",
    "explore_button = widgets.Button(description='Explore Regression', button_style='info')\n",
    "explore_output = widgets.Output()\n",
    "\n",
    "def explore_regression(b):\n",
    "    with explore_output:\n",
    "        explore_output.clear_output()\n",
    "        \n",
    "        # Get selected features\n",
    "        feat1 = feature1_dropdown.value\n",
    "        feat2 = feature2_dropdown.value\n",
    "        \n",
    "        # Prepare data\n",
    "        if feat2 == 'None':\n",
    "            # Simple linear regression\n",
    "            X_simple = df_housing[[feat1]].values\n",
    "            y_simple = df_housing[target_name].values\n",
    "            \n",
    "            # Fit model\n",
    "            lr_simple = LinearRegression()\n",
    "            lr_simple.fit(X_simple, y_simple)\n",
    "            y_pred_simple = lr_simple.predict(X_simple)\n",
    "            \n",
    "            # Calculate R¬≤\n",
    "            r2_simple = r2_score(y_simple, y_pred_simple)\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(X_simple, y_simple, alpha=0.6, s=20, label='Data')\n",
    "            plt.plot(X_simple, y_pred_simple, 'r-', linewidth=2, label='Regression Line')\n",
    "            plt.xlabel(feat1)\n",
    "            plt.ylabel(target_name)\n",
    "            plt.title(f'Simple Linear Regression (R¬≤ = {r2_simple:.3f})')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"üìà Simple Linear Regression Results:\")\n",
    "            print(f\"Feature: {feat1}\")\n",
    "            print(f\"Coefficient: {lr_simple.coef_[0]:.4f}\")\n",
    "            print(f\"Intercept: {lr_simple.intercept_:.4f}\")\n",
    "            print(f\"R¬≤: {r2_simple:.3f}\")\n",
    "            \n",
    "        else:\n",
    "            # Multiple linear regression\n",
    "            X_multi = df_housing[[feat1, feat2]].values\n",
    "            y_multi = df_housing[target_name].values\n",
    "            \n",
    "            # Fit model\n",
    "            lr_multi = LinearRegression()\n",
    "            lr_multi.fit(X_multi, y_multi)\n",
    "            y_pred_multi = lr_multi.predict(X_multi)\n",
    "            \n",
    "            # Calculate R¬≤\n",
    "            r2_multi = r2_score(y_multi, y_pred_multi)\n",
    "            \n",
    "            if show_plane_checkbox.value:\n",
    "                # 3D plot with regression plane\n",
    "                from mpl_toolkits.mplot3d import Axes3D\n",
    "                \n",
    "                fig = plt.figure(figsize=(12, 8))\n",
    "                ax = fig.add_subplot(111, projection='3d')\n",
    "                \n",
    "                # Create meshgrid for plane\n",
    "                x1_range = np.linspace(X_multi[:, 0].min(), X_multi[:, 0].max(), 20)\n",
    "                x2_range = np.linspace(X_multi[:, 1].min(), X_multi[:, 1].max(), 20)\n",
    "                X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "                \n",
    "                # Calculate plane\n",
    "                Y_grid = (lr_multi.coef_[0] * X1_grid + \n",
    "                         lr_multi.coef_[1] * X2_grid + \n",
    "                         lr_multi.intercept_)\n",
    "                \n",
    "                # Plot\n",
    "                ax.scatter(X_multi[:, 0], X_multi[:, 1], y_multi, alpha=0.6, s=20)\n",
    "                ax.plot_surface(X1_grid, X2_grid, Y_grid, alpha=0.3, color='red')\n",
    "                ax.set_xlabel(feat1)\n",
    "                ax.set_ylabel(feat2)\n",
    "                ax.set_zlabel(target_name)\n",
    "                ax.set_title(f'Multiple Linear Regression (R¬≤ = {r2_multi:.3f})')\n",
    "                \n",
    "                plt.show()\n",
    "            else:\n",
    "                # 2D plot with color mapping\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                scatter = plt.scatter(X_multi[:, 0], X_multi[:, 1], c=y_multi, alpha=0.6, s=30, cmap='viridis')\n",
    "                plt.colorbar(scatter, label=target_name)\n",
    "                plt.xlabel(feat1)\n",
    "                plt.ylabel(feat2)\n",
    "                plt.title(f'Multiple Linear Regression Features (R¬≤ = {r2_multi:.3f})')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "            \n",
    "            print(f\"üìà Multiple Linear Regression Results:\")\n",
    "            print(f\"Features: {feat1}, {feat2}\")\n",
    "            print(f\"Coefficient ({feat1}): {lr_multi.coef_[0]:.4f}\")\n",
    "            print(f\"Coefficient ({feat2}): {lr_multi.coef_[1]:.4f}\")\n",
    "            print(f\"Intercept: {lr_multi.intercept_:.4f}\")\n",
    "            print(f\"R¬≤: {r2_multi:.3f}\")\n",
    "\n",
    "explore_button.on_click(explore_regression)\n",
    "\n",
    "print(\"üéõÔ∏è Interactive Linear Regression Explorer\")\n",
    "print(\"Select features to explore their relationship with house prices!\")\n",
    "display(widgets.VBox([\n",
    "    feature1_dropdown, feature2_dropdown, show_plane_checkbox, \n",
    "    explore_button, explore_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Polynomial Regression: Capturing Non-linear Relationships\n",
    "\n",
    "Sometimes the relationship between features and target isn't linear. Let's explore polynomial regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic non-linear data\n",
    "np.random.seed(42)\n",
    "X_synthetic = np.linspace(0, 10, 100)\n",
    "y_synthetic = 2 * X_synthetic**2 - 3 * X_synthetic + 1 + np.random.normal(0, 2, 100)\n",
    "\n",
    "X_synthetic = X_synthetic.reshape(-1, 1)\n",
    "\n",
    "# Fit different polynomial degrees\n",
    "degrees = [1, 2, 3, 5]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X_synthetic)\n",
    "    \n",
    "    # Fit linear regression\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_poly, y_synthetic)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = lr_poly.predict(X_poly)\n",
    "    \n",
    "    # Calculate R¬≤\n",
    "    r2 = r2_score(y_synthetic, y_pred)\n",
    "    \n",
    "    # Plot\n",
    "    plt.scatter(X_synthetic, y_synthetic, alpha=0.6, s=30, label='Data')\n",
    "    plt.plot(X_synthetic, y_pred, color=colors[i], linewidth=2, label=f'Degree {degree}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Polynomial Regression (Degree {degree}, R¬≤ = {r2:.3f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Polynomial Regression Analysis:\")\n",
    "print(\"Higher degree polynomials can fit the training data better,\")\n",
    "print(\"but may lead to overfitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply polynomial regression to housing data\n",
    "# Let's focus on the most correlated feature\n",
    "best_feature = target_correlations.index[0]  # MedInc\n",
    "X_best = df_housing[[best_feature]].values\n",
    "y_best = df_housing[target_name].values\n",
    "\n",
    "# Split data\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
    "    X_best, y_best, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Try different polynomial degrees\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly_transformed = poly_features.fit_transform(X_train_poly)\n",
    "    X_test_poly_transformed = poly_features.transform(X_test_poly)\n",
    "    \n",
    "    # Fit model\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_train_poly_transformed, y_train_poly)\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_r2 = lr_poly.score(X_train_poly_transformed, y_train_poly)\n",
    "    test_r2 = lr_poly.score(X_test_poly_transformed, y_test_poly)\n",
    "    \n",
    "    train_scores.append(train_r2)\n",
    "    test_scores.append(test_r2)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_scores, 'o-', linewidth=2, markersize=8, label='Training R¬≤')\n",
    "plt.plot(degrees, test_scores, 'o-', linewidth=2, markersize=8, label='Testing R¬≤')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Polynomial Degree vs Model Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find best degree\n",
    "best_degree = degrees[np.argmax(test_scores)]\n",
    "print(f\"üèÜ Best polynomial degree: {best_degree}\")\n",
    "print(f\"Best testing R¬≤: {max(test_scores):.3f}\")\n",
    "\n",
    "# Compare with linear regression\n",
    "lr_simple = LinearRegression()\n",
    "lr_simple.fit(X_train_poly, y_train_poly)\n",
    "simple_r2 = lr_simple.score(X_test_poly, y_test_poly)\n",
    "print(f\"Linear regression R¬≤: {simple_r2:.3f}\")\n",
    "print(f\"Improvement with polynomial: {max(test_scores) - simple_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Regularization: Preventing Overfitting\n",
    "\n",
    "Regularization helps prevent overfitting by adding a penalty term to the loss function. Let's explore L1 (Lasso) and L2 (Ridge) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization techniques\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=1.0),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "print(\"üèÜ Regularization Model Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  R¬≤: {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = min(results.keys(), key=lambda x: results[x]['mse'])\n",
    "print(f\"üèÜ Best Model: {best_model_name} (MSE: {results[best_model_name]['mse']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore regularization strength impact\n",
    "ridge_train_scores = []\n",
    "ridge_test_scores = []\n",
    "lasso_train_scores = []\n",
    "lasso_test_scores = []\n",
    "lasso_coefficients = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    ridge_train_scores.append(ridge.score(X_train_scaled, y_train))\n",
    "    ridge_test_scores.append(ridge.score(X_test_scaled, y_test))\n",
    "    \n",
    "    # Lasso\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_train_scores.append(lasso.score(X_train_scaled, y_train))\n",
    "    lasso_test_scores.append(lasso.score(X_test_scaled, y_test))\n",
    "    lasso_coefficients.append(lasso.coef_)\n",
    "\n",
    "# Plot regularization impact\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.semilogx(alphas, ridge_train_scores, 'o-', linewidth=2, markersize=8, label='Training')\n",
    "plt.semilogx(alphas, ridge_test_scores, 'o-', linewidth=2, markersize=8, label='Testing')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Ridge Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.semilogx(alphas, lasso_train_scores, 'o-', linewidth=2, markersize=8, label='Training')\n",
    "plt.semilogx(alphas, lasso_test_scores, 'o-', linewidth=2, markersize=8, label='Testing')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Lasso Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "lasso_coefficients = np.array(lasso_coefficients)\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.semilogx(alphas, lasso_coefficients[:, i], 'o-', linewidth=2, markersize=6, label=feature)\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Lasso Coefficients')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Regularization Analysis:\")\n",
    "print(\"- Ridge: Shrinks coefficients but rarely sets them to zero\")\n",
    "print(\"- Lasso: Can set coefficients to zero (feature selection)\")\n",
    "print(\"- Higher alpha = stronger regularization = simpler models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Interactive Model Comparison\n",
    "\n",
    "Let's create an interactive tool to compare different regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive model comparison\n",
    "model_selector = widgets.SelectMultiple(\n",
    "    options=['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet', 'Random Forest'],\n",
    "    value=['Linear Regression', 'Ridge', 'Lasso'],\n",
    "    description='Models:'\n",
    ")\n",
    "\n",
    "test_size_slider = widgets.FloatSlider(\n",
    "    value=0.3, min=0.1, max=0.5, step=0.05, description='Test Size:'\n",
    ")\n",
    "\n",
    "cross_val_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Use Cross-Validation'\n",
    ")\n",
    "\n",
    "compare_button = widgets.Button(description='Compare Models', button_style='success')\n",
    "comparison_output = widgets.Output()\n",
    "\n",
    "def compare_models_interactive(b):\n",
    "    with comparison_output:\n",
    "        comparison_output.clear_output()\n",
    "        \n",
    "        # Split data\n",
    "        X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(\n",
    "            X, y, test_size=test_size_slider.value, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler_cv = StandardScaler()\n",
    "        X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n",
    "        X_test_cv_scaled = scaler_cv.transform(X_test_cv)\n",
    "        \n",
    "        # Initialize models\n",
    "        model_dict = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge': Ridge(alpha=1.0),\n",
    "            'Lasso': Lasso(alpha=1.0),\n",
    "            'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        \n",
    "        selected_models = [model_dict[name] for name in model_selector.value]\n",
    "        \n",
    "        # Compare models\n",
    "        results = {}\n",
    "        \n",
    "        print(f\"üéØ Model Comparison Results\")\n",
    "        print(f\"Test Size: {test_size_slider.value:.0%}\")\n",
    "        print(f\"Cross-Validation: {cross_val_checkbox.value}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i, (model, name) in enumerate(zip(selected_models, model_selector.value)):\n",
    "            if cross_val_checkbox.value:\n",
    "                # Use cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_cv_scaled, y_train_cv, cv=5, scoring='r2')\n",
    "                mean_score = cv_scores.mean()\n",
    "                std_score = cv_scores.std()\n",
    "                \n",
    "                # Train on full training set for final evaluation\n",
    "                model.fit(X_train_cv_scaled, y_train_cv)\n",
    "                test_score = model.score(X_test_cv_scaled, y_test_cv)\n",
    "                \n",
    "                results[name] = {\n",
    "                    'cv_mean': mean_score,\n",
    "                    'cv_std': std_score,\n",
    "                    'test_score': test_score\n",
    "                }\n",
    "                \n",
    "                print(f\"{name}:\")\n",
    "                print(f\"  CV R¬≤: {mean_score:.4f} (¬±{std_score:.4f})\")\n",
    "                print(f\"  Test R¬≤: {test_score:.4f}\")\n",
    "            else:\n",
    "                # Simple train-test split\n",
    "                model.fit(X_train_cv_scaled, y_train_cv)\n",
    "                train_score = model.score(X_train_cv_scaled, y_train_cv)\n",
    "                test_score = model.score(X_test_cv_scaled, y_test_cv)\n",
    "                \n",
    "                results[name] = {\n",
    "                    'train_score': train_score,\n",
    "                    'test_score': test_score\n",
    "                }\n",
    "                \n",
    "                print(f\"{name}:\")\n",
    "                print(f\"  Train R¬≤: {train_score:.4f}\")\n",
    "                print(f\"  Test R¬≤: {test_score:.4f}\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        if cross_val_checkbox.value:\n",
    "            model_names = list(results.keys())\n",
    "            cv_means = [results[name]['cv_mean'] for name in model_names]\n",
    "            cv_stds = [results[name]['cv_std'] for name in model_names]\n",
    "            test_scores = [results[name]['test_score'] for name in model_names]\n",
    "            \n",
    "            x_pos = np.arange(len(model_names))\n",
    "            \n",
    "            plt.bar(x_pos - 0.2, cv_means, 0.4, yerr=cv_stds, label='CV Score', alpha=0.7)\n",
    "            plt.bar(x_pos + 0.2, test_scores, 0.4, label='Test Score', alpha=0.7)\n",
    "            \n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('R¬≤ Score')\n",
    "            plt.title('Model Comparison with Cross-Validation')\n",
    "            plt.xticks(x_pos, model_names, rotation=45)\n",
    "            plt.legend()\n",
    "        else:\n",
    "            model_names = list(results.keys())\n",
    "            train_scores = [results[name]['train_score'] for name in model_names]\n",
    "            test_scores = [results[name]['test_score'] for name in model_names]\n",
    "            \n",
    "            x_pos = np.arange(len(model_names))\n",
    "            \n",
    "            plt.bar(x_pos - 0.2, train_scores, 0.4, label='Training Score', alpha=0.7)\n",
    "            plt.bar(x_pos + 0.2, test_scores, 0.4, label='Test Score', alpha=0.7)\n",
    "            \n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('R¬≤ Score')\n",
    "            plt.title('Model Comparison (Train vs Test)')\n",
    "            plt.xticks(x_pos, model_names, rotation=45)\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "compare_button.on_click(compare_models_interactive)\n",
    "\n",
    "print(\"üéõÔ∏è Interactive Model Comparison\")\n",
    "print(\"Select models to compare and explore their performance!\")\n",
    "display(widgets.VBox([\n",
    "    model_selector, test_size_slider, cross_val_checkbox, \n",
    "    compare_button, comparison_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Challenge Exercises\n",
    "\n",
    "Test your understanding with these regression challenges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Feature Engineering\n",
    "print(\"üèÜ Challenge 1: Feature Engineering\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Create new features to improve model performance\")\n",
    "\n",
    "# Create new features\n",
    "df_engineered = df_housing.copy()\n",
    "\n",
    "# Interaction features\n",
    "df_engineered['rooms_per_person'] = df_engineered['AveRooms'] / df_engineered['AveOccup']\n",
    "df_engineered['bedrooms_per_room'] = df_engineered['AveBedrms'] / df_engineered['AveRooms']\n",
    "df_engineered['income_per_room'] = df_engineered['MedInc'] / df_engineered['AveRooms']\n",
    "\n",
    "# Polynomial features for most important feature\n",
    "df_engineered['MedInc_squared'] = df_engineered['MedInc'] ** 2\n",
    "df_engineered['MedInc_cubed'] = df_engineered['MedInc'] ** 3\n",
    "\n",
    "# Binning features\n",
    "df_engineered['age_category'] = pd.cut(df_engineered['HouseAge'], \n",
    "                                     bins=[0, 10, 30, 52], \n",
    "                                     labels=['New', 'Middle-aged', 'Old'])\n",
    "\n",
    "print(\"‚úÖ New features created:\")\n",
    "print(\"- rooms_per_person: Average rooms per person\")\n",
    "print(\"- bedrooms_per_room: Bedroom to room ratio\")\n",
    "print(\"- income_per_room: Income per room\")\n",
    "print(\"- MedInc_squared/cubed: Polynomial income features\")\n",
    "print(\"- age_category: House age categories\")\n",
    "\n",
    "# Prepare data\n",
    "df_engineered = pd.get_dummies(df_engineered, drop_first=True)\n",
    "X_engineered = df_engineered.drop(target_name, axis=1).values\n",
    "y_engineered = df_engineered[target_name].values\n",
    "\n",
    "# Split and scale\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(\n",
    "    X_engineered, y_engineered, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_eng = StandardScaler()\n",
    "X_train_eng_scaled = scaler_eng.fit_transform(X_train_eng)\n",
    "X_test_eng_scaled = scaler_eng.transform(X_test_eng)\n",
    "\n",
    "# Train models\n",
    "lr_original = LinearRegression()\n",
    "lr_engineered = LinearRegression()\n",
    "\n",
    "lr_original.fit(X_train_scaled, y_train)\n",
    "lr_engineered.fit(X_train_eng_scaled, y_train_eng)\n",
    "\n",
    "# Evaluate\n",
    "original_score = lr_original.score(X_test_scaled, y_test)\n",
    "engineered_score = lr_engineered.score(X_test_eng_scaled, y_test_eng)\n",
    "\n",
    "print(f\"\\nüìä Performance Comparison:\")\n",
    "print(f\"Original features R¬≤: {original_score:.4f}\")\n",
    "print(f\"Engineered features R¬≤: {engineered_score:.4f}\")\n",
    "print(f\"Improvement: {(engineered_score - original_score):.4f}\")\n",
    "\n",
    "if engineered_score > original_score:\n",
    "    print(\"üéâ Feature engineering improved model performance!\")\n",
    "else:\n",
    "    print(\"ü§î Original features performed better. Feature engineering requires careful experimentation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Hyperparameter Tuning\n",
    "print(\"üèÜ Challenge 2: Hyperparameter Tuning\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Find optimal hyperparameters for Ridge regression\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "ridge = Ridge()\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"üèÜ Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"üèÜ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_ridge = grid_search.best_estimator_\n",
    "test_score = best_ridge.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"üèÜ Test score: {test_score:.4f}\")\n",
    "\n",
    "# Compare with default Ridge\n",
    "default_ridge = Ridge(alpha=1.0)\n",
    "default_ridge.fit(X_train_scaled, y_train)\n",
    "default_score = default_ridge.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "print(f\"Default Ridge R¬≤: {default_score:.4f}\")\n",
    "print(f\"Tuned Ridge R¬≤: {test_score:.4f}\")\n",
    "print(f\"Improvement: {(test_score - default_score):.4f}\")\n",
    "\n",
    "# Visualize grid search results\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='mean_test_score', \n",
    "    index='param_alpha', \n",
    "    columns='param_solver'\n",
    ")\n",
    "\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "plt.title('Grid Search Results: Ridge Regression')\n",
    "plt.xlabel('Solver')\n",
    "plt.ylabel('Alpha')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Real-world Application\n",
    "print(\"üèÜ Challenge 3: Sales Forecasting\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Apply regression techniques to a sales forecasting problem\")\n",
    "\n",
    "# Create synthetic sales data\n",
    "np.random.seed(42)\n",
    "n_months = 60\n",
    "\n",
    "# Time series with trend and seasonality\n",
    "time = np.arange(n_months)\n",
    "trend = 0.5 * time\n",
    "seasonality = 20 * np.sin(2 * np.pi * time / 12)  # Annual seasonality\n",
    "noise = np.random.normal(0, 10, n_months)\n",
    "\n",
    "sales = 100 + trend + seasonality + noise\n",
    "\n",
    "# Create features\n",
    "sales_data = pd.DataFrame({\n",
    "    'month': time + 1,\n",
    "    'sales': sales,\n",
    "    'trend': trend,\n",
    "    'seasonality': seasonality\n",
    "})\n",
    "\n",
    "# Add lag features\n",
    "for lag in [1, 2, 3, 6, 12]:\n",
    "    sales_data[f'sales_lag_{lag}'] = sales_data['sales'].shift(lag)\n",
    "\n",
    "# Add rolling features\n",
    "sales_data['sales_ma_3'] = sales_data['sales'].rolling(3).mean()\n",
    "sales_data['sales_ma_6'] = sales_data['sales'].rolling(6).mean()\n",
    "\n",
    "# Drop missing values\n",
    "sales_data = sales_data.dropna()\n",
    "\n",
    "print(\"üìà Sales Forecasting Dataset\")\n",
    "display(sales_data.head())\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['month', 'trend', 'seasonality', 'sales_lag_1', 'sales_lag_2', \n",
    "               'sales_lag_3', 'sales_ma_3', 'sales_ma_6']\n",
    "\n",
    "X_sales = sales_data[feature_cols].values\n",
    "y_sales = sales_data['sales'].values\n",
    "\n",
    "# Split data (time series split)\n",
    "split_point = int(0.8 * len(X_sales))\n",
    "X_train_sales = X_sales[:split_point]\n",
    "X_test_sales = X_sales[split_point:]\n",
    "y_train_sales = y_sales[:split_point]\n",
    "y_test_sales = y_sales[split_point:]\n",
    "\n",
    "# Scale features\n",
    "scaler_sales = StandardScaler()\n",
    "X_train_sales_scaled = scaler_sales.fit_transform(X_train_sales)\n",
    "X_test_sales_scaled = scaler_sales.transform(X_test_sales)\n",
    "\n",
    "# Train models\n",
    "models_sales = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results_sales = {}\n",
    "\n",
    "for name, model in models_sales.items():\n",
    "    model.fit(X_train_sales_scaled, y_train_sales)\n",
    "    y_pred_sales = model.predict(X_test_sales_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test_sales, y_pred_sales)\n",
    "    mae = mean_absolute_error(y_test_sales, y_pred_sales)\n",
    "    r2 = r2_score(y_test_sales, y_pred_sales)\n",
    "    \n",
    "    results_sales[name] = {'mse': mse, 'mae': mae, 'r2': r2, 'predictions': y_pred_sales}\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse:.2f}\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  R¬≤: {r2:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_data['month'][split_point:], y_test_sales, 'b-', linewidth=2, label='Actual')\n",
    "plt.plot(sales_data['month'][split_point:], results_sales['Random Forest']['predictions'], \n",
    "         'r--', linewidth=2, label='Random Forest Prediction')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Sales Forecasting: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Sales forecasting complete! Time series regression can be powerful for business applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "‚úÖ **Regression vs Classification**: Understanding when to predict continuous values vs categories\n",
    "\n",
    "‚úÖ **Linear Regression**: The foundation of regression with mathematical interpretation\n",
    "\n",
    "‚úÖ **Model Evaluation**: MSE, RMSE, MAE, and R¬≤ metrics for regression problems\n",
    "\n",
    "‚úÖ **Residual Analysis**: Checking model assumptions and diagnostics\n",
    "\n",
    "‚úÖ **Polynomial Regression**: Capturing non-linear relationships\n",
    "\n",
    "‚úÖ **Regularization**: Preventing overfitting with Ridge, Lasso, and ElasticNet\n",
    "\n",
    "‚úÖ **Feature Engineering**: Creating new features to improve performance\n",
    "\n",
    "‚úÖ **Hyperparameter Tuning**: Finding optimal model parameters\n",
    "\n",
    "‚úÖ **Real-world Applications**: Sales forecasting and time series regression\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Overfitting**: When models perform well on training data but poorly on test data\n",
    "- **Regularization**: Adding penalty terms to prevent overfitting\n",
    "- **Cross-validation**: Robust evaluation technique for model selection\n",
    "- **Feature importance**: Understanding which features drive predictions\n",
    "- **Model diagnostics**: Residual analysis to check model assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "### Continue Your Learning Journey:\n",
    "\n",
    "1. **üìö Next Notebook**: \"03_Advanced_Classification_Techniques.ipynb\" - Explore sophisticated classification methods\n",
    "\n",
    "2. **üéØ Practice Challenges**: Try regression on different datasets from sklearn or Kaggle\n",
    "\n",
    "3. **üåê Real Data**: Download datasets from UCI Machine Learning Repository\n",
    "\n",
    "4. **üìñ Recommended Reading\":\n",
    "   - \"Introduction to Statistical Learning\" - Chapter 3 on Linear Regression\n",
    "   - \"The Elements of Statistical Learning\" - Advanced regression techniques\n",
    "\n",
    "5. **üõ†Ô∏è Advanced Topics to Explore**:\n",
    "   - Gradient Boosting Machines (XGBoost, LightGBM)\n",
    "   - Support Vector Regression\n",
    "   - Neural Networks for regression\n",
    "   - Time series forecasting methods (ARIMA, Prophet)\n",
    "\n",
    "### Quick Self-Assessment:\n",
    "\n",
    "Can you explain:\n",
    "- The difference between MSE, MAE, and RMSE?\n",
    "- Why we need regularization in regression models?\n",
    "- How to interpret R¬≤ score?\n",
    "- What residual analysis tells us about model quality?\n",
    "- When to use polynomial regression vs linear regression?\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've completed the Regression Analysis notebook! You now have solid skills in predicting continuous values and understanding regression model behavior.\n",
    "\n",
    "**Remember**: Regression is a fundamental skill in machine learning with countless real-world applications. Keep practicing with different datasets and problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}