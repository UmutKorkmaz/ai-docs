{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models\n",
    "\n",
    "**Interactive Notebook** - Section 9: Large Language Models\n",
    "\n",
    "Welcome to the revolutionary world of Large Language Models! This notebook will guide you through the fundamental concepts of LLMs, from basic transformer architecture to practical applications and fine-tuning techniques.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand the transformer architecture that powers LLMs\n",
    "- ‚úÖ Learn about attention mechanisms and self-attention\n",
    "- ‚úÖ Explore popular LLMs (GPT, BERT, LLaMA, etc.)\n",
    "- ‚úÖ Implement text generation and completion tasks\n",
    "- ‚úÖ Learn about fine-tuning and prompt engineering\n",
    "- ‚úÖ Build practical LLM applications\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Completion of \"Introduction to Deep Learning\" notebook\n",
    "- Understanding of neural networks and deep learning\n",
    "- Basic NLP concepts (tokens, embeddings, sequences)\n",
    "- Python programming and PyTorch/TensorFlow basics\n",
    "\n",
    "**Estimated Time**: 4-5 hours\n",
    "\n",
    "‚ö†Ô∏è **Note**: Some exercises require API access to LLM services. Free tier options are provided where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Installation\n",
    "\n",
    "Let's set up our environment with the necessary libraries for working with Large Language Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch datasets accelerate sentencepiece tokenizers\n",
    "!pip install -q matplotlib seaborn numpy pandas scikit-learn ipywidgets\n",
    "!pip install -q openai cohere huggingface-hub\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
    "    GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer, BertModel,\n",
    "    TrainingArguments, Trainer, pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ LLM environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† What are Large Language Models?\n",
    "\n",
    "Large Language Models are deep learning models trained on massive amounts of text data that can understand and generate human-like text. They're based on the transformer architecture and use self-attention mechanisms to process sequential data.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Transformers**: The neural network architecture that revolutionized NLP\n",
    "2. **Attention Mechanisms**: Allow models to focus on relevant parts of input\n",
    "3. **Self-Attention**: Enables understanding relationships between words in a sequence\n",
    "4. **Positional Encoding**: Provides information about word positions\n",
    "5. **Tokenization**: Breaking text into smaller units (tokens)\n",
    "6. **Embeddings**: Vector representations of tokens\n",
    "\n",
    "### Popular LLMs:\n",
    "- **GPT (Generative Pre-trained Transformer)**: OpenAI's series of models\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Google's model\n",
    "- **LLaMA**: Meta's open-source large language model\n",
    "- **Claude**: Anthropic's AI assistant\n",
    "- **Gemini**: Google's multimodal AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LLM evolution and architecture\n",
    "def visualize_llm_evolution():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Large Language Models: Evolution and Architecture', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # LLM Evolution Timeline\n",
    "    ax = axes[0, 0]\n",
    "    models = [\n",
    "        ('ELMo', 2018, 94),\n",
    "        ('GPT-1', 2018, 117),\n",
    "        ('BERT', 2018, 340),\n",
    "        ('GPT-2', 2019, 1500),\n",
    "        ('GPT-3', 2020, 175000),\n",
    "        ('LLaMA', 2023, 65000),\n",
    "        ('GPT-4', 2023, 1000000),\n",
    "        ('Claude 2', 2023, 100000),\n",
    "        ('Gemini', 2023, 1000000)\n",
    "    ]\n",
    "    \n",
    "    years = [model[1] for model in models]\n",
    "    params = [model[2] for model in models]\n",
    "    names = [model[0] for model in models]\n",
    "    \n",
    "    # Create bubble chart\n",
    "    scatter = ax.scatter(years, np.log10(params), s=[p/1000 for p in params], c=range(len(models)), cmap='viridis', alpha=0.7)\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Parameters (log10 scale)')\n",
    "    ax.set_title('LLM Evolution Over Time')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, (name, year, param) in enumerate(models):\n",
    "        ax.annotate(name, (year, np.log10(param)), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Transformer Architecture\n",
    "    ax = axes[0, 1]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 8)\n",
    "    \n",
    "    # Input embedding\n",
    "    ax.add_patch(plt.Rectangle((1, 6), 1, 1, fill=True, color='lightblue', alpha=0.7))\n",
    "    ax.text(1.5, 6.5, 'Input\\nEmbedding', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Positional encoding\n",
    "    ax.add_patch(plt.Rectangle((2.5, 6), 1, 1, fill=True, color='lightgreen', alpha=0.7))\n",
    "    ax.text(3, 6.5, 'Positional\\nEncoding', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Encoder layers\n",
    "    for i in range(3):\n",
    "        y_pos = 4 - i*1.2\n",
    "        ax.add_patch(plt.Rectangle((4, y_pos), 2, 0.8, fill=True, color='lightcoral', alpha=0.7))\n",
    "        ax.text(5, y_pos+0.4, f'Encoder\\nLayer {i+1}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Decoder layers\n",
    "    for i in range(3):\n",
    "        y_pos = 4 - i*1.2\n",
    "        ax.add_patch(plt.Rectangle((7, y_pos), 2, 0.8, fill=True, color='lightyellow', alpha=0.7))\n",
    "        ax.text(8, y_pos+0.4, f'Decoder\\nLayer {i+1}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Output\n",
    "    ax.add_patch(plt.Rectangle((9.5, 3), 0.5, 3, fill=True, color='lightpink', alpha=0.7))\n",
    "    ax.text(9.75, 4.5, 'Output', ha='center', va='center', fontsize=8, rotation=90)\n",
    "    \n",
    "    ax.set_title('Transformer Architecture')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    ax = axes[1, 0]\n",
    "    # Create attention heatmap\n",
    "    attention_matrix = np.random.rand(8, 8)\n",
    "    attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    im = ax.imshow(attention_matrix, cmap='Blues', aspect='equal')\n",
    "    ax.set_title('Self-Attention Mechanism')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    \n",
    "    # Add word labels\n",
    "    words = ['The', 'cat', 'sat', 'on', 'the', 'mat', 'and', 'purred']\n",
    "    ax.set_xticks(range(8))\n",
    "    ax.set_yticks(range(8))\n",
    "    ax.set_xticklabels(words, rotation=45)\n",
    "    ax.set_yticklabels(words)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    # LLM Applications\n",
    "    ax = axes[1, 1]\n",
    "    applications = [\n",
    "        'Text Generation', 'Translation', 'Summarization', 'Question Answering',\n",
    "        'Code Generation', 'Sentiment Analysis', 'Named Entity Recognition', 'Dialogue Systems'\n",
    "    ]\n",
    "    \n",
    "    # Create a circular layout\n",
    "    angles = np.linspace(0, 2*np.pi, len(applications), endpoint=False)\n",
    "    x = np.cos(angles) * 0.8\n",
    "    y = np.sin(angles) * 0.8\n",
    "    \n",
    "    # Draw connections to center\n",
    "    for xi, yi, app in zip(x, y, applications):\n",
    "        ax.plot([0, xi], [0, yi], 'gray', alpha=0.3)\n",
    "        ax.scatter(xi, yi, s=200, c='lightblue', alpha=0.7)\n",
    "        ax.text(xi*1.2, yi*1.2, app, ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Center node\n",
    "    ax.scatter(0, 0, s=300, c='red', alpha=0.8)\n",
    "    ax.text(0, 0, 'LLM', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_title('LLM Applications')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_llm_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Understanding Tokenization\n",
    "\n",
    "Tokenization is the first step in working with LLMs. It breaks down text into smaller units called tokens that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore tokenization with different models\n",
    "def explore_tokenization():\n",
    "    print(\"üîç Exploring Tokenization\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Sample texts\n",
    "    texts = [\n",
    "        \"Hello, world!\",\n",
    "        \"Large Language Models are amazing!\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Artificial Intelligence will revolutionize technology.\"\n",
    "    ]\n",
    "    \n",
    "    # Load different tokenizers\n",
    "    tokenizers = {\n",
    "        'GPT-2': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "        'BERT': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        'T5': AutoTokenizer.from_pretrained('t5-small')\n",
    "    }\n",
    "    \n",
    "    # Tokenize each text with each tokenizer\n",
    "    for text in texts:\n",
    "        print(f\"\\nüìù Text: '{text}'\")\n",
    "        print(f\"Characters: {len(text)}\")\n",
    "        print(f\"Words: {len(text.split())}\")\n",
    "        \n",
    "        for name, tokenizer in tokenizers.items():\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            print(f\"\\n  {name}:\")\n",
    "            print(f\"    Tokens: {tokens}\")\n",
    "            print(f\"    Number of tokens: {len(tokens)}\")\n",
    "            print(f\"    Token IDs: {token_ids}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "    \n",
    "    # Visualize tokenization comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Tokenization Comparison Across Models', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    text_to_analyze = \"The transformer architecture has revolutionized natural language processing.\"\n",
    "    \n",
    "    for i, (name, tokenizer) in enumerate(tokenizers.items()):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        tokens = tokenizer.tokenize(text_to_analyze)\n",
    "        \n",
    "        # Create token length visualization\n",
    "        token_lengths = [len(token) for token in tokens]\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(tokens)))\n",
    "        \n",
    "        bars = ax.bar(range(len(tokens)), token_lengths, color=colors, alpha=0.7)\n",
    "        ax.set_title(f'{name} Tokenization')\n",
    "        ax.set_xlabel('Token Index')\n",
    "        ax.set_ylabel('Token Length')\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, length in zip(bars, token_lengths):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                    str(length), ha='center', va='bottom')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if len(tokenizers) < 4:\n",
    "        fig.delaxes(axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Vocabulary size comparison\n",
    "    print(\"\\nüìä Vocabulary Sizes:\")\n",
    "    print(\"=\"*30)\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        print(f\"{name}: {vocab_size:,} tokens\")\n",
    "    \n",
    "explore_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Building a Simple Text Generator with GPT-2\n",
    "\n",
    "Let's build a simple text generator using a pre-trained GPT-2 model to understand how LLMs work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "def load_gpt2_model():\n",
    "    print(\"üîÑ Loading GPT-2 model...\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    # Set pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully on {device}\")\n",
    "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "    return tokenizer, model, device\n",
    "\n",
    "tokenizer, model, device = load_gpt2_model()\n",
    "\n",
    "# Text generation function\n",
    "def generate_text(prompt, max_length=100, temperature=1.0, num_return_sequences=1):\n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Test text generation\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where robots and humans coexist\",\n",
    "    \"Machine learning algorithms are\",\n",
    "    \"The key to successful data science is\"\n",
    "]\n",
    "\n",
    "print(\"üé® Text Generation Examples\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nüìù Prompt: '{prompt}'\")\n",
    "    generated = generate_text(prompt, max_length=80, temperature=0.8)\n",
    "    print(f\"ü§ñ Generated: {generated[0]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive text generation with different parameters\n",
    "def interactive_text_generation():\n",
    "    # Create widgets\n",
    "    prompt_text = widgets.Textarea(\n",
    "        value='The future of technology is',\n",
    "        placeholder='Enter your prompt here...',\n",
    "        description='Prompt:',\n",
    "        layout=widgets.Layout(width='80%', height='80px')\n",
    "    )\n",
    "    \n",
    "    max_length_slider = widgets.IntSlider(\n",
    "        value=50, min=20, max=200, step=10,\n",
    "        description='Max Length:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    temperature_slider = widgets.FloatSlider(\n",
    "        value=0.8, min=0.1, max=2.0, step=0.1,\n",
    "        description='Temperature:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    generate_button = widgets.Button(\n",
    "        description='Generate Text',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_generate_clicked(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            \n",
    "            prompt = prompt_text.value\n",
    "            max_len = max_length_slider.value\n",
    "            temp = temperature_slider.value\n",
    "            \n",
    "            if not prompt.strip():\n",
    "                print(\"‚ùå Please enter a prompt!\")\n",
    "                return\n",
    "            \n",
    "            print(f\"üîÑ Generating text with parameters:\")\n",
    "            print(f\"   Prompt: '{prompt}'\")\n",
    "            print(f\"   Max Length: {max_len}\")\n",
    "            print(f\"   Temperature: {temp}\")\n",
    "            print(\"\\nü§ñ Generated Text:\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            try:\n",
    "                generated = generate_text(prompt, max_length=max_len, temperature=temp)\n",
    "                print(generated[0])\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                # Show generation statistics\n",
    "                tokens = tokenizer.encode(generated[0])\n",
    "                print(f\"\\nüìä Generation Stats:\")\n",
    "                print(f\"   Total tokens: {len(tokens)}\")\n",
    "                print(f\"   Prompt tokens: {len(tokenizer.encode(prompt))}\")\n",
    "                print(f\"   Generated tokens: {len(tokens) - len(tokenizer.encode(prompt))}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during generation: {e}\")\n",
    "    \n",
    "    generate_button.on_click(on_generate_clicked)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        prompt_text,\n",
    "        widgets.HBox([max_length_slider, temperature_slider]),\n",
    "        generate_button,\n",
    "        output_area\n",
    "    ]))\n",
    "    \n",
    "    # Show temperature explanation\n",
    "    print(\"\\nüå°Ô∏è Temperature Controls:\")\n",
    "    print(\"‚Ä¢ Low temperature (0.1-0.5): More focused, conservative text\")\n",
    "    print(\"‚Ä¢ Medium temperature (0.5-1.0): Balanced creativity\")\n",
    "    print(\"‚Ä¢ High temperature (1.0-2.0): More diverse, creative text\")\n",
    "\n",
    "interactive_text_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Text Classification with BERT\n",
    "\n",
    "Now let's use BERT for text classification to understand how LLMs can be used for specific NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model for text classification\n",
    "def load_bert_classifier():\n",
    "    print(\"üîÑ Loading BERT classifier...\")\n",
    "    \n",
    "    # Load tokenizer and model for sequence classification\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'textattack/bert-base-uncased-imdb',  # Pre-trained on IMDB sentiment analysis\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"‚úÖ BERT classifier loaded on {device}\")\n",
    "    return tokenizer, model, device\n",
    "\n",
    "bert_tokenizer, bert_model, bert_device = load_bert_classifier()\n",
    "\n",
    "# Function to classify text\n",
    "def classify_text(text, tokenizer, model, device):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    return predicted_class, confidence, predictions[0].cpu().numpy()\n",
    "\n",
    "# Test sentiment classification\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the plot was engaging.\",\n",
    "    \"I was really disappointed with this film. The story was predictable and the characters were boring.\",\n",
    "    \"The movie had some good moments but overall it was just average.\",\n",
    "    \"One of the worst movies I've ever seen. Complete waste of time and money.\",\n",
    "    \"A masterpiece of cinema! Every scene was beautifully crafted and the performances were outstanding.\",\n",
    "    \"The special effects were good, but the script needed a lot of work.\",\n",
    "]\n",
    "\n",
    "print(\"üé¨ Sentiment Analysis with BERT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sentiment_labels = ['Negative', 'Positive']\n",
    "results = []\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    predicted_class, confidence, probs = classify_text(text, bert_tokenizer, bert_model, bert_device)\n",
    "    \n",
    "    sentiment = sentiment_labels[predicted_class]\n",
    "    \n",
    "    print(f\"\\nüìù Text {i}: '{text[:60]}...'\")\n",
    "    print(f\"üé≠ Sentiment: {sentiment}\")\n",
    "    print(f\"üìä Confidence: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "    print(f\"üìà Probabilities: Negative={probs[0]:.3f}, Positive={probs[1]:.3f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence,\n",
    "        'negative_prob': probs[0],\n",
    "        'positive_prob': probs[1]\n",
    "    })\n",
    "\n",
    "# Visualize results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = df_results['sentiment'].value_counts()\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "ax1.set_title('Sentiment Distribution')\n",
    "\n",
    "# Confidence scores\n",
    "ax2.bar(range(len(df_results)), df_results['confidence'], color='lightblue', alpha=0.7)\n",
    "ax2.set_xlabel('Text Index')\n",
    "ax2.set_ylabel('Confidence Score')\n",
    "ax2.set_title('Classification Confidence')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add sentiment labels on bars\n",
    "for i, (_, row) in enumerate(df_results.iterrows()):\n",
    "    ax2.text(i, row['confidence'] + 0.02, row['sentiment'][:4], \n",
    "            ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding Model Embeddings\n",
    "\n",
    "Let's explore how LLMs represent text as embeddings and visualize the semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize embeddings\n",
    "def explore_embeddings():\n",
    "    print(\"üîç Exploring Text Embeddings\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Load BERT model for embeddings\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Sample texts with semantic relationships\n",
    "    texts = [\n",
    "        'cat', 'dog', 'animal', 'pet',\n",
    "        'car', 'truck', 'vehicle', 'transportation',\n",
    "        'happy', 'sad', 'emotion', 'feeling',\n",
    "        'computer', 'laptop', 'technology', 'device'\n",
    "    ]\n",
    "    \n",
    "    # Get embeddings for each text\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize and get embeddings\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use CLS token embedding as sentence representation\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(embedding.flatten())\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    print(f\"‚úÖ Extracted embeddings for {len(texts)} words\")\n",
    "    print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Reduce dimensionality for visualization\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d_pca = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d_tsne = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # PCA visualization\n",
    "    categories = ['Animals', 'Vehicles', 'Emotions', 'Technology']\n",
    "    category_colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for i, (category, color) in enumerate(zip(categories, category_colors)):\n",
    "        start_idx = i * 4\n",
    "        end_idx = start_idx + 4\n",
    "        \n",
    "        ax1.scatter(embeddings_2d_pca[start_idx:end_idx, 0], \n",
    "                   embeddings_2d_pca[start_idx:end_idx, 1], \n",
    "                   c=color, label=category, s=100, alpha=0.7)\n",
    "        \n",
    "        # Add text labels\n",
    "        for j in range(start_idx, end_idx):\n",
    "            ax1.annotate(texts[j], (embeddings_2d_pca[j, 0], embeddings_2d_pca[j, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax1.set_title('Word Embeddings (PCA)')\n",
    "    ax1.set_xlabel('PCA Component 1')\n",
    "    ax1.set_ylabel('PCA Component 2')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # t-SNE visualization\n",
    "    for i, (category, color) in enumerate(zip(categories, category_colors)):\n",
    "        start_idx = i * 4\n",
    "        end_idx = start_idx + 4\n",
    "        \n",
    "        ax2.scatter(embeddings_2d_tsne[start_idx:end_idx, 0], \n",
    "                   embeddings_2d_tsne[start_idx:end_idx, 1], \n",
    "                   c=color, label=category, s=100, alpha=0.7)\n",
    "        \n",
    "        # Add text labels\n",
    "        for j in range(start_idx, end_idx):\n",
    "            ax2.annotate(texts[j], (embeddings_2d_tsne[j, 0], embeddings_2d_tsne[j, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax2.set_title('Word Embeddings (t-SNE)')\n",
    "    ax2.set_xlabel('t-SNE Component 1')\n",
    "    ax2.set_ylabel('t-SNE Component 2')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find similar words\n",
    "    print(\"\\nüîó Finding Similar Words:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    for i, word in enumerate(texts):\n",
    "        # Get similarity scores for this word\n",
    "        similarities = similarity_matrix[i]\n",
    "        \n",
    "        # Get top 3 most similar words (excluding itself)\n",
    "        top_indices = np.argsort(similarities)[::-1][1:4]\n",
    "        \n",
    "        print(f\"\\nWords similar to '{word}':\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"  ‚Ä¢ {texts[idx]} (similarity: {similarities[idx]:.3f})\")\n",
    "    \n",
    "    return embeddings, embeddings_2d_pca, embeddings_2d_tsne\n",
    "\n",
    "embeddings, pca_embeddings, tsne_embeddings = explore_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Interactive Exercise: Prompt Engineering\n",
    "\n",
    "Let's experiment with different prompting techniques to see how they affect model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt engineering\n",
    "def interactive_prompt_engineering():\n",
    "    print(\"üîß Interactive Prompt Engineering\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Different prompt techniques\n",
    "    prompt_techniques = {\n",
    "        'Basic': \"{topic}\",\n",
    "        'Instruction': \"Explain {topic} in simple terms.\",\n",
    "        'Role-playing': \"You are an expert educator. Explain {topic} to a beginner.\",\n",
    "        'Step-by-step': \"Explain {topic} step by step. Use clear examples.\",\n",
    "        'Analogy': \"Explain {topic} using a simple analogy.\",\n",
    "        'Q&A': \"What is {topic}? Why is it important? How does it work?\"\n",
    "    }\n",
    "    \n",
    "    topics = [\n",
    "        'machine learning',\n",
    "        'blockchain technology',\n",
    "        'climate change',\n",
    "        'quantum computing',\n",
    "        'artificial intelligence ethics'\n",
    "    ]\n",
    "    \n",
    "    # Create widgets\n",
    "    topic_dropdown = widgets.Dropdown(\n",
    "        options=topics,\n",
    "        value=topics[0],\n",
    "        description='Topic:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    technique_dropdown = widgets.Dropdown(\n",
    "        options=list(prompt_techniques.keys()),\n",
    "        value='Basic',\n",
    "        description='Technique:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    generate_button = widgets.Button(\n",
    "        description='Generate Prompt',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_generate_clicked(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            \n",
    "            topic = topic_dropdown.value\n",
    "            technique = technique_dropdown.value\n",
    "            \n",
    "            # Generate prompt\n",
    "            prompt_template = prompt_techniques[technique]\n",
    "            prompt = prompt_template.format(topic=topic)\n",
    "            \n",
    "            print(f\"üéØ {technique} Prompt:\")\n",
    "            print(f\"\\\"{prompt}\\\"\")\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            \n",
    "            # Show technique explanation\n",
    "            explanations = {\n",
    "                'Basic': 'Simple and direct, good for straightforward queries',\n",
    "                'Instruction': 'Clear guidance on what the model should do',\n",
    "                'Role-playing': 'Helps the model adopt a specific perspective',\n",
    "                'Step-by-step': 'Encourages structured, detailed responses',\n",
    "                'Analogy': 'Makes complex topics more relatable',\n",
    "                'Q&A': 'Prompts comprehensive coverage of multiple aspects'\n",
    "            }\n",
    "            \n",
    "            print(f\"üí° Why this works: {explanations[technique]}\")\n",
    "            print(\"\\nüìù Expected Response Characteristics:\")\n",
    "            \n",
    "            if technique == 'Basic':\n",
    "                print(\"‚Ä¢ Concise definition\")\n",
    "                print(\"‚Ä¢ May vary in detail level\")\n",
    "            elif technique == 'Instruction':\n",
    "                print(\"‚Ä¢ Clear, structured explanation\")\n",
    "                print(\"‚Ä¢ Appropriate complexity level\")\n",
    "            elif technique == 'Role-playing':\n",
    "                print(\"‚Ä¢ Educational tone\")\n",
    "                print(\"‚Ä¢ Beginner-friendly language\")\n",
    "            elif technique == 'Step-by-step':\n",
    "                print(\"‚Ä¢ Sequential explanation\")\n",
    "                print(\"‚Ä¢ Clear examples\")\n",
    "            elif technique == 'Analogy':\n",
    "                print(\"‚Ä¢ Relatable comparison\")\n",
    "                print(\"‚Ä¢ Easy to understand\")\n",
    "            elif technique == 'Q&A':\n",
    "                print(\"‚Ä¢ Comprehensive coverage\")\n",
    "                print(\"‚Ä¢ Multiple perspectives\")\n",
    "            \n",
    "            # Show example response structure\n",
    "            print(\"\\nüìã Example Response Structure:\")\n",
    "            if technique == 'Step-by-step':\n",
    "                print(\"1. Definition of key terms\")\n",
    "                print(\"2. Core concepts and principles\")\n",
    "                print(\"3. Step-by-step explanation\")\n",
    "                print(\"4. Practical examples\")\n",
    "                print(\"5. Summary and applications\")\n",
    "            elif technique == 'Q&A':\n",
    "                print(\"‚Ä¢ Direct answer to 'What is...'?\")\n",
    "                print(\"‚Ä¢ Explanation of importance\")\n",
    "                print(\"‚Ä¢ Breakdown of how it works\")\n",
    "                print(\"‚Ä¢ Real-world applications\")\n",
    "    \n",
    "    generate_button.on_click(on_generate_clicked)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([topic_dropdown, technique_dropdown]),\n",
    "        generate_button,\n",
    "        output_area\n",
    "    ]))\n",
    "    \n",
    "    # Show prompt engineering tips\n",
    "    print(\"\\nüí° Prompt Engineering Tips:\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"1. Be specific and clear about what you want\")\n",
    "    print(\"2. Provide context and background information\")\n",
    "    print(\"3. Specify the desired format and length\")\n",
    "    print(\"4. Use examples to demonstrate expected output\")\n",
    "    print(\"5. Break complex tasks into smaller steps\")\n",
    "    print(\"6. Use constraints to guide the model\")\n",
    "    print(\"7. Iterate and refine based on results\")\n",
    "\n",
    "interactive_prompt_engineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Hands-on Challenge\n",
    "\n",
    "Now it's your turn to apply what you've learned! Complete the following challenges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Create a text summarization system\n",
    "def challenge_1():\n",
    "    print(\"üìù Challenge 1: Text Summarization System\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Sample long texts for summarization\n",
    "    long_texts = [\n",
    "        \"\"\"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\"\"\",\n",
    "        \n",
    "        \"\"\"Climate change refers to long-term shifts in temperatures and weather patterns. These shifts may be natural, such as through variations in the solar cycle. But since the 1800s, human activities have been the main driver of climate change, primarily due to burning fossil fuels (like coal, oil, and gas), which produces heat-trapping gases. Climate change is already impacting every region on Earth. The changes we're experiencing include more frequent and intense extreme weather events like heatwaves, storms, and droughts; rising sea levels; melting ice sheets; and warming oceans. These changes affect not only the environment but also human societies, disrupting economies, affecting food security, and threatening human health and safety. Addressing climate change requires global cooperation and significant changes in how we produce and consume energy.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # TODO: Complete the summarization task\n",
    "    \n",
    "    # Task 1: Basic extractive summarization\n",
    "    def extractive_summary(text, num_sentences=3):\n",
    "        \"\"\"Simple extractive summarization using sentence importance\"\"\"\n",
    "        import re\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return text\n",
    "        \n",
    "        # Calculate TF-IDF scores\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        # Get sentence scores\n",
    "        sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "        \n",
    "        # Get top sentences\n",
    "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "        top_indices = sorted(top_indices)  # Maintain original order\n",
    "        \n",
    "        summary = ' '.join([sentences[i] for i in top_indices])\n",
    "        return summary\n",
    "    \n",
    "    print(\"\\n‚úÖ Task 1: Extractive summarization function created!\")\n",
    "    \n",
    "    # Test the summarization\n",
    "    for i, text in enumerate(long_texts, 1):\n",
    "        print(f\"\\nüìÑ Text {i} (first 100 chars): {text[:100]}...\")\n",
    "        print(f\"Original length: {len(text)} characters\")\n",
    "        \n",
    "        summary = extractive_summary(text, num_sentences=2)\n",
    "        print(f\"\\nüìã Summary ({len(summary)} characters):\")\n",
    "        print(summary)\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Task 2: Create an abstractive summarization attempt with GPT-2\n",
    "    print(\"\\n‚úÖ Task 2: Abstractive summarization with GPT-2\")\n",
    "    \n",
    "    summarization_prompt = \"Summarize the following text in 2-3 sentences:\\n\\n{text}\\n\\nSummary:\"\n",
    "    \n",
    "    for i, text in enumerate(long_texts, 1):\n",
    "        print(f\"\\nüìÑ Text {i}:\")\n",
    "        \n",
    "        # Create prompt for summarization\n",
    "        prompt = summarization_prompt.format(text=text)\n",
    "        \n",
    "        try:\n",
    "            # Generate summary\n",
    "            generated = generate_text(prompt, max_length=150, temperature=0.7)\n",
    "            full_text = generated[0]\n",
    "            \n",
    "            # Extract summary part (after \"Summary:\")\n",
    "            if \"Summary:\" in full_text:\n",
    "                summary = full_text.split(\"Summary:\")[1].strip()\n",
    "                print(f\"ü§ñ GPT-2 Summary: {summary}\")\n",
    "            else:\n",
    "                print(f\"ü§ñ GPT-2 Generation: {full_text}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return extractive_summary\n",
    "\n",
    "challenge_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Build a simple question-answering system\n",
    "def challenge_2():\n",
    "    print(\"‚ùì Challenge 2: Question-Answering System\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Sample context passages\n",
    "    contexts = [\n",
    "        \"\"\"The Python programming language was created by Guido van Rossum and first released in 1991. Python emphasizes code readability and simplicity, making it an excellent choice for beginners. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python has a comprehensive standard library and a vast ecosystem of third-party packages. Popular frameworks like Django and Flask make web development easy, while libraries like NumPy, Pandas, and TensorFlow have made Python the dominant language in data science and machine learning.\"\"\",\n",
    "        \n",
    "        \"\"\"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) that process information through their connectionist structure. Each connection between neurons has a weight that adjusts as learning proceeds. Neural networks can learn to perform tasks by considering examples, generally without being programmed with task-specific rules. Deep learning uses neural networks with multiple layers between the input and output layers, enabling the learning of complex patterns in large datasets. Applications include image recognition, natural language processing, and autonomous vehicles.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Questions about each context\n",
    "    questions = [\n",
    "        [\n",
    "            \"Who created Python?\",\n",
    "            \"When was Python first released?\",\n",
    "            \"What programming paradigms does Python support?\",\n",
    "            \"Why is Python popular in data science?\",\n",
    "            \"Name two Python web frameworks.\"\n",
    "        ],\n",
    "        [\n",
    "            \"What are neural networks inspired by?\",\n",
    "            \"What adjusts during neural network learning?\",\n",
    "            \"What is deep learning?\",\n",
    "            \"What are some applications of neural networks?\",\n",
    "            \"How do neural networks learn tasks?\"\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # TODO: Complete the QA system\n",
    "    \n",
    "    # Task 1: Simple keyword-based QA\n",
    "    def simple_keyword_qa(question, context):\n",
    "        \"\"\"Simple keyword-based question answering\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Split context into sentences\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        # Extract keywords from question\n",
    "        question_words = re.findall(r'\\b\\w+\\b', question.lower())\n",
    "        stop_words = {'what', 'who', 'when', 'where', 'why', 'how', 'is', 'are', 'was', 'were', 'the', 'a', 'an', 'and', 'or', 'but'}\n",
    "        keywords = [w for w in question_words if w not in stop_words and len(w) > 2]\n",
    "        \n",
    "        # Score sentences based on keyword matches\n",
    "        sentence_scores = []\n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            score = sum(1 for keyword in keywords if keyword in sentence_lower)\n",
    "            sentence_scores.append(score)\n",
    "        \n",
    "        # Return sentence with highest score\n",
    "        if max(sentence_scores) > 0:\n",
    "            best_sentence = sentences[sentence_scores.index(max(sentence_scores))]\n",
    "            return best_sentence\n",
    "        else:\n",
    "            return \"I don't have enough information to answer that question.\"\n",
    "    \n",
    "    print(\"\\n‚úÖ Task 1: Keyword-based QA system created!\")\n",
    "    \n",
    "    # Test the QA system\n",
    "    for i, (context, context_questions) in enumerate(zip(contexts, questions), 1):\n",
    "        print(f\"\\nüìÑ Context {i}:\")\n",
    "        print(context[:200] + \"...\" if len(context) > 200 else context)\n",
    "        print(\"\\n‚ùì Questions and Answers:\")\n",
    "        \n",
    "        for question in context_questions:\n",
    "            answer = simple_keyword_qa(question, context)\n",
    "            print(f\"Q: {question}\")\n",
    "            print(f\"A: {answer}\")\n",
    "            print()\n",
    "    \n",
    "    # Task 2: Evaluate QA system performance\n",
    "    print(\"\\n‚úÖ Task 2: QA System Evaluation\")\n",
    "    \n",
    "    # Simple evaluation using keyword overlap\n",
    "    def evaluate_qa_system(contexts, questions_list, qa_function):\n",
    "        results = []\n",
    "        \n",
    "        for context, questions in zip(contexts, questions_list):\n",
    "            context_results = []\n",
    "            for question in questions:\n",
    "                answer = qa_function(question, context)\n",
    "                \n",
    "                # Simple relevance check (non-relevant answers are usually the default message)\n",
    "                is_relevant = \"I don't have enough information\" not in answer\n",
    "                \n",
    "                context_results.append({\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'relevant': is_relevant\n",
    "                })\n",
    "            \n",
    "            results.append(context_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    evaluation_results = evaluate_qa_system(contexts, questions, simple_keyword_qa)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_questions = sum(len(context_results) for context_results in evaluation_results)\n",
    "    relevant_answers = sum(\n",
    "        sum(1 for result in context_results if result['relevant'])\n",
    "        for context_results in evaluation_results\n",
    "    )\n",
    "    \n",
    "    relevance_rate = relevant_answers / total_questions if total_questions > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä QA System Performance:\")\n",
    "    print(f\"Total questions: {total_questions}\")\n",
    "    print(f\"Relevant answers: {relevant_answers}\")\n",
    "    print(f\"Relevance rate: {relevance_rate:.2%}\")\n",
    "    \n",
    "    return simple_keyword_qa\n",
    "\n",
    "challenge_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Build a text classification pipeline\n",
    "def challenge_3():\n",
    "    print(\"üè∑Ô∏è Challenge 3: Text Classification Pipeline\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Sample texts for classification\n",
    "    sample_texts = [\n",
    "        \"I absolutely love this new smartphone! The camera quality is amazing and the battery life is incredible.\",\n",
    "        \"The movie was disappointing. The plot was predictable and the acting was mediocre at best.\",\n",
    "        \"The weather today is perfect for a walk in the park. Sunny and warm with a gentle breeze.\",\n",
    "        \"I'm not sure about this restaurant. The food was okay but the service was slow.\",\n",
    "        \"This new software update has completely ruined my user experience. Everything is slower now.\",\n",
    "        \"The concert last night was fantastic! The band played all my favorite songs.\",\n",
    "        \"I can't decide whether I like this book or not. The story is interesting but the writing style is odd.\",\n",
    "        \"Excellent customer service! They resolved my issue quickly and were very polite.\"\n",
    "    ]\n",
    "    \n",
    "    # True labels (for evaluation)\n",
    "    true_labels = ['positive', 'negative', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'positive']\n",
    "    \n",
    "    # TODO: Complete the classification pipeline\n",
    "    \n",
    "    # Task 1: Create a simple rule-based classifier\n",
    "    def rule_based_classifier(text):\n",
    "        \"\"\"Simple rule-based sentiment classifier\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        positive_words = ['love', 'amazing', 'incredible', 'fantastic', 'excellent', 'perfect', 'great', 'awesome']\n",
    "        negative_words = ['hate', 'terrible', 'awful', 'disappointing', 'predictable', 'mediocre', 'ruined', 'slow']\n",
    "        \n",
    "        positive_score = sum(1 for word in positive_words if word in text_lower)\n",
    "        negative_score = sum(1 for word in negative_words if word in text_lower)\n",
    "        \n",
    "        if positive_score > negative_score:\n",
    "            return 'positive'\n",
    "        elif negative_score > positive_score:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    print(\"\\n‚úÖ Task 1: Rule-based classifier created!\")\n",
    "    \n",
    "    # Test the classifier\n",
    "    predictions = []\n",
    "    \n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        prediction = rule_based_classifier(text)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        print(f\"\\nüìù Text {i}: {text}\")\n",
    "        print(f\"üé≠ Predicted sentiment: {prediction}\")\n",
    "        print(f\"‚úÖ True sentiment: {true_labels[i-1]}\")\n",
    "        print(f\"{'‚úì Correct!' if prediction == true_labels[i-1] else '‚úó Incorrect'}\")\n",
    "    \n",
    "    # Task 2: Evaluate classifier performance\n",
    "    print(\"\\n‚úÖ Task 2: Classifier Evaluation\")\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"\\nüìä Classifier Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=['positive', 'negative', 'neutral'])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['positive', 'negative', 'neutral'],\n",
    "                yticklabels=['positive', 'negative', 'neutral'])\n",
    "    plt.title('Confusion Matrix - Rule-based Classifier')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Task 3: Compare with BERT classifier\n",
    "    print(\"\\n‚úÖ Task 3: Comparison with BERT Classifier\")\n",
    "    \n",
    "    bert_predictions = []\n",
    "    bert_confidences = []\n",
    "    \n",
    "    # Map BERT output (binary) to our three labels\n",
    "    for text in sample_texts:\n",
    "        try:\n",
    "            predicted_class, confidence, probs = classify_text(text, bert_tokenizer, bert_model, bert_device)\n",
    "            \n",
    "            # Convert binary sentiment to three classes based on confidence\n",
    "            if predicted_class == 1 and confidence > 0.8:  # Positive and confident\n",
    "                bert_pred = 'positive'\n",
    "            elif predicted_class == 0 and confidence > 0.8:  # Negative and confident\n",
    "                bert_pred = 'negative'\n",
    "            else:  # Low confidence or middle ground\n",
    "                bert_pred = 'neutral'\n",
    "            \n",
    "            bert_predictions.append(bert_pred)\n",
    "            bert_confidences.append(confidence)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying text: {e}\")\n",
    "            bert_predictions.append('neutral')\n",
    "            bert_confidences.append(0.0)\n",
    "    \n",
    "    bert_accuracy = accuracy_score(true_labels, bert_predictions)\n",
    "    print(f\"\\nüìä BERT Classifier Performance:\")\n",
    "    print(f\"Accuracy: {bert_accuracy:.2%}\")\n",
    "    \n",
    "    # Compare both classifiers\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Text': [text[:50] + '...' for text in sample_texts],\n",
    "        'True': true_labels,\n",
    "        'Rule_Based': predictions,\n",
    "        'BERT': bert_predictions,\n",
    "        'BERT_Confidence': bert_confidences\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüìã Classifier Comparison:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Final comparison\n",
    "    print(f\"\\nüèÜ Final Results:\")\n",
    "    print(f\"Rule-based accuracy: {accuracy:.2%}\")\n",
    "    print(f\"BERT accuracy: {bert_accuracy:.2%}\")\n",
    "    \n",
    "    if bert_accuracy > accuracy:\n",
    "        print(f\"‚úÖ BERT performs better by {(bert_accuracy - accuracy):.2%}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Rule-based performs better by {(accuracy - bert_accuracy):.2%}\")\n",
    "    \n",
    "    return rule_based_classifier\n",
    "\n",
    "challenge_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **LLM Fundamentals**:\n",
    "   - Transformer architecture and attention mechanisms\n",
    "   - Evolution of large language models\n",
    "   - Tokenization and embeddings\n",
    "\n",
    "2. **Text Generation**:\n",
    "   - Using GPT-2 for creative text generation\n",
    "   - Understanding temperature and sampling parameters\n",
    "   - Interactive text generation with user controls\n",
    "\n",
    "3. **Text Classification**:\n",
    "   - Sentiment analysis with BERT\n",
    "   - Understanding model confidence\n",
    "   - Interpreting classification results\n",
    "\n",
    "4. **Embeddings and Semantic Understanding**:\n",
    "   - Extracting text embeddings from LLMs\n",
    "   - Visualizing semantic relationships\n",
    "   - Finding similar words using cosine similarity\n",
    "\n",
    "5. **Practical Applications**:\n",
    "   - Text summarization techniques\n",
    "   - Question-answering systems\n",
    "   - Custom text classification pipelines\n",
    "\n",
    "6. **Prompt Engineering**:\n",
    "   - Different prompting strategies\n",
    "   - Crafting effective prompts\n",
    "   - Understanding model behavior with different inputs\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- **Continue to next notebook**: \"Advanced LLM Techniques\" for fine-tuning and advanced applications\n",
    "- **Explore API-based LLMs**: OpenAI GPT, Claude, Gemini API integration\n",
    "- **Learn about fine-tuning**: Adapting pre-trained models for specific tasks\n",
    "- **Study RAG (Retrieval-Augmented Generation)**: Combining LLMs with external knowledge\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- [Hugging Face Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [Attention Is All You Need (Original Transformer Paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "\n",
    "**üéâ Congratulations! You've completed your first Large Language Models notebook!**\n",
    "You now have a solid foundation in understanding and working with state-of-the-art language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}