{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "**Interactive Notebook** - Section 1: Foundational Machine Learning\n",
    "\n",
    "Welcome to your first step into the exciting world of Machine Learning! This notebook will guide you through the fundamental concepts with hands-on exercises and interactive visualizations.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand what Machine Learning is and its key concepts\n",
    "- ‚úÖ Differentiate between supervised, unsupervised, and reinforcement learning\n",
    "- ‚úÖ Learn about common ML algorithms and their applications\n",
    "- ‚úÖ Implement your first ML models using scikit-learn\n",
    "- ‚úÖ Evaluate model performance using appropriate metrics\n",
    "- ‚úÖ Practice with real datasets and hands-on exercises\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Basic Python knowledge (variables, functions, loops)\n",
    "- High school level mathematics (algebra, basic statistics)\n",
    "- Curiosity and willingness to experiment!\n",
    "\n",
    "**Estimated Time**: 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Installation\n",
    "\n",
    "Let's set up our environment with the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn ipywidgets\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î What is Machine Learning?\n",
    "\n",
    "Machine Learning is a subset of Artificial Intelligence that enables computers to learn and improve from experience without being explicitly programmed.\n",
    "\n",
    "### Traditional Programming vs Machine Learning\n",
    "\n",
    "**Traditional Programming:**\n",
    "- You write explicit rules and logic\n",
    "- Computer follows your instructions exactly\n",
    "- Limited to what you can anticipate\n",
    "\n",
    "**Machine Learning:**\n",
    "- You provide data and examples\n",
    "- Computer learns patterns and rules automatically\n",
    "- Can handle new, unseen situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive comparison: Traditional vs Machine Learning\n",
    "comparison_data = {\n",
    "    'Aspect': ['Rules Creation', 'Data Requirements', 'Flexibility', 'Learning Process', 'Scalability'],\n",
    "    'Traditional Programming': ['Manual programming', 'Limited', 'Fixed logic', 'No learning', 'Limited'],\n",
    "    'Machine Learning': ['Automatic learning', 'Large datasets', 'Adaptive', 'Continuous improvement', 'Highly scalable']\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "display(HTML(\"<h3>Traditional Programming vs Machine Learning</h3>\"))\n",
    "display(df_comparison.style.background_gradient(cmap='RdYlGn', subset=['Traditional Programming', 'Machine Learning']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Types of Machine Learning\n",
    "\n",
    "Machine Learning can be broadly categorized into three main types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualization for ML types\n",
    "ml_types = {\n",
    "    'Supervised Learning': {\n",
    "        'description': 'Learn from labeled data with known outputs',\n",
    "        'examples': ['Email spam detection', 'House price prediction', 'Image classification'],\n",
    "        'algorithms': ['Linear Regression', 'Decision Trees', 'Neural Networks', 'SVM']\n",
    "    },\n",
    "    'Unsupervised Learning': {\n",
    "        'description': 'Find patterns in unlabeled data',\n",
    "        'examples': ['Customer segmentation', 'Anomaly detection', 'Topic modeling'],\n",
    "        'algorithms': ['K-Means', 'Hierarchical Clustering', 'PCA', 'DBSCAN']\n",
    "    },\n",
    "    'Reinforcement Learning': {\n",
    "        'description': 'Learn through trial and error with rewards',\n",
    "        'examples': ['Game playing AI', 'Robotics', 'Resource management'],\n",
    "        'algorithms': ['Q-Learning', 'Policy Gradients', 'Actor-Critic']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create dropdown to explore ML types\n",
    "ml_type_dropdown = widgets.Dropdown(\n",
    "    options=list(ml_types.keys()),\n",
    "    value='Supervised Learning',\n",
    "    description='ML Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def show_ml_type_info(ml_type):\n",
    "    info = ml_types[ml_type]\n",
    "    display(HTML(f\"\"\"\n",
    "    <h3>{ml_type}</h3>\n",
    "    <p><strong>Description:</strong> {info['description']}</p>\n",
    "    <p><strong>Examples:</strong></p>\n",
    "    <ul>\n",
    "        {''.join([f'<li>{example}</li>' for example in info['examples']])}\n",
    "    </ul>\n",
    "    <p><strong>Key Algorithms:</strong></p>\n",
    "    <ul>\n",
    "        {''.join([f'<li>{algo}</li>' for algo in info['algorithms']])}\n",
    "    </ul>\n",
    "    \"\"\"))\n",
    "\n",
    "widgets.interactive(show_ml_type_info, ml_type=ml_type_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Supervised Learning: Your First Model\n",
    "\n",
    "Let's build our first supervised learning model! We'll use the famous Iris dataset to classify flower species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Create DataFrame for easier exploration\n",
    "df_iris = pd.DataFrame(X, columns=feature_names)\n",
    "df_iris['species'] = [target_names[i] for i in y]\n",
    "\n",
    "print(\"üå∫ Iris Dataset Overview\")\n",
    "print(f\"Number of samples: {len(df_iris)}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Number of classes: {len(target_names)}\")\n",
    "print(f\"\\nClasses: {target_names}\")\n",
    "print(f\"\\nFeatures: {feature_names}\")\n",
    "\n",
    "# Display first few samples\n",
    "display(HTML(\"<h4>Sample Data:</h4>\"))\n",
    "display(df_iris.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Iris Dataset Visualization', fontsize=16)\n",
    "\n",
    "# Pairwise scatter plots\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < 4:\n",
    "        feature = feature_names[i]\n",
    "        for species in target_names:\n",
    "            species_data = df_iris[df_iris['species'] == species]\n",
    "            ax.scatter(species_data.index, species_data[feature], label=species, alpha=0.7)\n",
    "        ax.set_ylabel(feature)\n",
    "        ax.set_xlabel('Sample Index')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_iris.iloc[:, :-1].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Data Preparation\n",
    "\n",
    "Before training our model, we need to prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"üìä Data Preparation Complete\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Show class distribution\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "print(\"\\nüìä Class Distribution:\")\n",
    "print(\"Training set:\")\n",
    "for i, count in enumerate(train_dist):\n",
    "    print(f\"  {target_names[i]}: {count} samples\")\n",
    "print(\"\\nTesting set:\")\n",
    "for i, count in enumerate(test_dist):\n",
    "    print(f\"  {target_names[i]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Model Training: Multiple Algorithms\n",
    "\n",
    "Let's train several different models and compare their performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize different models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=3)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"üéØ Training and Evaluating Models\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  Correct predictions: {np.sum(y_pred == y_test)}/{len(y_test)}\")\n",
    "    print()\n",
    "\n",
    "# Display results summary\n",
    "results_df = pd.DataFrame({name: {'accuracy': results[name]['accuracy']} for name in models.keys()}).T\n",
    "results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"üèÜ Model Performance Ranking\")\n",
    "display(results_df.style.background_gradient(cmap='RdYlGn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Model Evaluation\n",
    "\n",
    "Let's examine our best performing model in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = trained_models[best_model_name]\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nüìä Detailed Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéÆ Interactive Model Prediction\n",
    "\n",
    "Let's create an interactive widget to predict flower species based on user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive prediction widget\n",
    "sl_sepal_length = widgets.FloatSlider(\n",
    "    value=5.8, min=4.0, max=8.0, step=0.1, description='Sepal Length (cm):'\n",
    ")\n",
    "sl_sepal_width = widgets.FloatSlider(\n",
    "    value=3.0, min=2.0, max=4.5, step=0.1, description='Sepal Width (cm):'\n",
    ")\n",
    "sl_petal_length = widgets.FloatSlider(\n",
    "    value=3.8, min=1.0, max=7.0, step=0.1, description='Petal Length (cm):'\n",
    ")\n",
    "sl_petal_width = widgets.FloatSlider(\n",
    "    value=1.2, min=0.1, max=2.5, step=0.1, description='Petal Width (cm):'\n",
    ")\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=list(models.keys()),\n",
    "    value=best_model_name,\n",
    "    description='Model:'\n",
    ")\n",
    "\n",
    "predict_button = widgets.Button(description='Predict Species', button_style='success')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def make_prediction(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        # Get input values\n",
    "        features = np.array([[\n",
    "            sl_sepal_length.value,\n",
    "            sl_sepal_width.value,\n",
    "            sl_petal_length.value,\n",
    "            sl_petal_width.value\n",
    "        ]])\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        # Get model\n",
    "        model = trained_models[model_dropdown.value]\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features_scaled)[0]\n",
    "        prediction_proba = model.predict_proba(features_scaled)[0]\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üå∫ Prediction Results\")\n",
    "        print(f\"\"=\"*40)\n",
    "        print(f\"Predicted Species: {target_names[piction]}\")\n",
    "        print(f\"\\nPrediction Probabilities:\")\n",
    "        for i, (species, prob) in enumerate(zip(target_names, prediction_proba)):\n",
    "            print(f\"  {species}: {prob:.2%}\")\n",
    "        \n",
    "        # Visualize probabilities\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        bars = plt.bar(target_names, prediction_proba, color='lightblue')\n",
    "        bars[prediction].set_color('green')\n",
    "        plt.title(f'Prediction Probabilities - {model_dropdown.value}')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.show()\n",
    "\n",
    "predict_button.on_click(make_prediction)\n",
    "\n",
    "# Display widgets\n",
    "print(\"üéÆ Interactive Flower Species Predictor\")\n",
    "print(\"Adjust the flower measurements and click 'Predict Species'\")\n",
    "display(widgets.VBox([\n",
    "    sl_sepal_length, sl_sepal_width, sl_petal_length, sl_petal_width,\n",
    "    model_dropdown, predict_button, output_area\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Unsupervised Learning: Clustering\n",
    "\n",
    "Now let's explore unsupervised learning by finding natural groupings in our data without using the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Compare clustering results with actual labels\n",
    "print(\"üîç K-Means Clustering Results\")\n",
    "print(f\"Number of clusters: {len(np.unique(cluster_labels))}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot actual species\n",
    "for i, species in enumerate(target_names):\n",
    "    ax1.scatter(X_scaled[y == i, 0], X_scaled[y == i, 1], label=species, alpha=0.7)\n",
    "ax1.set_title('Actual Species')\n",
    "ax1.set_xlabel(feature_names[0] + ' (scaled)')\n",
    "ax1.set_ylabel(feature_names[1] + ' (scaled)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot clustering results\n",
    "for cluster in range(3):\n",
    "    ax2.scatter(X_scaled[cluster_labels == cluster, 0], \n",
    "                X_scaled[cluster_labels == cluster, 1], \n",
    "                label=f'Cluster {cluster}', alpha=0.7)\n",
    "ax2.set_title('K-Means Clusters')\n",
    "ax2.set_xlabel(feature_names[0] + ' (scaled)')\n",
    "ax2.set_ylabel(feature_names[1] + ' (scaled)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster composition\n",
    "print(\"\\nüìä Cluster Composition:\")\n",
    "for cluster in range(3):\n",
    "    cluster_mask = cluster_labels == cluster\n",
    "    actual_species_in_cluster = y[cluster_mask]\n",
    "    species_counts = pd.Series(actual_species_in_cluster).value_counts()\n",
    "    \n",
    "    print(f\"\\nCluster {cluster} (size: {np.sum(cluster_mask)}):\")\n",
    "    for species_idx, count in species_counts.items():\n",
    "        print(f\"  {target_names[species_idx]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Finding the Optimal Number of Clusters\n",
    "\n",
    "Let's use the Elbow Method to find the optimal number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters using Elbow Method\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertias, 'bo-', markersize=8, linewidth=2)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation for the \"elbow\"\n",
    "optimal_k = 3\n",
    "plt.annotate('Optimal k = 3', \n",
    "             xy=(optimal_k, inertias[optimal_k-1]),\n",
    "             xytext=(optimal_k+1, inertias[optimal_k-1]+50),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Elbow Method Analysis\")\n",
    "print(f\"The 'elbow' occurs at k = {optimal_k}, which matches our actual number of species!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercise: Model Comparison\n",
    "\n",
    "Now it's your turn! Use the interactive widget below to explore how different models perform with different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive model comparison\n",
    "k_slider = widgets.IntSlider(\n",
    "    value=3, min=1, max=10, step=1, description='KNN k-value:'\n",
    ")\n",
    "\n",
    "max_depth_slider = widgets.IntSlider(\n",
    "    value=3, min=1, max=10, step=1, description='Decision Tree Depth:'\n",
    ")\n",
    "\n",
    "test_size_slider = widgets.FloatSlider(\n",
    "    value=0.3, min=0.1, max=0.5, step=0.05, description='Test Size:'\n",
    ")\n",
    "\n",
    "compare_button = widgets.Button(description='Compare Models', button_style='info')\n",
    "comparison_output = widgets.Output()\n",
    "\n",
    "def compare_models_interactive(b):\n",
    "    with comparison_output:\n",
    "        comparison_output.clear_output()\n",
    "        \n",
    "        # Split data with specified test size\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size_slider.value, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Initialize models with custom parameters\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42),\n",
    "            'Decision Tree': DecisionTreeClassifier(max_depth=max_depth_slider.value, random_state=42),\n",
    "            'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=k_slider.value)\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate\n",
    "        results = {}\n",
    "        \n",
    "        print(f\"üéØ Model Comparison Results\")\n",
    "        print(f\"Test Size: {test_size_slider.value:.0%}\")\n",
    "        print(f\"KNN k-value: {k_slider.value}\")\n",
    "        print(f\"Decision Tree Max Depth: {max_depth_slider.value}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            results[name] = accuracy\n",
    "            print(f\"{name}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(results.keys(), results.values(), color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "        plt.title('Model Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, results.values()):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "compare_button.on_click(compare_models_interactive)\n",
    "\n",
    "print(\"üéÆ Interactive Model Comparison\")\n",
    "print(\"Adjust the parameters and click 'Compare Models' to see how they affect performance!\")\n",
    "display(widgets.VBox([\n",
    "    k_slider, max_depth_slider, test_size_slider, compare_button, comparison_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Challenge Exercises\n",
    "\n,
    "Test your understanding with these challenge exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Create your own classifier\n",
    "print(\"üèÜ Challenge 1: Build Your Own Classifier\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Create a simple rule-based classifier for the Iris dataset\")\n",
    "print(\"\\nRules to implement:\")\n",
    "print(\"- If petal length < 2.5 cm ‚Üí Iris-setosa\")\n",
    "print(\"- Else if petal width < 1.8 cm ‚Üí Iris-versicolor\")\n",
    "print(\"- Else ‚Üí Iris-virginica\")\n",
    "\n",
    "def simple_classifier(petal_length, petal_width):\n",
    "    \"\"\"Your rule-based classifier\"\"\"\n",
    "    if petal_length < 2.5:\n",
    "        return 0  # Iris-setosa\n",
    "    elif petal_width < 1.8:\n",
    "        return 1  # Iris-versicolor\n",
    "    else:\n",
    "        return 2  # Iris-virginica\n",
    "\n",
    "# Test the classifier\n",
    "y_pred_simple = [simple_classifier(X[i, 2], X[i, 3]) for i in range(len(X))]\n",
    "accuracy_simple = accuracy_score(y, y_pred_simple)\n",
    "\n",
    "print(f\"\\nSimple Classifier Accuracy: {accuracy_simple:.4f} ({accuracy_simple*100:.2f}%)\")\n",
    "print(f\"Correct predictions: {np.sum(np.array(y_pred_simple) == y)}/{len(y)}\")\n",
    "\n",
    "# Compare with our best ML model\n",
    "print(f\"\\nBest ML Model Accuracy: {results[best_model_name]['accuracy']:.4f} ({results[best_model_name]['accuracy']*100:.2f}%)\")\n",
    "\n",
    "if accuracy_simple >= results[best_model_name]['accuracy']:\n",
    "    print(\"üéâ Amazing! Your simple classifier performed as well as the ML models!\")\n",
    "else:\n",
    "    print(\"ü§ñ Machine learning models found more complex patterns in the data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Feature Importance Analysis\n",
    "print(\"üèÜ Challenge 2: Feature Importance\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Which features are most important for classification?\")\n",
    "\n",
    "# Use Decision Tree to get feature importance\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Rankings:\")\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(feature_importance['feature'], feature_importance['importance'], color='lightgreen')\n",
    "plt.title('Feature Importance (Decision Tree)')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(f\"{feature_importance.iloc[0]['feature']} is the most important feature!\")\n",
    "print(\"This matches our simple classifier that used petal measurements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Real-world Application\n",
    "print(\"üèÜ Challenge 3: Real-world Application\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Apply what you've learned to a different problem\")\n",
    "\n",
    "# Create a synthetic dataset for house price prediction\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Generate synthetic house data\n",
    "house_size = np.random.normal(2000, 500, n_samples)  # Square feet\n",
    "bedrooms = np.random.randint(1, 6, n_samples)\n",
    "age = np.random.randint(0, 30, n_samples)\n",
    "\n",
    "# Generate prices with some noise\n",
    "price = (house_size * 150 + bedrooms * 5000 - age * 1000 + np.random.normal(0, 20000, n_samples))\n",
    "\n",
    "# Create DataFrame\n",
    "house_data = pd.DataFrame({\n",
    "    'size': house_size,\n",
    "    'bedrooms': bedrooms,\n",
    "    'age': age,\n",
    "    'price': price\n",
    "})\n",
    "\n",
    "print(\"üè† Synthetic House Price Dataset\")\n",
    "display(house_data.head())\n",
    "\n",
    "# Simple linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_house = house_data[['size', 'bedrooms', 'age']]\n",
    "y_house = house_data['price']\n",
    "\n",
    "# Split and scale\n",
    "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
    "    X_house, y_house, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_house = StandardScaler()\n",
    "X_train_house_scaled = scaler_house.fit_transform(X_train_house)\n",
    "X_test_house_scaled = scaler_house.transform(X_test_house)\n",
    "\n",
    "# Train model\n",
    "house_model = LinearRegression()\n",
    "house_model.fit(X_train_house_scaled, y_train_house)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_house = house_model.predict(X_test_house_scaled)\n",
    "mse = np.mean((y_pred_house - y_test_house) ** 2)\n",
    "mae = np.mean(np.abs(y_pred_house - y_test_house))\n",
    "r2 = house_model.score(X_test_house_scaled, y_test_house)\n",
    "\n",
    "print(f\"\\nüìä House Price Prediction Results:\")\n",
    "print(f\"Mean Squared Error: ${mse:,.0f}\")\n",
    "print(f\"Mean Absolute Error: ${mae:,.0f}\")\n",
    "print(f\"R¬≤ Score: {r2:.3f}\")\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_house, y_pred_house, alpha=0.7)\n",
    "plt.plot([y_test_house.min(), y_test_house.max()], \n",
    "         [y_test_house.min(), y_test_house.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.title('House Price Prediction: Actual vs Predicted')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "‚úÖ **Machine Learning Fundamentals**: The difference between traditional programming and ML\n",
    "\n",
    "‚úÖ **Types of ML**: Supervised, unsupervised, and reinforcement learning approaches\n",
    "\n",
    "‚úÖ **Supervised Learning**: Built classification models using Logistic Regression, Decision Trees, and KNN\n",
    "\n",
    "‚úÖ **Data Preparation**: Train/test splits, feature scaling, and data exploration\n",
    "\n",
    "‚úÖ **Model Evaluation**: Accuracy, confusion matrices, and classification reports\n",
    "\n",
    "‚úÖ **Unsupervised Learning**: Applied K-means clustering and the Elbow Method\n",
    "\n",
    "‚úÖ **Feature Importance**: Understanding which features drive predictions\n",
    "\n",
    "‚úÖ **Real-world Application**: Extended concepts to regression problems\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Training/Testing Split**: Prevents overfitting and gives unbiased evaluation\n",
    "- **Feature Scaling**: Standardizes features for better model performance\n",
    "- **Cross-validation**: Robust way to evaluate model performance\n",
    "- **Hyperparameter Tuning**: Finding optimal model parameters\n",
    "- **Feature Engineering**: Creating and selecting relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "### Continue Your Learning Journey:\n",
    "\n",
    "1. **üìö Next Notebook**: \"02_Regression_Analysis.ipynb\" - Dive deeper into regression techniques\n",
    "\n",
    "2. **üéØ Practice Exercises**: Try applying these concepts to other datasets from sklearn\n",
    "\n",
    "3. **üåê Real Data**: Download datasets from Kaggle and apply what you've learned\n",
    "\n",
    "4. **üìñ Recommended Reading\":\n",
    "   - \"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\" by Aur√©lien G√©ron\n",
    "   - \"Introduction to Statistical Learning\" by James, Witten, Hastie, and Tibshirani\n",
    "\n",
    "5. **üõ†Ô∏è Tools to Explore**:\n",
    "   - Pandas for data manipulation\n",
    "   - Scikit-learn for classical ML algorithms\n",
    "   - Matplotlib/Seaborn for visualization\n",
    "   - Jupyter notebooks for interactive development\n",
    "\n",
    "### Quick Self-Assessment:\n",
    "\n",
    "Can you explain:\n",
    "- The difference between supervised and unsupervised learning?\n",
    "- Why we split data into training and testing sets?\n",
    "- What the confusion matrix tells us about model performance?\n",
    "- How the Elbow Method helps find the optimal number of clusters?\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've completed your first machine learning notebook! You now have a solid foundation in ML concepts and practical skills that you can build upon in the next notebooks.\n",
    "\n",
    "**Remember**: The key to mastering machine learning is practice! Keep experimenting with different datasets and algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}