{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview"
   },
   "source": [
    "# Advanced Prompt Engineering Techniques\n",
    "\n",
    "**Interactive Notebook** - Section 13: Prompt Engineering and Advanced Techniques\n",
    "\n",
    "This notebook provides hands-on experience with cutting-edge prompt engineering techniques including chain-of-thought reasoning, tree-of-thoughts exploration, ReAct patterns, and enterprise-grade optimization strategies.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Implement chain-of-thought reasoning for complex problems\n",
    "- Use tree-of-thoughts for multi-path exploration\n",
    "- Build ReAct agents that can use external tools\n",
    "- Optimize prompts using systematic evaluation\n",
    "- Deploy enterprise-grade prompt engineering systems\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- Basic understanding of LLM APIs\n",
    "- Familiarity with prompt engineering concepts\n",
    "- Access to OpenAI or Anthropic APIs (or local models)\n",
    "\n",
    "**Estimated Time**: 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ðŸ”§ Setup and Installation\n",
    "\n",
    "Let's start by installing the required dependencies and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai anthropic pandas numpy matplotlib seaborn plotly ipywidgets tqdm\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "configuration"
   },
   "source": [
    "## âš™ï¸ Configuration\n",
    "\n",
    "Set up your API keys and configuration parameters. You can either set environment variables or enter them directly in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_configuration"
   },
   "outputs": [],
   "source": [
    "# API Configuration - Replace with your actual API keys\n",
    "OPENAI_API_KEY = \"sk-your-openai-key-here\"  # Replace with your OpenAI API key\n",
    "ANTHROPIC_API_KEY = \"sk-ant-your-anthropic-key-here\"  # Replace with your Anthropic API key\n",
    "\n",
    "# Model Configuration\n",
    "DEFAULT_MODEL = \"gpt-3.5-turbo\"  # Fallback to 3.5-turbo for cost efficiency\n",
    "MAX_TOKENS = 2000\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# Display configuration status\n",
    "api_status = {\n",
    "    \"OpenAI\": \"âœ… Configured\" if OPENAI_API_KEY != \"sk-your-openai-key-here\" else \"âŒ Not Configured\",\n",
    "    \"Anthropic\": \"âœ… Configured\" if ANTHROPIC_API_KEY != \"sk-ant-your-anthropic-key-here\" else \"âŒ Not Configured\"\n",
    "}\n",
    "\n",
    "display(Markdown(\"### API Configuration Status\"))\n",
    "for api, status in api_status.items():\n",
    "    display(Markdown(f\"- **{api}**: {status}\"))\n",
    "\n",
    "if not any(\"âœ…\" in status for status in api_status.values()):\n",
    "    display(Markdown(\"âš ï¸ **Warning**: No API keys configured. Some examples will use mock responses.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "core_classes"
   },
   "source": [
    "## ðŸ—ï¸ Core Classes and Utilities\n",
    "\n",
    "Let's define the core classes for our advanced prompt engineering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataclasses"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PromptConfig:\n",
    "    \"\"\"Configuration for prompt generation\"\"\"\n",
    "    model: str = DEFAULT_MODEL\n",
    "    max_tokens: int = MAX_TOKENS\n",
    "    temperature: float = TEMPERATURE\n",
    "    technique: str = \"standard\"\n",
    "    show_reasoning: bool = True\n",
    "\n",
    "@dataclass\n",
    "class TaskDescription:\n",
    "    \"\"\"Description of the task to be performed\"\"\"\n",
    "    description: str\n",
    "    domain: str = \"general\"\n",
    "    complexity: str = \"medium\"\n",
    "    output_format: str = \"text\"\n",
    "\n",
    "@dataclass\n",
    "class PromptResult:\n",
    "    \"\"\"Result of prompt generation and execution\"\"\"\n",
    "    prompt: str\n",
    "    response: str\n",
    "    technique: str\n",
    "    execution_time: float\n",
    "    token_count: int\n",
    "    cost: float\n",
    "    quality_score: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Performance metrics for prompt evaluation\"\"\"\n",
    "    accuracy: float\n",
    "    completeness: float\n",
    "    coherence: float\n",
    "    relevance: float\n",
    "    efficiency: float\n",
    "    overall_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mock_llm_client"
   },
   "outputs": [],
   "source": [
    "class MockLLMClient:\n",
    "    \"\"\"Mock LLM client for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.responses = {\n",
    "            \"chain_of_thought\": self._generate_cot_response,\n",
    "            \"tree_of_thoughts\": self._generate_tot_response,\n",
    "            \"react\": self._generate_react_response,\n",
    "            \"standard\": self._generate_standard_response\n",
    "        }\n",
    "    \n",
    "    async def generate_response(self, prompt: str, technique: str = \"standard\") -> str:\n",
    "        \"\"\"Generate a mock response based on the technique\"\"\"\n",
    "        await asyncio.sleep(0.5)  # Simulate API latency\n",
    "        \n",
    "        if technique in self.responses:\n",
    "            return self.responses[technique](prompt)\n",
    "        else:\n",
    "            return self._generate_standard_response(prompt)\n",
    "    \n",
    "    def _generate_cot_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate chain-of-thought response\"\"\"\n",
    "        return f\"\"\"\n",
    "Let me think through this step by step:\n",
    "\n",
    "Step 1: First, I need to understand what the prompt is asking for.\n",
    "The prompt appears to be requesting analysis of: \"{prompt[:100]}...\"\n",
    "\n",
    "Step 2: Breaking down the key components:\n",
    "- Main task: Analysis and reasoning\n",
    "- Domain: General problem-solving\n",
    "- Expected output: Structured response\n",
    "\n",
    "Step 3: Applying logical reasoning:\n",
    "Based on the input, I can identify several key insights:\n",
    "1. The problem requires systematic analysis\n",
    "2. Multiple perspectives should be considered\n",
    "3. A structured approach will yield the best results\n",
    "\n",
    "Step 4: Formulating the final answer:\n",
    "After careful consideration, here's my comprehensive analysis...\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_tot_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate tree-of-thoughts response\"\"\"\n",
    "        return f\"\"\"\n",
    "Exploring multiple reasoning paths for: \"{prompt[:100]}...\"\n",
    "\n",
    "Path 1: Analytical Approach\n",
    "- Break down the problem systematically\n",
    "- Use logical deduction\n",
    "- Consider all variables\n",
    "Evaluation: High accuracy, moderate speed\n",
    "\n",
    "Path 2: Creative Approach\n",
    "- Think outside conventional boundaries\n",
    "- Consider innovative solutions\n",
    "- Explore unconventional angles\n",
    "Evaluation: High creativity, variable reliability\n",
    "\n",
    "Path 3: Practical Approach\n",
    "- Focus on actionable steps\n",
    "- Consider real-world constraints\n",
    "- Prioritize implementable solutions\n",
    "Evaluation: High practicality, immediate applicability\n",
    "\n",
    "Selected Path: Analytical Approach (best balance of accuracy and completeness)\n",
    "\n",
    "Final analysis based on selected path...\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_react_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate ReAct (Reasoning + Acting) response\"\"\"\n",
    "        return f\"\"\"\n",
    "Thought: I need to analyze this prompt and determine what external tools or information might be helpful.\n",
    "\n",
    "The prompt: \"{prompt[:100]}...\" requires analysis and reasoning.\n",
    "\n",
    "Action: I should search for relevant information and tools that can help with this analysis.\n",
    "\n",
    "Observation: I have access to analysis tools and can reason through the problem systematically.\n",
    "\n",
    "Thought: Based on the available tools and information, I can approach this by:\n",
    "1. Breaking down the problem into manageable components\n",
    "2. Applying analytical reasoning\n",
    "3. Considering multiple perspectives\n",
    "4. Synthesizing a comprehensive answer\n",
    "\n",
    "Action: Proceeding with systematic analysis using available reasoning capabilities.\n",
    "\n",
    "Observation: The analysis is proceeding well and I'm generating structured insights.\n",
    "\n",
    "Thought: I have sufficient information to provide a comprehensive response.\n",
    "\n",
    "Final Answer: Here's my analysis based on the reasoning process...\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_standard_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate standard response\"\"\"\n",
    "        return f\"\"\"\n",
    "Based on the input \"{prompt[:100]}...\", I can provide the following analysis:\n",
    "\n",
    "This appears to be a request for analysis and reasoning. The key elements I can identify are:\n",
    "\n",
    "1. The prompt requires thoughtful consideration\n",
    "2. Multiple aspects need to be addressed\n",
    "3. A structured response would be most helpful\n",
    "\n",
    "After considering the request, here's my response:\n",
    "\n",
    "[Comprehensive analysis would go here in a real implementation]\n",
    "\"\"\"\n",
    "\n",
    "# Initialize mock client for demonstration\n",
    "mock_client = MockLLMClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "technique_1"
   },
   "source": [
    "## ðŸ§  Technique 1: Chain-of-Thought (CoT) Reasoning\n",
    "\n",
    "Chain-of-Thought prompting encourages models to show their reasoning process step-by-step, leading to improved performance on complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cot_implementation"
   },
   "outputs": [],
   "source": [
    "class ChainOfThoughtEngine:\n",
    "    \"\"\"Advanced Chain-of-Thought reasoning engine\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client):\n",
    "        self.llm_client = llm_client\n",
    "        self.templates = {\n",
    "            'math': self._math_template,\n",
    "            'logic': self._logic_template,\n",
    "            'analysis': self._analysis_template,\n",
    "            'creative': self._creative_template\n",
    "        }\n",
    "    \n",
    "    def generate_cot_prompt(self, task: TaskDescription, context: Dict = None) -> str:\n",
    "        \"\"\"Generate a Chain-of-Thought enhanced prompt\"\"\"\n",
    "        \n",
    "        base_template = self.templates.get(task.domain, self.templates['analysis'])\n",
    "        \n",
    "        cot_enhancement = \"\"\"\n",
    "\n",
    "IMPORTANT: Please solve this step-by-step, showing your reasoning at each stage:\n",
    "\n",
    "1. First, identify and analyze the key components of the problem\n",
    "2. Break down complex elements into manageable parts\n",
    "3. Solve each part systematically\n",
    "4. Combine the results to reach the final answer\n",
    "5. Verify your solution by checking it against the original problem\n",
    "\n",
    "Show all intermediate steps and calculations. Your reasoning should be clear and logical.\n",
    "\"\"\"\n",
    "        \n",
    "        return base_template.format(task=task, **(context or {})) + cot_enhancement\n",
    "    \n",
    "    async def execute_cot_reasoning(self, task: TaskDescription, context: Dict = None) -> PromptResult:\n",
    "        \"\"\"Execute Chain-of-Thought reasoning\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate CoT-enhanced prompt\n",
    "        cot_prompt = self.generate_cot_prompt(task, context)\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = await self.llm_client.generate_response(cot_prompt, \"chain_of_thought\")\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        return PromptResult(\n",
    "            prompt=cot_prompt,\n",
    "            response=response,\n",
    "            technique=\"chain_of_thought\",\n",
    "            execution_time=execution_time,\n",
    "            token_count=len(cot_prompt.split()) + len(response.split()),\n",
    "            cost=self._calculate_cost(cot_prompt, response),\n",
    "            quality_score=self._estimate_quality(response)\n",
    "        )\n",
    "    \n",
    "    def _math_template(self, task, **kwargs) -> str:\n",
    "        return f\"\"\"\n",
    "Mathematical Problem: {task.description}\n",
    "\n",
    "Please solve this mathematical problem step-by-step:\n",
    "\"\"\"\n",
    "    \n",
    "    def _logic_template(self, task, **kwargs) -> str:\n",
    "        return f\"\"\"\n",
    "Logical Reasoning Problem: {task.description}\n",
    "\n",
    "Please analyze this logical reasoning problem step-by-step:\n",
    "\"\"\"\n",
    "    \n",
    "    def _analysis_template(self, task, **kwargs) -> str:\n",
    "        return f\"\"\"\n",
    "Analysis Task: {task.description}\n",
    "\n",
    "Domain: {task.domain}\n",
    "Complexity: {task.complexity}\n",
    "\n",
    "Please provide a step-by-step analysis:\n",
    "\"\"\"\n",
    "    \n",
    "    def _creative_template(self, task, **kwargs) -> str:\n",
    "        return f\"\"\"\n",
    "Creative Task: {task.description}\n",
    "\n",
    "Please approach this creative task with step-by-step reasoning:\n",
    "\"\"\"\n",
    "    \n",
    "    def _calculate_cost(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Calculate approximate cost\"\"\"\n",
    "        # Simplified cost calculation\n",
    "        total_tokens = len(prompt.split()) + len(response.split())\n",
    "        return total_tokens * 0.00002  # $0.02 per 1K tokens\n",
    "    \n",
    "    def _estimate_quality(self, response: str) -> float:\n",
    "        \"\"\"Estimate response quality based on characteristics\"\"\"\n",
    "        # Look for step indicators\n",
    "        step_indicators = ['step', 'first', 'second', 'then', 'finally', 'therefore']\n",
    "        step_score = sum(1 for indicator in step_indicators if indicator.lower() in response.lower())\n",
    "        \n",
    "        # Look for logical connectors\n",
    "        logical_connectors = ['because', 'since', 'therefore', 'however', 'moreover']\n",
    "        logic_score = sum(1 for connector in logical_connectors if connector.lower() in response.lower())\n",
    "        \n",
    "        # Normalize to 0-1 scale\n",
    "        return min(1.0, (step_score + logic_score) / 10.0)\n",
    "\n",
    "# Initialize CoT engine\n",
    "cot_engine = ChainOfThoughtEngine(mock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cot_demo"
   },
   "source": [
    "### ðŸŽ® Interactive CoT Demonstration\n",
    "\n",
    "Let's test Chain-of-Thought reasoning with different types of problems. Use the widgets below to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cot_interactive"
   },
   "outputs": [],
   "source": [
    "# Create interactive widgets for CoT demonstration\n",
    "task_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Mathematical Problem', 'math'),\n",
    "        ('Logical Reasoning', 'logic'), \n",
    "        ('Data Analysis', 'analysis'),\n",
    "        ('Creative Problem', 'creative')\n",
    "    ],\n",
    "    value='analysis',\n",
    "    description='Task Type:'\n",
    ")\n",
    "\n",
    "complexity_slider = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Complexity:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "task_input = widgets.Textarea(\n",
    "    value='A company has 150 employees. If 20% are in sales, 30% in engineering, and the rest in other departments, how many employees are in other departments?',\n",
    "    placeholder='Enter your task or problem...',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "execute_button = widgets.Button(\n",
    "    description='Execute CoT Reasoning',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "async def execute_cot_demo(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        display(Markdown(\"### ðŸ”„ Executing Chain-of-Thought Reasoning...\"))\n",
    "        \n",
    "        # Create task description\n",
    "        task = TaskDescription(\n",
    "            description=task_input.value,\n",
    "            domain=task_dropdown.value,\n",
    "            complexity=['low', 'medium', 'high', 'very high', 'expert'][complexity_slider.value - 1]\n",
    "        )\n",
    "        \n",
    "        # Execute CoT reasoning\n",
    "        result = await cot_engine.execute_cot_reasoning(task)\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"\"\"### ðŸ“Š Results\n",
    "\n",
    "**Technique**: Chain-of-Thought Reasoning  \n",
    "**Domain**: {task.domain.title()}  \n",
    "**Complexity**: {task.complexity.title()}  \n",
    "**Execution Time**: {result.execution_time:.2f}s  \n",
    "**Estimated Cost**: ${result.cost:.4f}  \n",
    "**Quality Score**: {result.quality_score:.2f}/1.0\n",
    "\n",
    "### ðŸ¤– Generated Response\n",
    "```markdown\n",
    "{result.response}\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "execute_button.on_click(execute_cot_demo)\n",
    "\n",
    "# Display widgets\n",
    "display(Markdown(\"## ðŸŽ›ï¸ Chain-of-Thought Configuration\"))\n",
    "display(widgets.VBox([task_dropdown, complexity_slider, task_input, execute_button]))\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "technique_2"
   },
   "source": [
    "## ðŸŒ³ Technique 2: Tree-of-Thoughts (ToT) Exploration\n",
    "\n",
    "Tree-of-Thoughts extends CoT by exploring multiple reasoning paths simultaneously and selecting the most promising one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tot_implementation"
   },
   "outputs": [],
   "source": [
    "class TreeOfThoughtsEngine:\n",
    "    \"\"\"Tree-of-Thoughts exploration engine\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client):\n",
    "        self.llm_client = llm_client\n",
    "        self.max_branches = 3\n",
    "        self.max_depth = 2\n",
    "    \n",
    "    def generate_tot_prompt(self, task: TaskDescription) -> str:\n",
    "        \"\"\"Generate a Tree-of-Thoughts enhanced prompt\"\"\"\n",
    "        \n",
    "        return f\"\"\"\n",
    "Complex Problem: {task.description}\n",
    "\n",
    "Domain: {task.domain}\n",
    "Complexity: {task.complexity}\n",
    "\n",
    "TREE OF THOUGHTS APPROACH:\n",
    "Please explore multiple solution paths simultaneously:\n",
    "\n",
    "1. Generate {self.max_branches} different initial approaches to this problem\n",
    "2. For each approach:\n",
    "   a. Evaluate its potential effectiveness\n",
    "   b. Identify pros and cons\n",
    "   c. Estimate success probability (0-100%)\n",
    "   d. Consider required resources and constraints\n",
    "3. Select the most promising approach(es)\n",
    "4. Develop the selected approach(es) further with sub-steps\n",
    "5. Compare final solutions and select the best one\n",
    "\n",
    "Document your reasoning tree structure clearly with branching points and decision criteria.\n",
    "\n",
    "Format your response as:\n",
    "- **Path 1**: [Approach name]\n",
    "  - Evaluation: [Effectiveness assessment]\n",
    "  - Pros: [Advantages]\n",
    "  - Cons: [Disadvantages]\n",
    "  - Success Probability: [0-100%]\n",
    "  \n",
    "- **Selected Path**: [Best approach]\n",
    "- **Final Solution**: [Comprehensive answer based on selected path]\n",
    "\"\"\"\n",
    "    \n",
    "    async def explore_thoughts(self, task: TaskDescription) -> PromptResult:\n",
    "        \"\"\"Execute Tree-of-Thoughts exploration\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate ToT-enhanced prompt\n",
    "        tot_prompt = self.generate_tot_prompt(task)\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = await self.llm_client.generate_response(tot_prompt, \"tree_of_thoughts\")\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        return PromptResult(\n",
    "            prompt=tot_prompt,\n",
    "            response=response,\n",
    "            technique=\"tree_of_thoughts\",\n",
    "            execution_time=execution_time,\n",
    "            token_count=len(tot_prompt.split()) + len(response.split()),\n",
    "            cost=self._calculate_cost(tot_prompt, response),\n",
    "            quality_score=self._estimate_tot_quality(response)\n",
    "        )\n",
    "    \n",
    "    def _calculate_cost(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Calculate approximate cost\"\"\"\n",
    "        total_tokens = len(prompt.split()) + len(response.split())\n",
    "        return total_tokens * 0.00002\n",
    "    \n",
    "    def _estimate_tot_quality(self, response: str) -> float:\n",
    "        \"\"\"Estimate ToT response quality\"\"\"\n",
    "        # Look for path exploration indicators\n",
    "        path_indicators = ['path', 'approach', 'branch', 'option']\n",
    "        path_score = sum(1 for indicator in path_indicators if indicator.lower() in response.lower())\n",
    "        \n",
    "        # Look for evaluation criteria\n",
    "        eval_indicators = ['evaluate', 'probability', 'pros', 'cons', 'effectiveness']\n",
    "        eval_score = sum(1 for indicator in eval_indicators if indicator.lower() in response.lower())\n",
    "        \n",
    "        # Look for selection logic\n",
    "        select_indicators = ['selected', 'best', 'chosen', 'final']\n",
    "        select_score = sum(1 for indicator in select_indicators if indicator.lower() in response.lower())\n",
    "        \n",
    "        # Normalize to 0-1 scale\n",
    "        return min(1.0, (path_score + eval_score + select_score) / 15.0)\n",
    "\n",
    "# Initialize ToT engine\n",
    "tot_engine = TreeOfThoughtsEngine(mock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tot_comparison"
   },
   "source": [
    "### ðŸ“Š CoT vs ToT Comparison\n",
    "\n",
    "Let's compare Chain-of-Thought and Tree-of-Thoughts approaches on the same problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison_demo"
   },
   "outputs": [],
    "source": [
    "comparison_task = TaskDescription(\n",
    "    description=\"Design a sustainable urban transportation system for a city of 1 million people\",\n",
    "    domain=\"analysis\",\n",
    "    complexity=\"high\"\n",
    ")\n",
    "\n",
    "async def run_comparison():\n",
    "    display(Markdown(\"### ðŸ”„ Running CoT vs ToT Comparison...\"))\n",
    "    \n",
    "    # Execute both techniques\n",
    "    cot_result = await cot_engine.execute_cot_reasoning(comparison_task)\n",
    "    tot_result = await tot_engine.explore_thoughts(comparison_task)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = {\n",
    "        'Technique': ['Chain-of-Thought', 'Tree-of-Thoughts'],\n",
    "        'Execution Time (s)': [cot_result.execution_time, tot_result.execution_time],\n",
    "        'Quality Score': [cot_result.quality_score, tot_result.quality_score],\n",
    "        'Token Count': [cot_result.token_count, tot_result.token_count],\n",
    "        'Estimated Cost ($)': [cot_result.cost, tot_result.cost]\n",
    "    }\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display comparison\n",
    "    display(Markdown(\"### ðŸ“ˆ Performance Comparison\"))\n",
    "    display(df_comparison.style.background_gradient(cmap='Blues', subset=['Quality Score'])\n",
    "                   .format({'Execution Time (s)': '{:.2f}', 'Estimated Cost ($)': '${:.4f}'}))\n",
    "    \n",
    "    # Create visual comparison\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Execution Time', 'Quality Score', 'Token Count', 'Cost'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Execution Time\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=['CoT', 'ToT'], y=[cot_result.execution_time, tot_result.execution_time],\n",
    "               name='Execution Time', marker_color='lightblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Quality Score\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=['CoT', 'ToT'], y=[cot_result.quality_score, tot_result.quality_score],\n",
    "               name='Quality Score', marker_color='lightgreen'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Token Count\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=['CoT', 'ToT'], y=[cot_result.token_count, tot_result.token_count],\n",
    "               name='Token Count', marker_color='lightcoral'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Cost\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=['CoT', 'ToT'], y=[cot_result.cost, tot_result.cost],\n",
    "               name='Cost', marker_color='lightyellow'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Chain-of-Thought vs Tree-of-Thoughts Comparison\",\n",
    "        showlegend=False,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Show sample responses\n",
    "    display(Markdown(\"### ðŸ“ Sample Responses\"))\n",
    "    \n",
    "    display(Markdown(\"**Chain-of-Thought Response (excerpt):**\"))\n",
    "    display(Markdown(f\"```markdown\\n{cot_result.response[:500]}...\\n```\"))\n",
    "    \n",
    "    display(Markdown(\"**Tree-of-Thoughts Response (excerpt):**\"))\n",
    "    display(Markdown(f\"```markdown\\n{tot_result.response[:500]}...\\n```\"))\n",
    "\n",
    "# Run the comparison\n",
    "await run_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "technique_3"
   },
   "source": [
    "## ðŸ¤– Technique 3: ReAct (Reasoning + Acting)\n",
    "\n",
    "ReAct combines reasoning with action-taking capabilities, allowing AI systems to interact with external tools and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "react_implementation"
   },
   "outputs": [],
   "source": [
    "class ReActEngine:\n",
    "    \"\"\"ReAct (Reasoning + Acting) engine\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = {\n",
    "            'search': self._search_tool,\n",
    "            'calculate': self._calculate_tool,\n",
    "            'analyze': self._analyze_tool,\n",
    "            'summarize': self._summarize_tool\n",
    "        }\n",
    "    \n",
    "    def generate_react_prompt(self, task: TaskDescription, available_tools: List[str] = None) -> str:\n",
    "        \"\"\"Generate a ReAct-enhanced prompt\"\"\"\n",
    "        \n",
    "        tools_desc = self._get_tools_description(available_tools or list(self.tools.keys()))\n",
    "        \n",
    "        return f\"\"\"\n",
    "Task: {task.description}\n",
    "\n",
    "Domain: {task.domain}\n",
    "Complexity: {task.complexity}\n",
    "\n",
    "REACT (Reasoning + Acting) Approach:\n",
    "You are an AI assistant that can reason and take actions. For each step, first think about what you need to do, then take the appropriate action.\n",
    "\n",
    "Available Tools:\n",
    "{tools_desc}\n",
    "\n",
    "Instructions:\n",
    "1. Start by analyzing the problem and determining what information or actions you need\n",
    "2. Use the Thought-Action-Observation cycle:\n",
    "   - Thought: What do I need to do next?\n",
    "   - Action: Which tool should I use? (specify tool name and parameters)\n",
    "   - Observation: What did I learn from the action?\n",
    "3. Continue this cycle until you have enough information to provide a comprehensive answer\n",
    "4. Provide your final answer based on all observations\n",
    "\n",
    "Format each step as:\n",
    "Thought: [your reasoning]\n",
    "Action: [tool_name]([parameters])\n",
    "Observation: [result of action]\n",
    "\n",
    "Important: Be specific about what you're looking for in each action.\n",
    "\"\"\"\n",
    "    \n",
    "    async def execute_react_reasoning(self, task: TaskDescription, max_steps: int = 5) -> PromptResult:\n",
    "        \"\"\"Execute ReAct reasoning\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate ReAct-enhanced prompt\n",
    "        react_prompt = self.generate_react_prompt(task)\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = await self.llm_client.generate_response(react_prompt, \"react\")\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        return PromptResult(\n",
    "            prompt=react_prompt,\n",
    "            response=response,\n",
    "            technique=\"react\",\n",
    "            execution_time=execution_time,\n",
    "            token_count=len(react_prompt.split()) + len(response.split()),\n",
    "            cost=self._calculate_cost(react_prompt, response),\n",
    "            quality_score=self._estimate_react_quality(response)\n",
    "        )\n",
    "    \n",
    "    def _get_tools_description(self, tools: List[str]) -> str:\n",
    "        \"\"\"Get description of available tools\"\"\"\n",
    "        descriptions = {\n",
    "            'search': 'Search for information (query: str) -> results: str',\n",
    "            'calculate': 'Perform mathematical calculations (expression: str) -> result: float',\n",
    "            'analyze': 'Analyze data or text (data: str, analysis_type: str) -> insights: str',\n",
    "            'summarize': 'Summarize text content (text: str, max_length: int) -> summary: str'\n",
    "        }\n",
    "        \n",
    "        return '\\n'.join(f\"- {tool}: {descriptions.get(tool, 'No description')}\" for tool in tools)\n",
    "    \n",
    "    def _search_tool(self, query: str) -> str:\n",
    "        \"\"\"Mock search tool\"\"\"\n",
    "        return f\"Search results for '{query}': Found relevant information including key concepts and examples.\"\n",
    "    \n",
    "    def _calculate_tool(self, expression: str) -> float:\n",
    "        \"\"\"Mock calculation tool\"\"\"\n",
    "        try:\n",
    "            # Simple expression evaluation for demonstration\n",
    "            if '+' in expression:\n",
    "                return sum(float(x) for x in expression.split('+'))\n",
    "            elif '*' in expression:\n",
    "                result = 1\n",
    "                for x in expression.split('*'):\n",
    "                    result *= float(x)\n",
    "                return result\n",
    "            else:\n",
    "                return float(expression)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _analyze_tool(self, data: str, analysis_type: str) -> str:\n",
    "        \"\"\"Mock analysis tool\"\"\"\n",
    "        return f\"Analysis of '{data[:50]}...' using {analysis_type}: Identified patterns and insights.\"\n",
    "    \n",
    "    def _summarize_tool(self, text: str, max_length: int) -> str:\n",
    "        \"\"\"Mock summarization tool\"\"\"\n",
    "        return f\"Summary of text (max {max_length} chars): {text[:max_length]}...\"\n",
    "    \n",
    "    def _calculate_cost(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Calculate approximate cost\"\"\"\n",
    "        total_tokens = len(prompt.split()) + len(response.split())\n",
    "        return total_tokens * 0.00002\n",
    "    \n",
    "    def _estimate_react_quality(self, response: str) -> float:\n",
    "        \"\"\"Estimate ReAct response quality\"\"\"\n",
    "        # Look for ReAct pattern indicators\n",
    "        react_indicators = ['thought:', 'action:', 'observation:']\n",
    "        react_score = sum(1 for indicator in react_indicators if indicator.lower() in response.lower())\n",
    "        \n",
    "        # Look for tool usage\n",
    "        tool_indicators = ['search', 'calculate', 'analyze', 'summarize']\n",
    "        tool_score = sum(1 for indicator in tool_indicators if indicator.lower() in response.lower())\n",
    "        \n",
    "        # Normalize to 0-1 scale\n",
    "        return min(1.0, (react_score + tool_score) / 10.0)\n",
    "\n",
    "# Initialize ReAct engine\n",
    "react_engine = ReActEngine(mock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "react_demo"
   },
   "source": [
    "### ðŸŽ® Interactive ReAct Demonstration\n",
    "\n",
    "Test the ReAct framework with different tool combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "react_interactive"
   },
   "outputs": [],
    "source": [
    "# Create interactive widgets for ReAct demonstration\n",
    "tool_checkboxes = widgets.SelectMultiple(\n",
    "    options=['search', 'calculate', 'analyze', 'summarize'],\n",
    "    value=['search', 'analyze'],\n",
    "    description='Available Tools:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "react_task_input = widgets.Textarea(\n",
    "    value='What are the key factors affecting employee productivity in remote work environments?',\n",
    "    placeholder='Enter a task that would benefit from tool usage...',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "max_steps_slider = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Max Steps:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "react_execute_button = widgets.Button(\n",
    "    description='Execute ReAct Reasoning',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "react_output_area = widgets.Output()\n",
    "\n",
    "async def execute_react_demo(b):\n",
    "    with react_output_area:\n",
    "        react_output_area.clear_output()\n",
    "        \n",
    "        display(Markdown(\"### ðŸ”„ Executing ReAct Reasoning...\"))\n",
    "        \n",
    "        # Create task description\n",
    "        task = TaskDescription(\n",
    "            description=react_task_input.value,\n",
    "            domain=\"analysis\",\n",
    "            complexity=\"medium\"\n",
    "        )\n",
    "        \n",
    "        # Execute ReAct reasoning\n",
    "        result = await react_engine.execute_react_reasoning(task, max_steps_slider.value)\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"\"\"### ðŸ“Š ReAct Results\n",
    "\n",
    "**Available Tools**: {', '.join(tool_checkboxes.value)}  \n",
    "**Max Steps**: {max_steps_slider.value}  \n",
    "**Execution Time**: {result.execution_time:.2f}s  \n",
    "**Estimated Cost**: ${result.cost:.4f}  \n",
    "**Quality Score**: {result.quality_score:.2f}/1.0\n",
    "\n",
    "### ðŸ¤– ReAct Response\n",
    "```markdown\n",
    "{result.response}\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "react_execute_button.on_click(execute_react_demo)\n",
    "\n",
    "# Display widgets\n",
    "display(Markdown(\"## ðŸŽ›ï¸ ReAct Configuration\"))\n",
    "display(widgets.VBox([tool_checkboxes, max_steps_slider, react_task_input, react_execute_button]))\n",
    "display(react_output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_analysis"
   },
   "source": [
    "## ðŸ“ˆ Performance Analysis & Optimization\n",
    "\n",
    "Let's analyze the performance of different prompt engineering techniques and identify optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_benchmark"
   },
   "outputs": [],
    "source": [
    "async def benchmark_techniques():\n",
    "    \"\"\"Benchmark different prompt engineering techniques\"\"\"\n",
    "    \n",
    "    # Test tasks\n",
    "    test_tasks = [\n",
    "        TaskDescription(\"Calculate the total cost of 150 items at $23.50 each with 8% tax\", \"math\", \"medium\"),\n",
    "        TaskDescription(\"Analyze the pros and cons of remote work policies\", \"analysis\", \"medium\"),\n",
    "        TaskDescription(\"Design a sustainable urban garden\", \"creative\", \"medium\"),\n",
    "        TaskDescription(\"Debug why a Python program is crashing\", \"logic\", \"high\"),\n",
    "    ]\n",
    "    \n",
    "    techniques = [\n",
    "        (\"Standard\", lambda task: mock_client.generate_response(task.description, \"standard\")),\n",
    "        (\"Chain-of-Thought\", lambda task: cot_engine.execute_cot_reasoning(task)),\n",
    "        (\"Tree-of-Thoughts\", lambda task: tot_engine.explore_thoughts(task)),\n",
    "        (\"ReAct\", lambda task: react_engine.execute_react_reasoning(task)),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    display(Markdown(\"### ðŸ”„ Running Performance Benchmark...\"))\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        display(Markdown(f\"**Testing**: {task.description[:50]}...\"))\n",
    "        \n",
    "        for technique_name, technique_func in techniques:\n",
    "            try:\n",
    "                if technique_name == \"Standard\":\n",
    "                    start_time = time.time()\n",
    "                    response = await technique_func(task)\n",
    "                    execution_time = time.time() - start_time\n",
    "                    \n",
    "                    result = PromptResult(\n",
    "                        prompt=task.description,\n",
    "                        response=response,\n",
    "                        technique=technique_name.lower(),\n",
    "                        execution_time=execution_time,\n",
    "                        token_count=len(task.description) + len(response),\n",
    "                        cost=execution_time * 0.001,\n",
    "                        quality_score=0.7  # Mock quality score\n",
    "                    )\n",
    "                else:\n",
    "                    result = await technique_func(task)\n",
    "                \n",
    "                results.append({\n",
    "                    'task': task.description[:50] + '...',\n",
    "                    'technique': technique_name,\n",
    "                    'execution_time': result.execution_time,\n",
    "                    'quality_score': result.quality_score,\n",
    "                    'token_count': result.token_count,\n",
    "                    'cost': result.cost\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {technique_name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_df = await benchmark_techniques()\n",
    "\n",
    "# Display comprehensive results\n",
    "display(Markdown(\"### ðŸ“Š Comprehensive Performance Benchmark\"))\n",
    "display(benchmark_df.style.background_gradient(cmap='RdYlGn', subset=['quality_score'])\n",
    "               .format({'execution_time': '{:.3f}s', 'cost': '${:.4f}'}))\n",
    "\n",
    "# Create visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Execution Time by Technique', 'Quality Score by Technique', \n",
    "                   'Cost vs Quality', 'Token Efficiency'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Execution Time by Technique\n",
    "time_data = benchmark_df.groupby('technique')['execution_time'].mean().reset_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=time_data['technique'], y=time_data['execution_time'],\n",
    "           name='Avg Execution Time', marker_color='lightblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Quality Score by Technique\n",
    "quality_data = benchmark_df.groupby('technique')['quality_score'].mean().reset_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=quality_data['technique'], y=quality_data['quality_score'],\n",
    "           name='Avg Quality Score', marker_color='lightgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Cost vs Quality Scatter\n",
    "for technique in benchmark_df['technique'].unique():\n",
    "    technique_data = benchmark_df[benchmark_df['technique'] == technique]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=technique_data['cost'],\n",
    "            y=technique_data['quality_score'],\n",
    "            mode='markers',\n",
    "            name=technique,\n",
    "            marker=dict(size=8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Token Efficiency (Quality per Token)\n",
    "benchmark_df['efficiency'] = benchmark_df['quality_score'] / benchmark_df['token_count']\n",
    "efficiency_data = benchmark_df.groupby('technique')['efficiency'].mean().reset_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=efficiency_data['technique'], y=efficiency_data['efficiency'],\n",
    "           name='Token Efficiency', marker_color='lightcoral'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Prompt Engineering Techniques Performance Analysis\",\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimization_strategies"
   },
   "source": [
    "## ðŸŽ¯ Optimization Strategies\n",
    "\n",
    "Based on our analysis, here are key optimization strategies for prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimization_analysis"
   },
   "outputs": [],
    "source": [
    "# Analyze optimization opportunities\n",
    "display(Markdown(\"### ðŸ“ˆ Optimization Insights\"))\n",
    "\n",
    "# Calculate key metrics\n",
    "technique_stats = benchmark_df.groupby('technique').agg({\n",
    "    'execution_time': ['mean', 'std'],\n",
    "    'quality_score': ['mean', 'std'],\n",
    "    'cost': ['mean', 'std'],\n",
    "    'token_count': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "display(Markdown(\"#### Technique Performance Statistics\"))\n",
    "display(technique_stats)\n",
    "\n",
    "# Optimization recommendations\n",
    "optimization_tips = [\n",
    "    {\n",
    "        'technique': 'Chain-of-Thought',\n",
    "        'best_for': 'Complex mathematical and logical problems',\n",
    "        'optimization': 'Use for problems requiring step-by-step reasoning',\n",
    "        'cost_tip': 'Moderate cost, high accuracy for structured problems'\n",
    "    },\n",
    "    {\n",
    "        'technique': 'Tree-of-Thoughts',\n",
    "        'best_for': 'Creative and multi-solution problems',\n",
    "        'optimization': 'Ideal for exploring multiple approaches',\n",
    "        'cost_tip': 'Higher cost but better for ambiguous problems'\n",
    "    },\n",
    "    {\n",
    "        'technique': 'ReAct',\n",
    "        'best_for': 'Problems requiring external information',\n",
    "        'optimization': 'Use when tools and APIs can enhance reasoning',\n",
    "        'cost_tip': 'Variable cost depending on tool usage'\n",
    "    },\n",
    "    {\n",
    "        'technique': 'Standard',\n",
    "        'best_for': 'Simple, straightforward tasks',\n",
    "        'optimization': 'Most efficient for basic queries',\n",
    "        'cost_tip': 'Lowest cost, sufficient for simple tasks'\n",
    "    }\n",
    "]\n",
    "\n",
    "for tip in optimization_tips:\n",
    "    display(Markdown(f\"\"\"#### {tip['technique']}\n",
    "- **Best For**: {tip['best_for']}\n",
    "- **Optimization**: {tip['optimization']}\n",
    "- **Cost Tip**: {tip['cost_tip']}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "production_systems"
   },
   "source": [
    "## ðŸ­ Production Systems Integration\n",
    "\n",
    "Let's explore how to integrate these techniques into production systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "production_implementation"
   },
   "outputs": [],
    "source": [
    "class ProductionPromptEngine:\n",
    "    \"\"\"Enterprise-grade prompt engineering system\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client):\n",
    "        self.llm_client = llm_client\n",
    "        self.cot_engine = ChainOfThoughtEngine(llm_client)\n",
    "        self.tot_engine = TreeOfThoughtsEngine(llm_client)\n",
    "        self.react_engine = ReActEngine(llm_client)\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.performance_history = []\n",
    "        self.cache = {}\n",
    "    \n",
    "    async def optimize_and_execute(self, task: TaskDescription, \n",
    "                                  context: Dict = None,\n",
    "                                  technique: str = \"auto\") -> PromptResult:\n",
    "        \"\"\"Automatically select and execute best technique\"\"\"\n",
    "        \n",
    "        # Technique selection logic\n",
    "        if technique == \"auto\":\n",
    "            technique = self._select_best_technique(task, context)\n",
    "        \n",
    "        # Execute with selected technique\n",
    "        if technique == \"chain_of_thought\":\n",
    "            return await self.cot_engine.execute_cot_reasoning(task, context)\n",
    "        elif technique == \"tree_of_thoughts\":\n",
    "            return await self.tot_engine.explore_thoughts(task)\n",
    "        elif technique == \"react\":\n",
    "            return await self.react_engine.execute_react_reasoning(task)\n",
    "        else:\n",
    "            # Standard response\n",
    "            start_time = time.time()\n",
    "            response = await self.llm_client.generate_response(task.description, \"standard\")\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            return PromptResult(\n",
    "                prompt=task.description,\n",
    "                response=response,\n",
    "                technique=\"standard\",\n",
    "                execution_time=execution_time,\n",
    "                token_count=len(task.description) + len(response),\n",
    "                cost=execution_time * 0.001,\n",
    "                quality_score=0.7\n",
    "            )\n",
    "    \n",
    "    def _select_best_technique(self, task: TaskDescription, context: Dict) -> str:\n",
    "        \"\"\"Automatically select the best technique based on task characteristics\"\"\"\n",
    "        \n",
    "        # Simple heuristic-based selection\n",
    "        if task.domain == \"math\":\n",
    "            return \"chain_of_thought\"\n",
    "        elif task.domain == \"creative\":\n",
    "            return \"tree_of_thoughts\"\n",
    "        elif task.complexity == \"high\":\n",
    "            return \"tree_of_thoughts\"\n",
    "        elif context and context.get(\"tools_available\"):\n",
    "            return \"react\"\n",
    "        else:\n",
    "            return \"chain_of_thought\"  # Default to CoT\n",
    "    \n",
    "    def get_performance_dashboard(self) -> Dict:\n",
    "        \"\"\"Generate performance dashboard data\"\"\"\n",
    "        if not self.performance_history:\n",
    "            return {\"message\": \"No performance data available\"}\n",
    "        \n",
    "        df = pd.DataFrame(self.performance_history)\n",
    "        \n",
    "        dashboard = {\n",
    "            \"total_requests\": len(df),\n",
    "            \"avg_execution_time\": df['execution_time'].mean(),\n",
    "            \"avg_quality_score\": df['quality_score'].mean(),\n",
    "            \"total_cost\": df['cost'].sum(),\n",
    "            \"technique_usage\": df['technique'].value_counts().to_dict(),\n",
    "            \"performance_by_technique\": df.groupby('technique')[['quality_score', 'execution_time']].mean().to_dict()\n",
    "        }\n",
    "        \n",
    "        return dashboard\n",
    "\n",
    "# Initialize production engine\n",
    "production_engine = ProductionPromptEngine(mock_client)\n",
    "\n",
    "# Demonstrate auto-optimization\n",
    "demo_tasks = [\n",
    "    TaskDescription(\"Calculate compound interest on $10,000 at 5% for 10 years\", \"math\", \"medium\"),\n",
    "    TaskDescription(\"Design a mobile app for habit tracking\", \"creative\", \"high\"),\n",
    "    TaskDescription(\"Find the best restaurants in Tokyo for sushi\", \"analysis\", \"medium\", \"tools_available\"),\n",
    "]\n",
    "\n",
    "async def demonstrate_production_system():\n",
    "    display(Markdown(\"### ðŸ­ Production System Demonstration\"))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for task in demo_tasks:\n",
    "        display(Markdown(f\"**Task**: {task.description}\"))\n",
    "        \n",
    "        # Auto-select and execute technique\n",
    "        result = await production_engine.optimize_and_execute(task)\n",
    "        \n",
    "        selected_technique = production_engine._select_best_technique(task, {})\n",
    "        \n",
    "        display(Markdown(f\"- **Selected Technique**: {selected_technique.replace('_', ' ').title()}\"))\n",
    "        display(Markdown(f\"- **Execution Time**: {result.execution_time:.2f}s\"))\n",
    "        display(Markdown(f\"- **Quality Score**: {result.quality_score:.2f}/1.0\"))\n",
    "        display(Markdown(f\"- **Cost**: ${result.cost:.4f}\"))\n",
    "        \n",
    "        results.append(result)\n",
    "        production_engine.performance_history.append({\n",
    "            'technique': result.technique,\n",
    "            'execution_time': result.execution_time,\n",
    "            'quality_score': result.quality_score,\n",
    "            'cost': result.cost\n",
    "        })\n",
    "        \n",
    "        display(Markdown(\"---\"))\n",
    "    \n",
    "    # Show dashboard\n",
    "    dashboard = production_engine.get_performance_dashboard()\n",
    "    \n",
    "    display(Markdown(\"### ðŸ“Š Performance Dashboard\"))\n",
    "    for key, value in dashboard.items():\n",
    "        if isinstance(value, dict):\n",
    "            display(Markdown(f\"**{key.replace('_', ' ').title()}:**\"))\n",
    "            for k, v in value.items():\n",
    "                if isinstance(v, dict):\n",
    "                    for sub_k, sub_v in v.items():\n",
    "                        display(Markdown(f\"- {sub_k}: {sub_v:.3f}\"))\n",
    "                else:\n",
    "                    display(Markdown(f\"- {k}: {v}\"))\n",
    "        else:\n",
    "            display(Markdown(f\"**{key.replace('_', ' ').title()}:** {value}\"))\n",
    "\n",
    "# Run production demonstration\n",
    "await demonstrate_production_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "best_practices"
   },
   "source": [
    "## ðŸ“‹ Best Practices and Guidelines\n",
    "\n",
    "Based on our comprehensive analysis, here are the key best practices for advanced prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "best_practices_summary"
   },
   "outputs": [],
    "source": [
    "# Create best practices summary\n",
    "best_practices = {\n",
    "    \"Technique Selection\": {\n",
    "        \"Chain-of-Thought\": \"Best for structured, logical problems with clear steps\",\n",
    "        \"Tree-of-Thoughts\": \"Ideal for creative problems with multiple solutions\",\n",
    "        \"ReAct\": \"Perfect for tasks requiring external tools or information\",\n",
    "        \"Standard\": \"Most efficient for simple, straightforward queries\"\n",
    "    },\n",
    "    \"Performance Optimization\": {\n",
    "        \"Cache Results\": \"Implement prompt caching for repeated queries\",\n",
    "        \"Batch Processing\": \"Process similar prompts together for efficiency\",\n",
    "        \"Model Selection\": \"Choose appropriate model based on complexity\",\n",
    "        \"Token Management\": \"Optimize prompts to reduce token usage\"\n",
    "    },\n",
    "    \"Quality Assurance\": {\n",
    "        \"Output Validation\": \"Validate responses against expected criteria\",\n",
    "        \"A/B Testing\": \"Continuously test and refine prompts\",\n",
    "        \"Human Review\": \"Include human oversight for critical applications\",\n",
    "        \"Feedback Loops\": \"Collect and incorporate user feedback\"\n",
    "    },\n",
    "    \"Production Deployment\": {\n",
    "        \"Monitoring\": \"Track performance metrics and costs\",\n",
    "        \"Scalability\": \"Design for horizontal scaling\",\n",
    "        \"Error Handling\": \"Implement robust error recovery\",\n",
    "        \"Security\": \"Include input validation and rate limiting\"\n",
    "    }\n",
    "}\n",
    "\n",
    "display(Markdown(\"## ðŸŽ¯ Best Practices Summary\"))\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    display(Markdown(f\"### {category}\"))\n",
    "    for practice, description in practices.items():\n",
    "        display(Markdown(f\"- **{practice}**: {description}\"))\n",
    "    display(Markdown(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ðŸŽ‰ Conclusion & Next Steps\n",
    "\n",
    "### ðŸ† What You've Learned\n",
    "\n",
    "âœ… **Chain-of-Thought Reasoning**: Step-by-step logical reasoning for complex problems  \n",
    "âœ… **Tree-of-Thoughts Exploration**: Multi-path problem solving with optimal path selection  \n",
    "âœ… **ReAct Framework**: Integration of reasoning with external tools and APIs  \n",
    "âœ… **Performance Analysis**: Comprehensive benchmarking and optimization strategies  \n",
    "âœ… **Production Integration**: Enterprise-grade deployment patterns  \n",
    "\n",
    "### ðŸš€ Real-World Applications\n",
    "\n",
    "- **Customer Service**: Automated support with intelligent reasoning\n",
    "- **Content Creation**: Enhanced writing and creative processes\n",
    "- **Data Analysis**: Complex analytical workflows\n",
    "- **Education**: Intelligent tutoring and learning systems\n",
    "- **Research**: Automated literature review and synthesis\n",
    "\n",
    "### ðŸ“š Further Learning\n",
    "\n",
    "1. **Advanced Topics**: Meta-prompting, self-consistency decoding\n",
    "2. **Model Fine-tuning**: Custom models for specific domains\n",
    "3. **Multi-Modal Integration**: Combining text, images, and audio\n",
    "4. **Safety & Ethics**: Responsible AI deployment practices\n",
    "5. **Emerging Research**: Latest papers and techniques\n",
    "\n",
    "### ðŸ› ï¸ Practice Exercises\n",
    "\n",
    "1. **Apply CoT to your specific domain problems**\n",
    "2. **Build a ReAct agent for your workflow**\n",
    "3. **Optimize existing prompts using these techniques**\n",
    "4. **Implement A/B testing for prompt variants**\n",
    "5. **Deploy a production prompt engineering system**\n",
    "\n",
    "### ðŸ¤ Community & Resources\n",
    "\n",
    "- **GitHub Repository**: Complete code examples and implementations\n",
    "- **Documentation**: Detailed guides and API references\n",
    "- **Community Forum**: Ask questions and share insights\n",
    "- **Workshop Schedule**: Live training sessions\n",
    "- **Research Papers**: Latest advances in prompt engineering\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ Congratulations!** You've completed the Advanced Prompt Engineering interactive notebook. You now have hands-on experience with cutting-edge prompt engineering techniques and are ready to apply them in real-world applications.\n",
    "\n",
    "**Next Steps**: Explore the other interactive notebooks in this series to dive deeper into specific applications and advanced topics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}