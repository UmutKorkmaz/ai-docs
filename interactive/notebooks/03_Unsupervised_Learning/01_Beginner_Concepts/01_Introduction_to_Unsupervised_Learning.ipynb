{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Unsupervised Learning\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamental concepts of unsupervised learning\n",
    "- Learn about clustering algorithms and their applications\n",
    "- Explore dimensionality reduction techniques\n",
    "- Implement association rule mining\n",
    "- Apply unsupervised learning to real-world datasets\n",
    "\n",
    "**Expected Duration:** 60-90 minutes\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python programming\n",
    "- Understanding of basic statistics\n",
    "- Familiarity with NumPy and Pandas\n",
    "- Basic knowledge of supervised learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Unsupervised Learning?\n",
    "\n",
    "Unsupervised learning is a type of machine learning where algorithms work with unlabeled data. The goal is to discover hidden patterns, structures, or relationships within the data without explicit guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA, t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles, load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Unsupervised Learning\n",
    "\n",
    "### 2.1 Clustering\n",
    "Grouping similar data points together based on their characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic clustering datasets\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Dataset 1: Well-separated blobs\n",
    "X1, y1 = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "axes[0, 0].scatter(X1[:, 0], X1[:, 1], c=y1, cmap='viridis', alpha=0.6)\n",
    "axes[0, 0].set_title('Well-separated Clusters')\n",
    "\n",
    "# Dataset 2: Overlapping clusters\n",
    "X2, y2 = make_blobs(n_samples=300, centers=3, cluster_std=2.0, random_state=42)\n",
    "axes[0, 1].scatter(X2[:, 0], X2[:, 1], c=y2, cmap='viridis', alpha=0.6)\n",
    "axes[0, 1].set_title('Overlapping Clusters')\n",
    "\n",
    "# Dataset 3: Different cluster sizes\n",
    "X3, y3 = make_blobs(n_samples=300, centers=3, cluster_std=[0.5, 1.5, 2.5], random_state=42)\n",
    "axes[0, 2].scatter(X3[:, 0], X3[:, 1], c=y3, cmap='viridis', alpha=0.6)\n",
    "axes[0, 2].set_title('Different Cluster Sizes')\n",
    "\n",
    "# Dataset 4: Moon shapes\n",
    "X4, y4 = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "axes[1, 0].scatter(X4[:, 0], X4[:, 1], c=y4, cmap='viridis', alpha=0.6)\n",
    "axes[1, 0].set_title('Moon-shaped Clusters')\n",
    "\n",
    "# Dataset 5: Concentric circles\n",
    "X5, y5 = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "axes[1, 1].scatter(X5[:, 0], X5[:, 1], c=y5, cmap='viridis', alpha=0.6)\n",
    "axes[1, 1].set_title('Concentric Circles')\n",
    "\n",
    "# Dataset 6: Anisotropic clusters\n",
    "X6 = np.random.randn(300, 2)\n",
    "X6[:, 1] = X6[:, 1] * 3 + X6[:, 0] * 0.5\n",
    "y6 = np.random.choice([0, 1, 2], 300)\n",
    "axes[1, 2].scatter(X6[:, 0], X6[:, 1], c=y6, cmap='viridis', alpha=0.6)\n",
    "axes[1, 2].set_title('Anisotropic Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Different types of clustering challenges:\")\n",
    "print(\"1. Well-separated clusters: Easy to identify\")\n",
    "print(\"2. Overlapping clusters: Hard to distinguish boundaries\")\n",
    "print(\"3. Different cluster sizes: Requires robust algorithms\")\n",
    "print(\"4. Non-linear patterns: Traditional methods fail\")\n",
    "print(\"5. Complex shapes: Need specialized algorithms\")\n",
    "print(\"6. Anisotropic distributions: Distance-based methods struggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dimensionality Reduction\n",
    "Reducing the number of features while preserving important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-dimensional dataset for dimensionality reduction\n",
    "X_high_dim, y_high_dim = make_blobs(n_samples=500, centers=4, n_features=10, \n",
    "                                   cluster_std=1.5, random_state=42)\n",
    "\n",
    "print(f\"High-dimensional dataset shape: {X_high_dim.shape}\")\n",
    "print(f\"Number of features: {X_high_dim.shape[1]}\")\n",
    "print(f\"Number of samples: {X_high_dim.shape[0]}\")\n",
    "print(f\"Number of actual clusters: {len(np.unique(y_high_dim))}\")\n",
    "\n",
    "# Visualize feature correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = np.corrcoef(X_high_dim.T)\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix (10 dimensions)')\n",
    "plt.show()\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_high_dim)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_high_dim, cmap='viridis', alpha=0.6)\n",
    "plt.title('Data Reduced to 2D using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering Algorithms\n",
    "\n",
    "### 3.1 K-Means Clustering\n",
    "Partitioning data into K clusters based on distance to centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering implementation\n",
    "def kmeans_clustering(X, n_clusters=3, random_state=42):\n",
    "    \"\"\"Perform K-Means clustering and return results\"\"\"\n",
    "    \n",
    "    # Initialize and fit K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    calinski_score = calinski_harabasz_score(X, cluster_labels)\n",
    "    davies_score = davies_bouldin_score(X, cluster_labels)\n",
    "    \n",
    "    return {\n",
    "        'model': kmeans,\n",
    "        'labels': cluster_labels,\n",
    "        'centroids': kmeans.cluster_centers_,\n",
    "        'silhouette': silhouette_avg,\n",
    "        'calinski': calinski_score,\n",
    "        'davies': davies_score\n",
    "    }\n",
    "\n",
    "# Apply K-Means to different datasets\n",
    "datasets = [\n",
    "    ('Well-separated', X1),\n",
    "    ('Overlapping', X2),\n",
    "    ('Different sizes', X3)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, X) in enumerate(datasets):\n",
    "    results = kmeans_clustering(X, n_clusters=3)\n",
    "    \n",
    "    # Plot clusters\n",
    "    scatter = axes[idx].scatter(X[:, 0], X[:, 1], c=results['labels'], \n",
    "                               cmap='viridis', alpha=0.6)\n",
    "    axes[idx].scatter(results['centroids'][:, 0], results['centroids'][:, 1], \n",
    "                     c='red', marker='x', s=200, linewidths=3)\n",
    "    axes[idx].set_title(f'{name}\\nSilhouette: {results[\"silhouette\"]:.3f}')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "    \n",
    "    print(f\"{name} - Silhouette: {results['silhouette']:.3f}, \"\n",
    "          f\"Calinski: {results['calinski']:.1f}, \"\n",
    "          f\"Davies: {results['davies']:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Finding Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters using Elbow Method and Silhouette Analysis\n",
    "def find_optimal_clusters(X, max_clusters=10):\n",
    "    \"\"\"Find optimal number of clusters using multiple methods\"\"\"\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    inertia_values = []\n",
    "    calinski_scores = []\n",
    "    \n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        silhouette_scores.append(silhouette_score(X, cluster_labels))\n",
    "        inertia_values.append(kmeans.inertia_)\n",
    "        calinski_scores.append(calinski_harabasz_score(X, cluster_labels))\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Elbow Method\n",
    "    axes[0].plot(range(2, max_clusters + 1), inertia_values, 'bo-')\n",
    "    axes[0].set_xlabel('Number of Clusters')\n",
    "    axes[0].set_ylabel('Inertia')\n",
    "    axes[0].set_title('Elbow Method')\n",
    "    \n",
    "    # Silhouette Analysis\n",
    "    axes[1].plot(range(2, max_clusters + 1), silhouette_scores, 'go-')\n",
    "    axes[1].set_xlabel('Number of Clusters')\n",
    "    axes[1].set_ylabel('Silhouette Score')\n",
    "    axes[1].set_title('Silhouette Analysis')\n",
    "    \n",
    "    # Calinski-Harabasz Index\n",
    "    axes[2].plot(range(2, max_clusters + 1), calinski_scores, 'ro-')\n",
    "    axes[2].set_xlabel('Number of Clusters')\n",
    "    axes[2].set_ylabel('Calinski-Harabasz Score')\n",
    "    axes[2].set_title('Calinski-Harabasz Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal clusters\n",
    "    optimal_silhouette = np.argmax(silhouette_scores) + 2\n",
    "    optimal_calinski = np.argmax(calinski_scores) + 2\n",
    "    \n",
    "    print(f\"Optimal number of clusters:\")\n",
    "    print(f\"- Silhouette method: {optimal_silhouette}\")\n",
    "    print(f\"- Calinski-Harabasz method: {optimal_calinski}\")\n",
    "    \n",
    "    return optimal_silhouette, optimal_calinski\n",
    "\n",
    "# Apply to synthetic dataset\n",
    "X_synth, y_synth = make_blobs(n_samples=300, centers=4, cluster_std=1.5, random_state=42)\n",
    "optimal_k = find_optimal_clusters(X_synth, max_clusters=8)\n",
    "\n",
    "# Visualize with optimal clusters\n",
    "results = kmeans_clustering(X_synth, n_clusters=optimal_k[0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_synth[:, 0], X_synth[:, 1], c=results['labels'], cmap='viridis', alpha=0.6)\n",
    "plt.scatter(results['centroids'][:, 0], results['centroids'][:, 1], \n",
    "           c='red', marker='x', s=200, linewidths=3)\n",
    "plt.title(f'K-Means with {optimal_k[0]} Clusters (Optimal)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Advanced Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different clustering algorithms\n",
    "def compare_clustering_algorithms(X, true_labels=None):\n",
    "    \"\"\"Compare multiple clustering algorithms\"\"\"\n",
    "    \n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    agg = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "    agg_labels = agg.fit_predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'K-Means': {\n",
    "            'labels': kmeans_labels,\n",
    "            'silhouette': silhouette_score(X, kmeans_labels),\n",
    "            'n_clusters': len(np.unique(kmeans_labels))\n",
    "        },\n",
    "        'DBSCAN': {\n",
    "            'labels': dbscan_labels,\n",
    "            'silhouette': silhouette_score(X, dbscan_labels) if len(np.unique(dbscan_labels)) > 1 else -1,\n",
    "            'n_clusters': len(np.unique(dbscan_labels))\n",
    "        },\n",
    "        'Agglomerative': {\n",
    "            'labels': agg_labels,\n",
    "            'silhouette': silhouette_score(X, agg_labels),\n",
    "            'n_clusters': len(np.unique(agg_labels))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test on different datasets\n",
    "test_datasets = [\n",
    "    ('Well-separated', X1),\n",
    "    ('Moons', X4),\n",
    "    ('Circles', X5)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "for row_idx, (name, X) in enumerate(test_datasets):\n",
    "    results = compare_clustering_algorithms(X)\n",
    "    \n",
    "    for col_idx, (algorithm, result) in enumerate(results.items()):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        scatter = ax.scatter(X[:, 0], X[:, 1], c=result['labels'], \n",
    "                           cmap='viridis', alpha=0.6)\n",
    "        ax.set_title(f'{algorithm}\\nSilhouette: {result[\"silhouette\"]:.3f}')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        \n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f'{name}\\n{algorithm}\\nSilhouette: {result[\"silhouette\"]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print performance summary\n",
    "print(\"Clustering Algorithm Performance Summary:\")\n",
    "for name, X in test_datasets:\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    results = compare_clustering_algorithms(X)\n",
    "    for algorithm, result in results.items():\n",
    "        print(f\"  {algorithm}: Silhouette={result['silhouette']:.3f}, \"\n",
    "              f\"Clusters={result['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction Techniques\n",
    "\n",
    "### 4.1 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive PCA analysis\n",
    "def pca_comprehensive_analysis(X, n_components=None):\n",
    "    \"\"\"Comprehensive PCA analysis with visualization\"\"\"\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return pca, X_pca\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Apply PCA\n",
    "pca, X_pca = pca_comprehensive_analysis(X_iris, n_components=2)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original data (first two features)\n",
    "axes[0].scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_xlabel(feature_names[0])\n",
    "axes[0].set_ylabel(feature_names[1])\n",
    "axes[0].set_title('Original Data (First 2 Features)')\n",
    "\n",
    "# PCA results\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_iris, cmap='viridis', alpha=0.6)\n",
    "axes[1].set_xlabel('Principal Component 1')\n",
    "axes[1].set_ylabel('Principal Component 2')\n",
    "axes[1].set_title('PCA (2 Components)')\n",
    "\n",
    "# Explained variance\n",
    "axes[2].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "          pca.explained_variance_ratio_)\n",
    "axes[2].set_xlabel('Principal Component')\n",
    "axes[2].set_ylabel('Explained Variance Ratio')\n",
    "axes[2].set_title('Explained Variance')\n",
    "axes[2].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"PCA Analysis Results:\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {np.cumsum(pca.explained_variance_ratio_)}\")\n",
    "print(f\"Total variance retained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Show feature contributions\n",
    "print(\"\\nFeature Contributions to Principal Components:\")\n",
    "for i, component in enumerate(pca.components_):\n",
    "    print(f\"\\nPC{i+1}:\")\n",
    "    for j, (feature, loading) in enumerate(zip(feature_names, component)):\n",
    "        print(f\"  {feature}: {loading:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE for visualization\n",
    "def tsne_visualization(X, y, perplexity=30, n_iter=1000):\n",
    "    \"\"\"Apply t-SNE for visualization\"\"\"\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, \n",
    "               random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    return X_tsne\n",
    "\n",
    "# Compare PCA and t-SNE\n",
    "X_tsne = tsne_visualization(X_iris, y_iris, perplexity=30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PCA\n",
    "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_iris, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_xlabel('Principal Component 1')\n",
    "axes[0].set_ylabel('Principal Component 2')\n",
    "axes[0].set_title('PCA')\n",
    "\n",
    "# t-SNE\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_iris, cmap='viridis', alpha=0.6)\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "axes[1].set_title('t-SNE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# t-SNE with different perplexity values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "perplexities = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for idx, perplexity in enumerate(perplexities):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    X_tsne = tsne_visualization(X_iris, y_iris, perplexity=perplexity)\n",
    "    \n",
    "    axes[row, col].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_iris, cmap='viridis', alpha=0.6)\n",
    "    axes[row, col].set_title(f't-SNE (Perplexity={perplexity})')\n",
    "    axes[row, col].set_xlabel('t-SNE 1')\n",
    "    axes[row, col].set_ylabel('t-SNE 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE Parameter Effects:\")\n",
    "print(\"- Low perplexity (5-10): Focuses on local structure\")\n",
    "print(\"- Medium perplexity (20-40): Balanced view of local and global structure\")\n",
    "print(\"- High perplexity (50+): Emphasizes global structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World Application: Customer Segmentation\n",
    "\n",
    "Let's apply unsupervised learning to segment customers based on their behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer segmentation dataset\n",
    "def create_customer_data(n_samples=1000):\n",
    "    \"\"\"Create synthetic customer data for segmentation\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Customer segments\n",
    "    segments = ['High Value', 'Medium Value', 'Low Value', 'New Customer']\n",
    "    segment_probs = [0.15, 0.35, 0.35, 0.15]\n",
    "    \n",
    "    # Generate segment-specific characteristics\n",
    "    data = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        n_segment = int(n_samples * segment_probs[i])\n",
    "        \n",
    "        if segment == 'High Value':\n",
    "            age = np.random.normal(45, 10, n_segment)\n",
    "            income = np.random.normal(120000, 30000, n_segment)\n",
    "            spending = np.random.normal(5000, 1000, n_segment)\n",
    "            frequency = np.random.normal(25, 5, n_segment)\n",
    "            tenure = np.random.normal(60, 15, n_segment)\n",
    "        elif segment == 'Medium Value':\n",
    "            age = np.random.normal(38, 12, n_segment)\n",
    "            income = np.random.normal(70000, 20000, n_segment)\n",
    "            spending = np.random.normal(2500, 500, n_segment)\n",
    "            frequency = np.random.normal(15, 4, n_segment)\n",
    "            tenure = np.random.normal(36, 12, n_segment)\n",
    "        elif segment == 'Low Value':\n",
    "            age = np.random.normal(55, 15, n_segment)\n",
    "            income = np.random.normal(40000, 15000, n_segment)\n",
    "            spending = np.random.normal(800, 200, n_segment)\n",
    "            frequency = np.random.normal(5, 2, n_segment)\n",
    "            tenure = np.random.normal(24, 10, n_segment)\n",
    "        else:  # New Customer\n",
    "            age = np.random.normal(30, 8, n_segment)\n",
    "            income = np.random.normal(60000, 25000, n_segment)\n",
    "            spending = np.random.normal(1500, 800, n_segment)\n",
    "            frequency = np.random.normal(3, 2, n_segment)\n",
    "            tenure = np.random.normal(3, 2, n_segment)\n",
    "        \n",
    "        # Ensure positive values\n",
    "        income = np.maximum(income, 20000)\n",
    "        spending = np.maximum(spending, 100)\n",
    "        frequency = np.maximum(frequency, 1)\n",
    "        tenure = np.maximum(tenure, 1)\n",
    "        \n",
    "        segment_data = pd.DataFrame({\n",
    "            'age': age,\n",
    "            'income': income,\n",
    "            'spending': spending,\n",
    "            'frequency': frequency,\n",
    "            'tenure': tenure,\n",
    "            'segment': segment\n",
    "        })\n",
    "        \n",
    "        data.append(segment_data)\n",
    "    \n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Create and explore the dataset\n",
    "customer_df = create_customer_data(1000)\n",
    "print(\"Customer Segmentation Dataset:\")\n",
    "print(customer_df.head())\n",
    "print(f\"\\nDataset shape: {customer_df.shape}\")\n",
    "print(f\"\\nSegment distribution:\")\n",
    "print(customer_df['segment'].value_counts())\n",
    "\n",
    "# Visualize customer characteristics\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.boxplot(data=customer_df, x='segment', y='income')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Income by Segment')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.boxplot(data=customer_df, x='segment', y='spending')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Spending by Segment')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.scatterplot(data=customer_df, x='income', y='spending', hue='segment', alpha=0.6)\n",
    "plt.title('Income vs Spending')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.scatterplot(data=customer_df, x='tenure', y='frequency', hue='segment', alpha=0.6)\n",
    "plt.title('Tenure vs Frequency')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.boxplot(data=customer_df, x='segment', y='age')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Age by Segment')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "customer_df['segment'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Segment Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering to customer data\n",
    "# Prepare features for clustering\n",
    "X_customers = customer_df.drop('segment', axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_customers_scaled = scaler.fit_transform(X_customers)\n",
    "\n",
    "# Find optimal number of clusters\n",
    "print(\"Finding optimal number of clusters for customer data...\")\n",
    "optimal_k_customers = find_optimal_clusters(X_customers_scaled, max_clusters=8)\n",
    "\n",
    "# Apply K-Means with optimal clusters\n",
    "n_clusters = optimal_k_customers[0]\n",
    "kmeans_customers = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "customer_clusters = kmeans_customers.fit_predict(X_customers_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "customer_df['cluster'] = customer_clusters\n",
    "\n",
    "# Analyze clusters\n",
    "print(f\"\\nCustomer Segmentation Results:\")\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Silhouette score: {silhouette_score(X_customers_scaled, customer_clusters):.3f}\")\n",
    "\n",
    "# Cluster characteristics\n",
    "cluster_analysis = customer_df.groupby('cluster').agg({\n",
    "    'age': 'mean',\n",
    "    'income': 'mean',\n",
    "    'spending': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'tenure': 'mean',\n",
    "    'segment': lambda x: x.value_counts().index[0]\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "display(cluster_analysis)\n",
    "\n",
    "# Visualize clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# PCA visualization\n",
    "pca_customers = PCA(n_components=2)\n",
    "X_pca_customers = pca_customers.fit_transform(X_customers_scaled)\n",
    "\n",
    "axes[0, 0].scatter(X_pca_customers[:, 0], X_pca_customers[:, 1], \n",
    "                   c=customer_clusters, cmap='viridis', alpha=0.6)\n",
    "axes[0, 0].set_title('Customer Clusters (PCA)')\n",
    "axes[0, 0].set_xlabel('Principal Component 1')\n",
    "axes[0, 0].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Income vs Spending\n",
    "axes[0, 1].scatter(customer_df['income'], customer_df['spending'], \n",
    "                   c=customer_clusters, cmap='viridis', alpha=0.6)\n",
    "axes[0, 1].set_title('Income vs Spending by Cluster')\n",
    "axes[0, 1].set_xlabel('Income')\n",
    "axes[0, 1].set_ylabel('Spending')\n",
    "\n",
    "# Tenure vs Frequency\n",
    "axes[1, 0].scatter(customer_df['tenure'], customer_df['frequency'], \n",
    "                   c=customer_clusters, cmap='viridis', alpha=0.6)\n",
    "axes[1, 0].set_title('Tenure vs Frequency by Cluster')\n",
    "axes[1, 0].set_xlabel('Tenure')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Cluster size distribution\n",
    "cluster_sizes = customer_df['cluster'].value_counts().sort_index()\n",
    "axes[1, 1].bar(cluster_sizes.index, cluster_sizes.values)\n",
    "axes[1, 1].set_title('Cluster Size Distribution')\n",
    "axes[1, 1].set_xlabel('Cluster')\n",
    "axes[1, 1].set_ylabel('Number of Customers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Provide business insights\n",
    "print(\"\\nBusiness Insights:\")\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_data = customer_df[customer_df['cluster'] == cluster]\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    print(f\"  Size: {len(cluster_data)} customers ({len(cluster_data)/len(customer_df):.1%})\")\n",
    "    print(f\"  Average income: ${cluster_data['income'].mean():,.0f}\")\n",
    "    print(f\"  Average spending: ${cluster_data['spending'].mean():,.0f}\")\n",
    "    print(f\"  Average tenure: {cluster_data['tenure'].mean():.1f} months\")\n",
    "    print(f\"  Most common segment: {cluster_data['segment'].value_counts().index[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Exploration\n",
    "\n",
    "### 6.1 Parameter Tuning Widget\n",
    "Explore how different parameters affect clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive clustering widget\n",
    "def interactive_clustering(X, n_clusters, algorithm='kmeans'):\n",
    "    \"\"\"Interactive clustering with different algorithms\"\"\"\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply clustering\n",
    "    if algorithm == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    elif algorithm == 'dbscan':\n",
    "        model = DBSCAN(eps=0.5, min_samples=5)\n",
    "    elif algorithm == 'agglomerative':\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    \n",
    "    labels = model.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silhouette = silhouette_score(X_scaled, labels)\n",
    "    else:\n",
    "        silhouette = -1\n",
    "    \n",
    "    return labels, silhouette\n",
    "\n",
    "# Create widgets\n",
    "algorithm_widget = widgets.Dropdown(\n",
    "    options=['kmeans', 'dbscan', 'agglomerative'],\n",
    "    value='kmeans',\n",
    "    description='Algorithm:'\n",
    ")\n",
    "\n",
    "clusters_widget = widgets.IntSlider(\n",
    "    value=3, min=2, max=8, step=1,\n",
    "    description='Clusters:'\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget:\n",
    "        output_widget.clear_output()\n",
    "        \n",
    "        labels, silhouette = interactive_clustering(\n",
    "            X_customers_scaled, clusters_widget.value, algorithm_widget.value\n",
    "        )\n",
    "        \n",
    "        print(f\"Algorithm: {algorithm_widget.value}\")\n",
    "        print(f\"Number of clusters: {len(np.unique(labels))}\")\n",
    "        print(f\"Silhouette score: {silhouette:.3f}\")\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(X_pca_customers[:, 0], X_pca_customers[:, 1], \n",
    "                   c=labels, cmap='viridis', alpha=0.6)\n",
    "        plt.title(f'{algorithm_widget.value.title()} Clustering Results')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.show()\n",
    "\n",
    "button = widgets.Button(description=\"Apply Clustering\")\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    algorithm_widget, clusters_widget, button, output_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Concepts Summary\n",
    "\n",
    "### What We Learned:\n",
    "1. **Types of Unsupervised Learning**: Clustering, dimensionality reduction, and association rule mining\n",
    "2. **Clustering Algorithms**: K-Means, DBSCAN, and hierarchical clustering\n",
    "3. **Evaluation Metrics**: Silhouette score, Calinski-Harabasz index, Davies-Bouldin index\n",
    "4. **Dimensionality Reduction**: PCA and t-SNE for visualization and feature extraction\n",
    "5. **Real-World Applications**: Customer segmentation, anomaly detection, and pattern discovery\n",
    "6. **Parameter Tuning**: How to optimize clustering parameters for better results\n",
    "\n",
    "### Best Practices:\n",
    "- Always standardize data before clustering\n",
    "- Use multiple evaluation metrics to assess clustering quality\n",
    "- Consider the business context when interpreting clusters\n",
    "- Visualize results to validate clustering effectiveness\n",
    "- Try multiple algorithms and compare results\n",
    "- Handle outliers appropriately\n",
    "\n",
    "### Common Challenges:\n",
    "- Determining the optimal number of clusters\n",
    "- Handling high-dimensional data\n",
    "- Dealing with different cluster shapes and sizes\n",
    "- Interpreting cluster meanings\n",
    "- Scalability with large datasets\n",
    "\n",
    "### Next Steps:\n",
    "- Explore advanced clustering algorithms (GMM, Spectral Clustering)\n",
    "- Learn about anomaly detection techniques\n",
    "- Study association rule mining (Apriori, FP-Growth)\n",
    "- Dive into deep learning for unsupervised learning (Autoencoders)\n",
    "- Learn about reinforcement learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises and Challenges\n",
    "\n",
    "### Exercise 1: Cluster Analysis\n",
    "Apply K-Means clustering to a dataset of your choice and determine the optimal number of clusters using elbow method and silhouette analysis.\n",
    "\n",
    "### Exercise 2: Dimensionality Reduction\n",
    "Take a high-dimensional dataset and apply PCA to reduce it to 2D or 3D. Visualize the results and interpret the principal components.\n",
    "\n",
    "### Exercise 3: Algorithm Comparison\n",
    "Compare at least three different clustering algorithms on the same dataset. Evaluate them using multiple metrics and discuss their strengths and weaknesses.\n",
    "\n",
    "### Exercise 4: Real-World Application\n",
    "Find a real-world dataset (e.g., customer data, image data, text data) and apply unsupervised learning techniques to discover patterns or insights.\n",
    "\n",
    "### Exercise 5: Parameter Optimization\n",
    "Create a systematic approach to optimize clustering parameters. Consider using GridSearchCV or manual parameter sweeps.\n",
    "\n",
    "**Challenge**: Build a complete unsupervised learning pipeline that includes data preprocessing, clustering, evaluation, and business interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Further Learning Resources\n",
    "\n",
    "### Books:\n",
    "- \"Introduction to Statistical Learning\" by James, Witten, Hastie, and Tibshirani\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- \"Mining of Massive Datasets\" by Leskovec, Rajaraman, and Ullman\n",
    "\n",
    "### Online Courses:\n",
    "- Andrew Ng's Machine Learning Course (Coursera)\n",
    "- Unsupervised Learning in Python (DataCamp)\n",
    "- Deep Learning Specialization (Coursera)\n",
    "\n",
    "### Documentation:\n",
    "- [Scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [Scikit-learn Dimensionality Reduction Documentation](https://scikit-learn.org/stable/modules/decomposition.html)\n",
    "- [UMAP Documentation](https://umap-learn.readthedocs.io/)\n",
    "\n",
    "### Practice Platforms:\n",
    "- Kaggle (kaggle.com)\n",
    "- UCI Machine Learning Repository\n",
    "- Google Dataset Search\n",
    "\n",
    "### Community:\n",
    "- Stack Overflow\n",
    "- Reddit r/MachineLearning\n",
    "- Towards Data Science (Medium)\n",
    "- KDnuggets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}